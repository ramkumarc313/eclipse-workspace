<?xml version="1.0" encoding="utf-8" ?>
<rss version="2.0"
     xmlns:dc="http://purl.org/dc/elements/1.1/"
     xmlns:content="http://purl.org/rss/1.0/modules/content/">

<channel>
  <title>Planet MySQL</title>
  <link>http://www.planetmysql.org/</link>
  <pubDate>Wed, 04 Mar 2015 12:30:01 +0000</pubDate>
  <language>en</language>
  <description>Planet MySQL - http://www.planetmysql.org/</description>

  <item>
    <title>FromDual.en: Nagios and Icinga plug-ins for MySQL/MariaDB released</title>
    <guid isPermaLink="false">http://www.fromdual.com/newsletter-en-2015-02</guid>
    <link>http://www.fromdual.com/newsletter-en-2015-02</link>
    <description>Taxonomy upgrade extras:&amp;nbsp;nagiosicingaplug-inmysqlmariadbpercona serverGalera ClusterreleaseFromDual is pleased to announce the release of a new version 1.0.0 of the Nagios and Icinga plug-ins for MySQL, MariaDB, Percona Server and Galera Cluster.Any information about the changes and the new functions of the Nagios and Icinga plug-ins you can find here.
If you have any problems with the new version you can reach us by e-mail at: contact@fromdual.com or by phone on +41 44 940 24 82.
Your FromDual Team</description>
    <content:encoded><![CDATA[<div><div>Taxonomy upgrade extras:&nbsp;</div><div><div><a href="http://www.fromdual.ch/forum/667" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">nagios</a></div><div><a href="http://www.fromdual.ch/forum/668" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">icinga</a></div><div><a href="http://www.fromdual.ch/forum/669" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">plug-in</a></div><div><a href="http://www.fromdual.ch/taxonomy/term/26" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">mysql</a></div><div><a href="http://www.fromdual.ch/taxonomy/term/200" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">mariadb</a></div><div><a href="http://www.fromdual.ch/taxonomy/term/638" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">percona server</a></div><div><a href="http://www.fromdual.ch/forum/513" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Galera Cluster</a></div><div><a href="http://www.fromdual.ch/taxonomy/term/289" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">release</a></div></div></div><div><div><div property="content:encoded"><p>FromDual is pleased to announce the release of a new version 1.0.0 of the Nagios and Icinga plug-ins for MySQL, MariaDB, Percona Server and Galera Cluster.</p><br /><p>Any information about the changes and the new functions of the Nagios and Icinga plug-ins you can find <a href="http://fromdual.com/nagios-and-icinga-plug-ins-for-mysql-1.0.0-have-been-released" title="Nagios and Icinga plug-ins for MySQL 1.0.0" target="_blank">here</a>.</p>
<p>If you have any problems with the new version you can reach us by e-mail at: <a href="mailto:contact@fromdual.com">contact@fromdual.com</a> or by phone on +41 44 940 24 82.</p>
<br /><br /><p>Your FromDual Team</p></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989128&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989128&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 04 Mar 2015 12:03:58 +0000</pubDate>
  </item>

  <item>
    <title>Improving your MariaDB Back-Up Plan</title>
    <guid isPermaLink="false">1820 at http://mariadb.com</guid>
    <link>http://mariadb.com/blog/improving-your-mariadb-back-plan</link>
    <description>Wed, 2015-03-04 10:26stephanevaroquiEvery organization needs a good database back-up plan, to include plans for data restoration. Such a plan should include, of course making proper back-ups and doing in a way that won't disturb user traffic. It should also include making sure that you're ready to restore back-ups quickly, so that your organization can operate with minimal downtime and without loss of data when a problem occurs. There are many things such as these to consider for developing a good back-up and restoration plan.
Full Back-Ups
It's strongly recommended to make a back-up of all of the databases in the same dump. This will allow for a consistent view of the global state of the databases. Plus, a full back-up can be used for provisioning a new replication when needed. A full back-up will save the system tables and stay consistent with multiple database queries and transactions. There are certainly some advantages to making back-ups based on single databases, but a full back-up is preferred. If you're concerned by the size of the resulting dump file, it's possible to split a global dump between multiple files. If you search the web, you will find some back-up scripts that will help you to do this.
It is possible to make a so called, incremental backup. This is a back-up in which a full back-up is made not so often—perhaps only once a week—and copy is made of the binary logs every day. You might want to make full back-ups soon depending on the size of the binary logs generated by your database. Otherwise, the restore process will take longer using the binary logs.
Blocking Back-Ups
The most simple back-up is a blocking back-up. This is a back-up in which all of the databases and their tables are locked for writes during the back-up process—or at least all of the tables when making a back-up of a single database. This is ideal for consistentcy of data between tables, but it can be inconvenient with high-traffic, large databases. For those situations, you may want to make some changes to the types of storage engines you use.
The nature of MariaDB and MySQL allows transactional and non-transactional tables to be shared in the same database. This is a nice feature. However, storage engines like MyISAM require the blocking method, whereas ones like InnoDB don't. For a good back-up policy, you may want to make some compromises related to storage engines and switch all of your tables to InnoDB or other tables that will allow you to make non-blocking back-ups.
You could store all of your data tables using transactional storage engines such as, InnoDB, XtraDB, and TokuDB. These storage engines can be mixed to make a consistent point-in-time, non-blocking back-ups.
When making a dump using a version prior to 5.3, or when making a physical back-ups, one needed to do a FTWRL (Flush Table With Read Lock ), to be able to flush the server and reset the binary log, before starting a transactional repeatable read snapshot. This generally isn't good, if you make the back-up on a loaded server. It causes a metadata lock until all of the currently running transactions are finished processing. Note, this may not be a problem if you have only small transactions—even plenty of them. Fortunately, MariaDB 5.3 introduced some new sessions variables that provide non-stalling positioning of the binary logs for every transaction. The utility mysqldump makes good use of such variables to avoid stalling on the master that FTWRL introduced previously.
Physical Back-Ups
Another possible back-up method is to perform physical back-ups using tools such as LVM, XtraBackup, TokuDB Hot Back-up. These will greatly improve recovery time as restoration is a matter of moving files around. Compression tends to defeat this strategy, unless you use uncompressed tools like, Pigz. Physical back-ups can be useful to shorten the restoration time. The most widely used is prossibly the LVM snapshot, which uses an excellent Perl script as a wrapper (http://www.lenzg.net/mylvmbackup/).
Keep in mind that when making a physical back-up, you may copy physical corruption, as well. When restoring a physical back-up that contains corruption, although the back-up will look fine, you will encounter problems—either during the restoration or later. The server may stop on a checksum error. If that happens, a time machine back-up will be needed to go back incrementally until the corruption is eliminated.
You should experiment with restoring from a logical back-up to improve your restore time. Try to restore a back-up just after making a logical back-up so that you can improve your routine and your restoration plan. If the test restore takes too long, consider how to improve your time—with better hardware like SSD or a flash card for your back-up node, or use a concurrent back-up/restore tool like the mydumper or myloader.
Performance Impact
The mysqldump utility doesn't necessarily consume much CPU resources on modern hardware. This is because it uses by default a single thread, consuming one core for dumping a database. This method is good for a heavily loaded server. The disk input/outputs per second (IOPS), though, can increase for multiple reasons. When you make a back-up on the same device as the database, this cause problems, producing unnecessary random IOPS. The dump is done sequentially, on a per table basis. Because it causes a full table scan so that a lot buffer page miss on tables that are not fully cached in memory, producing again read IO. For InnoDB tables, data is usually read in disk order when using auto-increment primary key or when optimized on a regular basis. In this case, disk read-ahead can increase the back-up throughput. For MyISAM or Aria tables, the IO scheduler has a better chance to merge IOPS on sequential, table per table dumps.
As a good practice, it's recommend that you make a back-up from a network location to remove disk IOPS on the database server. However, it is vital to use a separate network card to keep network bandwidth available for regular traffic.
Although mysqldump will by default preserve your resources for regular spindle disks and low-core hardware, this doesn't mean that concurrent dumps cannot benefit from hardware architecture like SAN, flash storage, low write workload. The back-up time would benefits from a tool such as MyDumper.
Including More in Back-Ups
When making a back-up with mysqldump, triggers are dumped together with tables, since they're part of the table definition. However, stored procedures, views, and events need extra parameters to be recreated explicitly. Procedures and functions are also part of system tables, but may want to add --routines --event to the command.
Off-Site Back-Ups
Another thing to consider when creating a back-up and restoration policy is where you will store back-ups. You should store your back-up files in a secure, separate physical location, or off-line. Just be sure they're readily available if the database server is compromised or suffers damage that might also affect files located in the same facility.
Conclusion
There are other things to consider regarding developing a back-up and restoration policy, regarding making better back-ups. I'll cover some of them in another article. These are just a few that you may not have considered.
Tags:&amp;nbsp;DBA
About the Author
  
      


Stephane Varoqui
Stephane Varoqui is a Consultant, mainly in French speaking areas. Stephane originally started working at MySQL in 2005.



</description>
    <content:encoded><![CDATA[<div><div><div>Wed, 2015-03-04 10:26</div></div></div><div></div><div><div><div>stephanevaroqui</div></div></div><div><div><div property="content:encoded"><p><img alt="Database Backup Logo" src="http://mariadb.com/sites/default/files/elfinder/94/Blog/data_backup.png" style="float:right; height:200px; margin:20px; width:200px" />Every organization needs a good database back-up plan, to include plans for data restoration. Such a plan should include, of course making proper back-ups and doing in a way that won't disturb user traffic. It should also include making sure that you're ready to restore back-ups quickly, so that your organization can operate with minimal downtime and without loss of data when a problem occurs. There are many things such as these to consider for developing a good back-up and restoration plan.</p>
<p><strong>Full Back-Ups</strong></p>
<p>It's strongly recommended to make a back-up of all of the databases in the same dump. This will allow for a consistent view of the global state of the databases. Plus, a full back-up can be used for provisioning a new replication when needed. A full back-up will save the system tables and stay consistent with multiple database queries and transactions. There are certainly some advantages to making back-ups based on single databases, but a full back-up is preferred. If you're concerned by the size of the resulting dump file, it's possible to split a global dump between multiple files. If you search the web, you will find some back-up scripts that will help you to do this.</p>
<p>It is possible to make a so called, incremental backup. This is a back-up in which a full back-up is made not so often—perhaps only once a week—and copy is made of the binary logs every day. You might want to make full back-ups soon depending on the size of the binary logs generated by your database. Otherwise, the restore process will take longer using the binary logs.</p>
<p><strong>Blocking Back-Ups</strong></p>
<p>The most simple back-up is a blocking back-up. This is a back-up in which all of the databases and their tables are locked for writes during the back-up process—or at least all of the tables when making a back-up of a single database. This is ideal for consistentcy of data between tables, but it can be inconvenient with high-traffic, large databases. For those situations, you may want to make some changes to the types of storage engines you use.</p>
<p>The nature of MariaDB and MySQL allows transactional and non-transactional tables to be shared in the same database. This is a nice feature. However, storage engines like MyISAM require the blocking method, whereas ones like InnoDB don't. For a good back-up policy, you may want to make some compromises related to storage engines and switch all of your tables to InnoDB or other tables that will allow you to make non-blocking back-ups.</p>
<p>You could store all of your data tables using transactional storage engines such as, InnoDB, XtraDB, and TokuDB. These storage engines can be mixed to make a consistent point-in-time, non-blocking back-ups.</p>
<p>When making a dump using a version prior to 5.3, or when making a physical back-ups, one needed to do a FTWRL (Flush Table With Read Lock ), to be able to flush the server and reset the binary log, before starting a transactional repeatable read snapshot. This generally isn't good, if you make the back-up on a loaded server. It causes a metadata lock until all of the currently running transactions are finished processing. Note, this may not be a problem if you have only small transactions—even plenty of them. Fortunately, MariaDB 5.3 introduced some new sessions variables that provide non-stalling positioning of the binary logs for every transaction. The utility mysqldump makes good use of such variables to avoid stalling on the master that FTWRL introduced previously.</p>
<p><strong>Physical Back-Ups</strong></p>
<p>Another possible back-up method is to perform physical back-ups using tools such as LVM, XtraBackup, TokuDB Hot Back-up. These will greatly improve recovery time as restoration is a matter of moving files around. Compression tends to defeat this strategy, unless you use uncompressed tools like, Pigz. Physical back-ups can be useful to shorten the restoration time. The most widely used is prossibly the LVM snapshot, which uses an excellent Perl script as a wrapper (<a href="http://www.lenzg.net/mylvmbackup/">http://www.lenzg.net/mylvmbackup/</a>).</p>
<p>Keep in mind that when making a physical back-up, you may copy physical corruption, as well. When restoring a physical back-up that contains corruption, although the back-up will look fine, you will encounter problems—either during the restoration or later. The server may stop on a checksum error. If that happens, a time machine back-up will be needed to go back incrementally until the corruption is eliminated.</p>
<p>You should experiment with restoring from a logical back-up to improve your restore time. Try to restore a back-up just after making a logical back-up so that you can improve your routine and your restoration plan. If the test restore takes too long, consider how to improve your time—with better hardware like SSD or a flash card for your back-up node, or use a concurrent back-up/restore tool like the mydumper or myloader.</p>
<p><strong>Performance Impact</strong></p>
<p>The mysqldump utility doesn't necessarily consume much CPU resources on modern hardware. This is because it uses by default a single thread, consuming one core for dumping a database. This method is good for a heavily loaded server. The disk input/outputs per second (IOPS), though, can increase for multiple reasons. When you make a back-up on the same device as the database, this cause problems, producing unnecessary random IOPS. The dump is done sequentially, on a per table basis. Because it causes a full table scan so that a lot buffer page miss on tables that are not fully cached in memory, producing again read IO. For InnoDB tables, data is usually read in disk order when using auto-increment primary key or when optimized on a regular basis. In this case, disk read-ahead can increase the back-up throughput. For MyISAM or Aria tables, the IO scheduler has a better chance to merge IOPS on sequential, table per table dumps.</p>
<p>As a good practice, it's recommend that you make a back-up from a network location to remove disk IOPS on the database server. However, it is vital to use a separate network card to keep network bandwidth available for regular traffic.</p>
<p>Although mysqldump will by default preserve your resources for regular spindle disks and low-core hardware, this doesn't mean that concurrent dumps cannot benefit from hardware architecture like SAN, flash storage, low write workload. The back-up time would benefits from a tool such as MyDumper.</p>
<p><strong>Including More in Back-Ups</strong></p>
<p>When making a back-up with mysqldump, triggers are dumped together with tables, since they're part of the table definition. However, stored procedures, views, and events need extra parameters to be recreated explicitly. Procedures and functions are also part of system tables, but may want to add --routines --event to the command.</p>
<p><strong>Off-Site Back-Ups</strong></p>
<p>Another thing to consider when creating a back-up and restoration policy is where you will store back-ups. You should store your back-up files in a secure, separate physical location, or off-line. Just be sure they're readily available if the database server is compromised or suffers damage that might also affect files located in the same facility.</p>
<p><strong>Conclusion</strong></p>
<p>There are other things to consider regarding developing a back-up and restoration policy, regarding making better back-ups. I'll cover some of them in another article. These are just a few that you may not have considered.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/dba" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">DBA</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-4511-1405421785.jpg?itok=KQczHIKV" width="72" height="72" alt="stephanevaroqui's picture" title="stephanevaroqui's picture" />  </div>
</div>
<div>
<div>Stephane Varoqui</div>
<div><p>Stephane Varoqui is a Consultant, mainly in French speaking areas. Stephane originally started working at MySQL in 2005.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989126&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989126&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 04 Mar 2015 10:26:44 +0000</pubDate>
    <dc:creator>MariaDB</dc:creator>
  </item>

  <item>
    <title>Restart phases of a node restart in MySQL Cluster</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-14455177.post-7533186217653580437</guid>
    <link>http://mikaelronstrom.blogspot.com/2015/03/restart-phases-of-node-restart-in-mysql.html</link>
    <description>In MySQL Cluster 7.4 we have worked on improving the speed of restarts.In addition we have made it easier to track how fast the restarts areproceeding and to see which phases of the restart take a long time.For this purpose we have added a new Ndbinfo table in MySQL Cluster 7.4.I will try to explain the restart phases in a series of blogs and how they mapto the columns in this Ndbinfo table.First some short intro to concepts in MySQL Cluster. LCP stands for localcheckpoints and this is where the main memory content of tables arewritten to disk at regular intervals. There is also a GCP, global checkpoint,this is a protocol to synchronise the REDO log writes and thus aboutdurability of transactions. LDM stands for Local Data Manager and isthe thread that runs all the code that maintains the actual data in MySQLCluster. This includes row storage, REDO logging, hash index, T-treeindex (ordered index) and various internal triggering functions to supportvarious high-level functionality such as unique indexes, foreign keys andso forth.In a node restart there is a number of nodes involved, at a minimum thereare two nodes involved and there could be more than two nodes involved aswell. At least one node is a starting node and one live node is always amaster node. There can also be other live nodes that have already started andthat participate in some phases of the restart but not in all phases.There can also be other starting nodes that have to be synchronised tosome extent with the starting node.A node restart always begins with a node failing. This could be a controlledfailure as is used in e.g. a rolling upgrade of the MySQL Cluster software.In this case there are usually multiple nodes that are set to fail at thesame time, often one fails half the nodes as part of a software upgrade.So the first phase of the node restart phases is to complete the node failure.This involves handling of in progress transactions involving the failed node.It involves taking over as a master if the failed node wasa master node. It involves handling a number of protocols in the data nodeswhere other nodes are waiting for the failed node. In 7.3 and older versionsthis can at times be a fairly lengthy phase, in particular in taking over themaster role of the local checkpoint protocol. With the restart improvementsin MySQL Cluster 7.4 it should be a rare event that this phase takes a longtime.The next node restart phase is that the crashed node starts up again. The firstthing the starting node does is to attempt to allocate a node id. The allocationof a node id will have to wait for the node failure phase to complete first. Sothis means that this phase can take some time, but normally it proceeds veryquickly.When the new node has received a node id the next step is to be included inthe heartbeat protocol. One node can enter into the heartbeat protocol per3 seconds. So if many nodes start up at the same time a node can be heldwaiting here until the cluster accepts us into the cluster.Once the node have been included into the heartbeat protocol which is thelowest node handling layer in MySQL Cluster, we will ask for permissionto get included in the cluster at the next level of maintenance. The nodecould be blocked to be included at this level for example if other nodes arestill removing our node from local checkpoints after we did an initial noderestart, the node could also be blocked by include node protocols still goingon. So the node could be held in this layer at times, but it's a rare event andshould not happen very often.Also only one node at a time can be in this phase at a time, the next node willbe allowed to proceed after the node have completed this phase and the nextphase where we get a copy of the meta data in the cluster.After being accepted into the cluster the node have to wait until we have pausedthe LCP protocol. This is a new feature of MySQL Cluster 7.4 that have beenadded to avoid having to wait in this state until a local checkpoint is completedbefore we can proceed. In 7.4 we can pause the checkpoint execution (for themost part this will not block checkpoint execution since a major part of thecheckpoint is off-loaded to the LDM threads that can execute up to 64 localcheckpoint fragments without getting any new requests from the master node).As soon as the node managed to pause the local checkpoint execution we canstart to receive the meta data from the master node. The copy phase of themeta data could take some time if there are very many tables in the cluster.Normally it is a very quick action.Now that the node have copied the meta data to our node, the next node canproceed with that phase. Our node will continue by being included into the globalcheckpoint protocol and a number of other node handling protocols. This isnormally a quick phase.Now that the node have been included in the protocols the master node willtell us what fragments to restore from disk. This phase is usually also fast.Now the node have reached the first phase that is normal to take a bit longertime. This phase is where the node restores all requested fragments from acheckpoint stored on disk. All LDM threads work in parallel on restoringall requested fragments here. So the node should be able to restore at a speedof at least a few million records per second here.After completing restoring the fragment checkpoints the next phase is to executethe UNDO logs against all disk data fragments. If there are no disk data tablesin the cluster then this phase will be very quick, otherwise it can take sometime.After restoring the fragments and applying the disk data UNDO log the node isnow ready to apply the logical REDO log. All LDM threads will execute theREDO logs in parallel. There are 4 phases of REDO log execution, but in realityonly the first phase is used. The time of this phase depends on how fast thecheckpoints were generated before the crash. The faster we execute thecheckpoints the smaller amount of REDO log is needed to execute as part ofa node restart.Also here the node should be able to process some millions of log records persecond since we are executing this in parallel in the LDM threads.Now that we have restored a global checkpoint for all fragments the node isready to rebuild all ordered indexes. The fragments are check pointed withonly its data, indexes are not check pointed. The indexes are rebuilt as partof a node restart. The primary key hash index is rebuilt as part of restore ofthe fragments and execution of the REDO log. The ordered index is rebuiltin a separate phase after completing the applying of the REDO log.This is also a phase that can take some time. All LDM threads execute inparallel in this phase. The more records and the more ordered indexes wehave the longer this phase takes to execute.Now the node has managed to restore an old but consistent version of thefragments we were asked to restore.The next phase is to synchronise the old data with the live data in thenodes that are up and running and contains the latest version of eachrecord.As a new feature of MySQL Cluster 7.4 this phase can be parallelisedamong the LDM threads. So the node can synchronise with data in thelive nodes in many threads in parallel.This phase can be delayed in cases with many tables and many concurrentnode restarts since we need a lock on a fragment before copying itsmeta data to new starting nodes which is also needed before startingthe copying phase to synchronise with the live nodes.At this point in time the node have an up-to-date version of all restoredfragments in the node. The node do however not have a durable versionyet. For the fragments to be restorable on our starting node we need thenode to participate in a full local checkpoint.Waiting for local checkpoints in various manners is the most commonactivity in delaying restarts in 7.3 and older versions of MySQL Cluster.In MySQL Cluster 7.4 we have managed to decrease the amount ofwaiting for local checkpoints, in addition we have also been able to speedup the local checkpointing in general and more specifically when noderestarts are executing. Those two things plus the parallelisation of thesynchronisation phase are the main ways of how we managedto decrease restart times by a factor of 5.5X in MySQL Cluster 7.4.In order to ensure that as many nodes as possible get started togetherafter the waiting for the same local checkpoint execution, we have theability to block the start of a local checkpoint if we have nodes that areclose to reaching this wait state. Given that we have statistics on how longtime the restarts takes, we can predict if it is useful to wait foranother node to reach this state before we start a new local checkpoint.Finally when we have waited for a complete LCP it is time for the finalphase which is to wait for subscription handover to occur. This relatesto the MySQL replication support in MySQL Cluster where we have tocontact all MySQL Servers in the cluster informing them of our newnode that can also generate replication events.After this final phase the restart is completed.As an example we measured a node restart of around 26 GByte of datatook around 3-4 minutes in MySQL Cluster 7.4.The speed of node restarts is highly dependent on local checkpoint speed,we have introduced a number of new configuration parameters to controlthis speed, we have also introduced a new Ndbinfo table also to track thecheckpoint speed. We will describe this in a separate blog entry.Each of the phases described above is represented as a column in thendbinfo.restart_info table. We report the time that we spend in eachphase measured in seconds. There is a row for each restart that hasbeen observed. This means that not all nodes are present in this table.After an initial start or after a cluster restart the table is even empty.We will also describe the additional log printouts that we have added tothe node logs describing what is going in the node during a node restartin a separate blog entry.</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on">In MySQL Cluster 7.4 we have worked on improving the speed of restarts.<br />In addition we have made it easier to track how fast the restarts are<br />proceeding and to see which phases of the restart take a long time.<br /><br />For this purpose we have added a new Ndbinfo table in MySQL Cluster 7.4.<br />I will try to explain the restart phases in a series of blogs and how they map<br />to the columns in this Ndbinfo table.<br /><br />First some short intro to concepts in MySQL Cluster. LCP stands for local<br />checkpoints and this is where the main memory content of tables are<br />written to disk at regular intervals. There is also a GCP, global checkpoint,<br />this is a protocol to synchronise the REDO log writes and thus about<br />durability of transactions. LDM stands for Local Data Manager and is<br />the thread that runs all the code that maintains the actual data in MySQL<br />Cluster. This includes row storage, REDO logging, hash index, T-tree<br />index (ordered index) and various internal triggering functions to support<br />various high-level functionality such as unique indexes, foreign keys and<br />so forth.<br /><br />In a node restart there is a number of nodes involved, at a minimum there<br />are two nodes involved and there could be more than two nodes involved as<br />well. At least one node is a starting node and one live node is always a<br />master node. There can also be other live nodes that have already started and<br />that participate in some phases of the restart but not in all phases.<br />There can also be other starting nodes that have to be synchronised to<br />some extent with the starting node.<br /><br />A node restart always begins with a node failing. This could be a controlled<br />failure as is used in e.g. a rolling upgrade of the MySQL Cluster software.<br />In this case there are usually multiple nodes that are set to fail at the<br />same time, often one fails half the nodes as part of a software upgrade.<br /><br />So the first phase of the node restart phases is to complete the node failure.<br />This involves handling of in progress transactions involving the failed node.<br />It involves taking over as a master if the failed node was<br />a master node. It involves handling a number of protocols in the data nodes<br />where other nodes are waiting for the failed node. In 7.3 and older versions<br />this can at times be a fairly lengthy phase, in particular in taking over the<br />master role of the local checkpoint protocol. With the restart improvements<br />in MySQL Cluster 7.4 it should be a rare event that this phase takes a long<br />time.<br /><br />The next node restart phase is that the crashed node starts up again. The first<br />thing the starting node does is to attempt to allocate a node id. The allocation<br />of a node id will have to wait for the node failure phase to complete first. So<br />this means that this phase can take some time, but normally it proceeds very<br />quickly.<br /><br />When the new node has received a node id the next step is to be included in<br />the heartbeat protocol. One node can enter into the heartbeat protocol per<br />3 seconds. So if many nodes start up at the same time a node can be held<br />waiting here until the cluster accepts us into the cluster.<br /><br />Once the node have been included into the heartbeat protocol which is the<br />lowest node handling layer in MySQL Cluster, we will ask for permission<br />to get included in the cluster at the next level of maintenance. The node<br />could be blocked to be included at this level for example if other nodes are<br />still removing our node from local checkpoints after we did an initial node<br />restart, the node could also be blocked by include node protocols still going<br />on. So the node could be held in this layer at times, but it's a rare event and<br />should not happen very often.<br /><br />Also only one node at a time can be in this phase at a time, the next node will<br />be allowed to proceed after the node have completed this phase and the next<br />phase where we get a copy of the meta data in the cluster.<br /><br />After being accepted into the cluster the node have to wait until we have paused<br />the LCP protocol. This is a new feature of MySQL Cluster 7.4 that have been<br />added to avoid having to wait in this state until a local checkpoint is completed<br />before we can proceed. In 7.4 we can pause the checkpoint execution (for the<br />most part this will not block checkpoint execution since a major part of the<br />checkpoint is off-loaded to the LDM threads that can execute up to 64 local<br />checkpoint fragments without getting any new requests from the master node).<br /><br />As soon as the node managed to pause the local checkpoint execution we can<br />start to receive the meta data from the master node. The copy phase of the<br />meta data could take some time if there are very many tables in the cluster.<br />Normally it is a very quick action.<br /><br />Now that the node have copied the meta data to our node, the next node can<br />proceed with that phase. Our node will continue by being included into the global<br />checkpoint protocol and a number of other node handling protocols. This is<br />normally a quick phase.<br /><br />Now that the node have been included in the protocols the master node will<br />tell us what fragments to restore from disk. This phase is usually also fast.<br /><br />Now the node have reached the first phase that is normal to take a bit longer<br />time. This phase is where the node restores all requested fragments from a<br />checkpoint stored on disk. All LDM threads work in parallel on restoring<br />all requested fragments here. So the node should be able to restore at a speed<br />of at least a few million records per second here.<br /><br />After completing restoring the fragment checkpoints the next phase is to execute<br />the UNDO logs against all disk data fragments. If there are no disk data tables<br />in the cluster then this phase will be very quick, otherwise it can take some<br />time.<br /><br />After restoring the fragments and applying the disk data UNDO log the node is<br />now ready to apply the logical REDO log. All LDM threads will execute the<br />REDO logs in parallel. There are 4 phases of REDO log execution, but in reality<br />only the first phase is used. The time of this phase depends on how fast the<br />checkpoints were generated before the crash. The faster we execute the<br />checkpoints the smaller amount of REDO log is needed to execute as part of<br />a node restart.<br /><br />Also here the node should be able to process some millions of log records per<br />second since we are executing this in parallel in the LDM threads.<br /><br />Now that we have restored a global checkpoint for all fragments the node is<br />ready to rebuild all ordered indexes. The fragments are check pointed with<br />only its data, indexes are not check pointed. The indexes are rebuilt as part<br />of a node restart. The primary key hash index is rebuilt as part of restore of<br />the fragments and execution of the REDO log. The ordered index is rebuilt<br />in a separate phase after completing the applying of the REDO log.<br /><br />This is also a phase that can take some time. All LDM threads execute in<br />parallel in this phase. The more records and the more ordered indexes we<br />have the longer this phase takes to execute.<br /><br />Now the node has managed to restore an old but consistent version of the<br />fragments we were asked to restore.<br /><br />The next phase is to synchronise the old data with the live data in the<br />nodes that are up and running and contains the latest version of each<br />record.<br /><br />As a new feature of MySQL Cluster 7.4 this phase can be parallelised<br />among the LDM threads. So the node can synchronise with data in the<br />live nodes in many threads in parallel.<br /><br />This phase can be delayed in cases with many tables and many concurrent<br />node restarts since we need a lock on a fragment before copying its<br />meta data to new starting nodes which is also needed before starting<br />the copying phase to synchronise with the live nodes.<br /><br />At this point in time the node have an up-to-date version of all restored<br />fragments in the node. The node do however not have a durable version<br />yet. For the fragments to be restorable on our starting node we need the<br />node to participate in a full local checkpoint.<br /><br />Waiting for local checkpoints in various manners is the most common<br />activity in delaying restarts in 7.3 and older versions of MySQL Cluster.<br />In MySQL Cluster 7.4 we have managed to decrease the amount of<br />waiting for local checkpoints, in addition we have also been able to speed<br />up the local checkpointing in general and more specifically when node<br />restarts are executing. Those two things plus the parallelisation of the<br />synchronisation phase are the main ways of how we managed<br />to decrease restart times by a factor of 5.5X in MySQL Cluster 7.4.<br /><br />In order to ensure that as many nodes as possible get started together<br />after the waiting for the same local checkpoint execution, we have the<br />ability to block the start of a local checkpoint if we have nodes that are<br />close to reaching this wait state. Given that we have statistics on how long<br />time the restarts takes, we can predict if it is useful to wait for<br />another node to reach this state before we start a new local checkpoint.<br /><br />Finally when we have waited for a complete LCP it is time for the final<br />phase which is to wait for subscription handover to occur. This relates<br />to the MySQL replication support in MySQL Cluster where we have to<br />contact all MySQL Servers in the cluster informing them of our new<br />node that can also generate replication events.<br /><br />After this final phase the restart is completed.<br /><br />As an example we measured a node restart of around 26 GByte of data<br />took around 3-4 minutes in MySQL Cluster 7.4.<br /><br />The speed of node restarts is highly dependent on local checkpoint speed,<br />we have introduced a number of new configuration parameters to control<br />this speed, we have also introduced a new Ndbinfo table also to track the<br />checkpoint speed. We will describe this in a separate blog entry.<br /><br />Each of the phases described above is represented as a column in the<br />ndbinfo.restart_info table. We report the time that we spend in each<br />phase measured in seconds. There is a row for each restart that has<br />been observed. This means that not all nodes are present in this table.<br />After an initial start or after a cluster restart the table is even empty.<br /><br />We will also describe the additional log printouts that we have added to<br />the node logs describing what is going in the node during a node restart<br />in a separate blog entry.</div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989125&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989125&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 04 Mar 2015 08:06:00 +0000</pubDate>
    <dc:creator>Mikael Ronstr&amp;ouml;m</dc:creator>
  </item>

  <item>
    <title>MySQL Utilities 1.5</title>
    <guid isPermaLink="false">http://dev.mysql.com/downloads/utilities/1.5.html</guid>
    <link>http://dev.mysql.com/downloads/utilities/1.5.html</link>
    <description>MySQL Utilities 1.5 (1.5.4 GA, published on Wednesday, 04 Mar 2015)</description>
    <content:encoded><![CDATA[MySQL Utilities 1.5 (1.5.4 GA, published on Wednesday, 04 Mar 2015)<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=697250&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=697250&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 04 Mar 2015 00:00:00 +0000</pubDate>
    <dc:creator>MySQL</dc:creator>
  </item>

  <item>
    <title>Updating millions of rows in MySQL with common_schema</title>
    <guid isPermaLink="false">http://mechanics.flite.com/blog/2015/03/03/updating-millions-of-rows-in-mysql-with-common-schema</guid>
    <link>http://mechanics.flite.com/blog/2015/03/03/updating-millions-of-rows-in-mysql-with-common-schema/</link>
    <description>Last week I had to update several million rows in a single MySQL table. Rather that executing the update directly in the database, I decided to use common_schema's split() function.

There are two reasons I chose to use common_schema for this task:

Chunking

Chunking breaks a single update statement into multiple statements, each updating at most a certain specified number of rows. The default chunk size is 1000, and I changed it to 2000 by setting the size parameter.

Throttling

Throttling has two benefits: it minimizes the amount of load on the master host, and it minimizes the replication lag introduced when the update statements are executed on the replicas. It achieves this by introducing sleep statements after each chunk is updated. The duration of the sleep statement is proportional to the runtime of the update statement. After experimenting with several different throttle values in a test environment, I decided on a value of 4, which means it will sleep for 4 times as long as each update takes. Thus if an update takes 100 milliseconds, it will sleep for 400 milliseconds before executing the next update. Using throttle does not guarantee that you will completely avoid replication lag, but if you do some testing you should be able to find an appropriate value that will strike a balance between the total run time of your updates and the level of replication lag that it introduces.




If you want to learn more about throttling in common_schema you can read the documentation.

Here's the statement I executed in common_schema. I modified the SQL a bit for this post in order to make it more readable. The SELECT I do on the line below the throttle executes after each chunk and outputs a running total of how many rows have been updated:

```
set @script := &quot;
  split({size:2000} :
update ad_parameter
set scope = -1
where
(
  name in (
  'parameter_name_a',
  'parameter_name_b',
  'parameter_name_c'
  )
  or name like 'parameter_prefix_x_%'
  or name like 'parameter_prefix_y_%'
  or name like 'parameter_prefix_z_%'
)
and scope != -1
  )
{
  throttle 4;
  SELECT $split_total_rowcount AS 'rows updated so far';
}
&quot;;

call common_schema.run(@script);
```</description>
    <content:encoded><![CDATA[<p>Last week I had to update several million rows in a single MySQL table. Rather that executing the update directly in the database, I decided to use <a href="https://code.google.com/p/common-schema/">common_schema</a>'s <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html"><code>split()</code></a> function.</p>

<p>There are two reasons I chose to use <a href="https://code.google.com/p/common-schema/">common_schema</a> for this task:</p>

<h2>Chunking</h2>

<p>Chunking breaks a single update statement into multiple statements, each updating <em>at most</em> a certain specified number of rows. The default chunk size is 1000, and I changed it to 2000 by setting the <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_split.html#parameters"><code>size</code></a> parameter.</p>

<h2>Throttling</h2>

<p>Throttling has two benefits: it minimizes the amount of load on the master host, and it minimizes the replication lag introduced when the update statements are executed on the replicas. It achieves this by introducing sleep statements after each chunk is updated. The duration of the sleep statement is proportional to the runtime of the update statement. After experimenting with several different <code>throttle</code> values in a test environment, I decided on a value of 4, which means it will sleep for 4 times as long as each update takes. Thus if an update takes 100 milliseconds, it will sleep for 400 milliseconds before executing the next update. Using <code>throttle</code> does not guarantee that you will completely avoid replication lag, but if you do some testing you should be able to find an appropriate value that will strike a balance between the total run time of your updates and the level of replication lag that it introduces.</p>

<!--more-->


<p>If you want to learn more about throttling in <a href="https://code.google.com/p/common-schema/">common_schema</a> you can read the <a href="http://common-schema.googlecode.com/svn/trunk/common_schema/doc/html/query_script_throttle.html">documentation</a>.</p>

<p>Here's the statement I executed in <a href="https://code.google.com/p/common-schema/">common_schema</a>. I modified the SQL a bit for this post in order to make it more readable. The <code>SELECT</code> I do on the line below the <code>throttle</code> executes after each chunk and outputs a running total of how many rows have been updated:</p>

<p>```
set @script := "
  split({size:2000} :
update ad_parameter
set scope = -1
where
(
  name in (
  'parameter_name_a',
  'parameter_name_b',
  'parameter_name_c'
  )
  or name like 'parameter_prefix_x_%'
  or name like 'parameter_prefix_y_%'
  or name like 'parameter_prefix_z_%'
)
and scope != -1
  )
{
  throttle 4;
  SELECT $split_total_rowcount AS 'rows updated so far';
}
";</p>

<p>call common_schema.run(@script);
```</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989119&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989119&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 22:42:00 +0000</pubDate>
    <dc:creator>Ike Walker</dc:creator>
  </item>

  <item>
    <title>Introducing ‘MySQL 101,’ a 2-day intensive educational track at Percona Live this April 15-16</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28914</guid>
    <link>http://www.percona.com/blog/2015/03/03/introducing-mysql-101-a-2-day-intensive-educational-track-at-percona-live/</link>
    <description>Talking with Percona Live attendees last year I heard a couple of common themes. First, people told me that there is a lot of great advanced content at Percona Live but there is not much for people just starting to learn the ropes with MySQL. Second, they would like us to find a way to make such basic content less expensive.I’m pleased to say we’re able to accommodate both of these wishes this year at Percona Live! We have created a two-day intensive track called “MySQL 101” that runs April 15-16. MySQL 101 is designed for developers, system administrators and DBAs familiar with other databases but not with MySQL. And of course it’s ideal for anyone else who would like to expand their professional experience to include MySQL. The sessions are designed to lay a solid foundation on many aspects of MySQL development, design and operations.As for the price: Just $101 for both full days, but only if you are among the first 101 people to register using the promo code “101” at checkout.  After that the price returns to $400 (still a great price!). The MySQL 101 registration pass includes full access to the Percona Live expo hall (and all the fun stuff happening out there) as well as keynotes, which will inform you about most significant achievements in MySQL ecosystem.As there is so much information to cover in the MySQL 101 track, we’re running two sessions in parallel &amp;#8211; one geared more toward developers using MySQL and the other toward sysadmins and MySQL DBAs, focusing more on database operations. Though I want to point out that you do not have to chose one track to attend exclusively, but rather can mix and match sessions depending what is most relevant to your specific circumstances.I will be leading a couples tracks myself alongside many other Percona experts who are joining me for those two days!Here’s a peek at just some of the many classes on the MySQL 101 agenda:     The 7 deadly sins of MySQL performance     Introduction to High Availability options for MySQL     Becoming a DBA for 5 minutes a day: The most important things to know for sysadmins and developers     Designing Effective Schema for InnoDB     MySQL Security Basics     MySQL backups: strategy, tools, recovery scenarios     Highly efficient MySQL backups     How to create a useful MySQL bug report     MySQL &amp;amp; InnoDB fundamentals and configuration     MySQL indexing best practices     MySQL query tuning 101     MySQL replication: setup, benefits, limitations     Overview of MySQL connectors for PHP     Practical MySQL troubleshooting and performance optimization   Running MySQL in OpenStack     Using MySQL with Java     Writing High Performance SQL StatementsYou can see the full MySQL 101 agenda here. Don&amp;#8217;t forget the promo code &amp;#8220;101&amp;#8221; and please feel free to ask any questions below. I hope to see you in Santa Clara at Percona Live! The conference runs April 13-16 in sunny Santa Clara, California. The post Introducing &amp;#8216;MySQL 101,&amp;#8217; a 2-day intensive educational track at Percona Live this April 15-16 appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p>Talking with Percona Live attendees last year I heard a couple of common themes. First, people told me that there is a lot of great advanced content at Percona Live but there is not much for people just starting to learn the ropes with MySQL. Second, they would like us to find a way to make such basic content less expensive.</p><p>I’m pleased to say we’re able to accommodate both of these wishes this year at Percona Live! We have created a two-day intensive track called “<a href="http://www.percona.com/live/mysql-conference-2015/program/mysql-101" target="_blank">MySQL 101</a>” that runs April 15-16. MySQL 101 is designed for developers, system administrators and DBAs familiar with other databases but not with MySQL. And of course it’s ideal for anyone else who would like to expand their professional experience to include MySQL. The sessions are designed to lay a solid foundation on many aspects of MySQL development, design and operations.</p><p>As for the price: Just $101 for both full days, but only if you are among the first 101 people to register using the promo code “101” at <a href="http://www.percona.com/live/mysql-conference-2015/registration" target="_blank">checkout</a>.  After that the price returns to $400<em> (still a great price!)</em>. <img src="http://www.percona.com/blog/wp-includes/images/smilies/icon_smile.gif" alt=":)" class="wp-smiley" /></p><p>The MySQL 101 registration pass includes full access to the Percona Live expo hall <em>(and all the fun stuff happening out there)</em> as well as keynotes, which will inform you about most significant achievements in MySQL ecosystem.</p><p><img class="alignright wp-image-28919 size-medium" src="http://www.percona.com/blog/wp-content/uploads/2015/03/MySQL-101-Percona-Live-2015-300x157.jpg" alt="MySQL 101 Percona Live 2015" width="300" height="157" />As there is so much information to cover in the MySQL 101 track, we’re running two sessions in parallel &#8211; one geared more toward developers using MySQL and the other toward sysadmins and MySQL DBAs, focusing more on database operations. Though I want to point out that you do not have to chose one track to attend exclusively, but rather can mix and match sessions depending what is most relevant to your specific circumstances.</p><p>I will be leading a couples tracks myself alongside many other Percona experts who are joining me for those two days!</p><p>Here’s a peek at just some of the many classes on the MySQL 101 agenda:</p><ul><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/7-or-so-deadly-mysql-performance-sins">The 7 deadly sins of MySQL performance</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/introduction-high-availability-options-mysql">Introduction to High Availability options for MySQL</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/becoming-dba-5-minutes-day-most-important-things-know-sysadmins-and-developers">Becoming a DBA for 5 minutes a day: The most important things to know for sysadmins and developers</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/designing-effective-schema-innodb">Designing Effective Schema for InnoDB</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/hardening-mysql-mysql-security-basics">MySQL Security Basics</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/mysql-backups-strategy-tools-recovery-scenarios">MySQL backups: strategy, tools, recovery scenarios</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/highly-efficient-backups-percona-xtrabackup">Highly efficient MySQL backups</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/how-create-useful-mysql-bug-report">How to create a useful MySQL bug report</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/mysql-innodb-fundamentals-and-configuration">MySQL &amp; InnoDB fundamentals and configuration</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/mysql-indexing-best-practices">MySQL indexing best practices</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/mysql-query-tuning-101">MySQL query tuning 101</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/mysql-replication-setup-benefits-limitations">MySQL replication: setup, benefits, limitations</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/overview-mysql-connectors-php">Overview of MySQL connectors for PHP</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/practical-mysql-troubleshooting-and-performance-optimization-percona-cloud-tools">Practical MySQL troubleshooting and performance optimization</a></li><li>   <a href="http://www.percona.com/live/mysql-conference-2015/sessions/running-mysql-openstack-environment">Running MySQL in OpenStack</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/using-mysql-java">Using MySQL with Java</a></li><li>     <a href="http://www.percona.com/live/mysql-conference-2015/sessions/writing-high-performance-sql-statements">Writing High Performance SQL Statements</a></li></ul><p>You can see the full MySQL 101 agenda <a href="http://www.percona.com/live/mysql-conference-2015/program/mysql-101" target="_blank">here</a>. Don&#8217;t forget the promo code &#8220;101&#8221; and please feel free to ask any questions below. I hope to see you in Santa Clara at Percona Live! The conference runs April 13-16 in sunny Santa Clara, California.<i><br
/> </i></p><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/03/03/introducing-mysql-101-a-2-day-intensive-educational-track-at-percona-live/">Introducing &#8216;MySQL 101,&#8217; a 2-day intensive educational track at Percona Live this April 15-16</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989117&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989117&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 17:18:23 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>InnoDB</category>
    <category>MySQL</category>
    <category>OpenStack</category>
    <category>Percona Live</category>
    <category>Backups</category>
    <category>indexing</category>
    <category>Java</category>
    <category>MySQL 101</category>
    <category>MySQL classes</category>
    <category>percona live</category>
    <category>Performance optimization</category>
    <category>Peter Zaitsev</category>
    <category>PHP connectors</category>
    <category>query tuning</category>
    <category>Replication</category>
    <category>security</category>
    <category>SQL statements</category>
    <category>troubleshooting</category>
  </item>

  <item>
    <title>Proposal to deprecate &amp;quot;compatibility&amp;quot; SQL Modes</title>
    <guid isPermaLink="false">http://www.tocker.ca/?p=596</guid>
    <link>http://www.tocker.ca/2015/03/03/proposal-to-deprecate-compatibility-sql-modes.html</link>
    <description>In the MySQL team, we are currently discussing deprecating several of the SQL mode options which are used by mysqldump to change the output format.  From the mysqldump command:

$ mysqldump --help
..
--compatible=name   Change the dump to be compatible with a given mode. By
                    default tables are dumped in a format optimized for
                    MySQL. Legal modes are: ansi, mysql323, mysql40,
                    postgresql, oracle, mssql, db2, maxdb, no_key_options,
                    no_table_options, no_field_options. One can use several
                    modes separated by commas. Note: Requires MySQL server
                    version 4.1.0 or higher. This option is ignored with
                    earlier server versions.

To explain the rationale for this proposal:

The options mysql323, mysql40 are designed to allow mysqldump to create an output format that can be restored on a MySQL Server of version 3.23 or 4.0.  While we aim to support the upgrade case from these versions, supporting a downgrade is not something we support, as restoring data to a 10 year old release poses a number of challenges.
The options postgresql, oracle, mssql, db2, maxdb are 'special' SQL modes, in that they are not really modes themselves but aliases to switch on other SQL modes.  For example:

mysql&gt; set sql_mode = 'mssql';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; select @@sql_mode\G
*************************** 1. row ***************************
@@sql_mode: PIPES_AS_CONCAT,ANSI_QUOTES,IGNORE_SPACE,MSSQL,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_FIELD_OPTIONS
1 row in set (0.00 sec)

Having aliases creates a number of interesting challenges:

It complicates training and support as users may have multiple options in order to achieve the same outcome (either listing all sql modes individually or via one of the aliases).  In a similar example we removed unique option prefixes in MySQL 5.7 to reduce this confusion.
The options that we enable for each alias imply (but have not measurably offered) compatibility with the other database that is mentioned.  Furthermore, as other databases will release newer versions, the singularity of the alias name does not account for this.
Related to the above; if newer versions of other database products desire new sql modes enabled, it is more flexible (and future proof) to have the list of which behaviour options that should be enabled for each other database in documentation or client programs rather than the server itself.  This allows us to not change behavior in a server GA release.


The options no_key_options, no_field_options and no_table_options remove MySQL-specific meta data which I have highlighted below in bold:

# no_key_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL AUTO_INCREMENT,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'
) ENGINE=InnoDB DEFAULT CHARSET=latin1

# no_field_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL AUTO_INCREMENT,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'
) ENGINE=InnoDB DEFAULT CHARSET=latin1

# no_table_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL AUTO_INCREMENT,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'
) ENGINE=InnoDB DEFAULT CHARSET=latin1

Note that no_field_options did not remove the character set attribute for the column b and thus is incomplete in its current implementation.  no_field_options also disables the same meta-data which is disabled by no_key_options.
The no_key_options and no_table_options remain useful, although it should be noted that information_schema may better support a custom view of schema to match the capabilities of the destined database.  We are working on making information_schema queries much faster in the future, via our native data dictionary project.


To summarize this proposal:

Our plan is to deprecate the following options in MySQL 5.7, for removal in a later version:
mysql323, mysql40, postgresql, oracle, mssql, db2, maxdb.
We are also seeking input from those that use the following SQL modes, as we may decide to deprecate these in the future:
no_field_options, no_key_options, no_table_options.

Will these changes impact you?  Please leave a comment or get in touch!</description>
    <content:encoded><![CDATA[<p>In the MySQL team, we are currently discussing deprecating several of the SQL mode options which are used by mysqldump to change the output format.  From the <tt>mysqldump</tt> command:</p>
<pre>
$ mysqldump --help
..
--compatible=name   Change the dump to be compatible with a given mode. By
                    default tables are dumped in a format optimized for
                    MySQL. Legal modes are: ansi, mysql323, mysql40,
                    postgresql, oracle, mssql, db2, maxdb, no_key_options,
                    no_table_options, no_field_options. One can use several
                    modes separated by commas. Note: Requires MySQL server
                    version 4.1.0 or higher. This option is ignored with
                    earlier server versions.
</pre>
<h3>To explain the rationale for this proposal:</h3>
<ol>
<li>The options <tt>mysql323</tt>, <tt>mysql40</tt> are designed to allow mysqldump to create an output format that can be restored on a MySQL Server of version 3.23 or 4.0.  While we aim to support the <em>upgrade</em> case from these versions, supporting a <em>downgrade</em> is not something we support, as restoring data to a 10 year old release poses a number of challenges.</li>
<li>The options <tt>postgresql, oracle, mssql, db2, maxdb</tt> are 'special' SQL modes, in that they are not really modes themselves but aliases to switch on other SQL modes.  For example:
<pre>
mysql> set sql_mode = 'mssql';
Query OK, 0 rows affected (0.00 sec)

mysql> select @@sql_mode\G
*************************** 1. row ***************************
@@sql_mode: PIPES_AS_CONCAT,ANSI_QUOTES,IGNORE_SPACE,MSSQL,NO_KEY_OPTIONS,NO_TABLE_OPTIONS,NO_FIELD_OPTIONS
1 row in set (0.00 sec)
</pre>
<p>Having aliases creates a number of interesting challenges:</p>
<ul>
<li>It complicates training and support as users may have multiple options in order to achieve the same outcome (either listing all sql modes individually or via one of the aliases).  In a similar example we removed <a href="http://dev.mysql.com/worklog/task/?id=6978">unique option prefixes</a> in MySQL 5.7 to reduce this confusion.</li>
<li>The options that we enable for each alias imply (but have not measurably offered) compatibility with the other database that is mentioned.  Furthermore, as other databases will release newer versions, the singularity of the alias name does not account for this.</li>
<li>Related to the above; if newer versions of other database products desire new sql modes enabled, it is more flexible (and future proof) to have the list of which behaviour options that should be enabled for each other database in documentation or client programs rather than the server itself.  This allows us to not change behavior in a server GA release.</li>
</ul>
</li>
<li>The options <tt>no_key_options</tt>, <tt>no_field_options</tt> and <tt>no_table_options</tt> remove MySQL-specific meta data which I have highlighted below in bold:
<pre>
# no_key_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL AUTO_INCREMENT,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) <strong>USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'</strong>
) ENGINE=InnoDB DEFAULT CHARSET=latin1

# no_field_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL <strong>AUTO_INCREMENT</strong>,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) <strong>USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'</strong>
) ENGINE=InnoDB DEFAULT CHARSET=latin1

# no_table_options
CREATE TABLE `t` (
  `i` int(11) NOT NULL AUTO_INCREMENT,
  `b` varchar(20) CHARACTER SET utf8 DEFAULT NULL,
  KEY `i` (`i`) USING BTREE KEY_BLOCK_SIZE=2048 COMMENT 'My comment'
) <strong>ENGINE=InnoDB DEFAULT CHARSET=latin1</strong>
</pre>
<p>Note that <tt>no_field_options</tt> did not remove the character set attribute for the column <tt>b</tt> and thus is incomplete in its current implementation.  <tt>no_field_options</tt> also disables the same meta-data which is disabled by <tt>no_key_options</tt>.</p>
<p>The <tt>no_key_options</tt> and <tt>no_table_options</tt> remain useful, although it should be noted that <tt>information_schema</tt> may better support a custom view of schema to match the capabilities of the destined database.  We are working on making <tt>information_schema</tt> queries much faster in the future, via our <a href="http://www.tocker.ca/2014/07/30/beyond-the-frm-ideas-for-a-native-mysql-data-dictionary.html">native data dictionary project</a>.</p>
</li>
</ol>
<h3>To summarize this proposal:</h3>
<ul>
<li><strong>Our plan is to deprecate the following options in MySQL 5.7, for removal in a later version:</strong><br />
mysql323, mysql40, postgresql, oracle, mssql, db2, maxdb.</li>
<li><strong>We are also seeking input from those that use the following SQL modes, as we may decide to deprecate these in the future:</strong><br />
no_field_options, no_key_options, no_table_options.</li>
</ul>
<p>Will these changes impact you?  Please leave a comment or <a href="http://www.tocker.ca/contact/">get in touch!</a></p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989116&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989116&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 16:31:28 +0000</pubDate>
    <dc:creator>Morgan Tocker</dc:creator>
    <category>Community</category>
  </item>

  <item>
    <title>Improving Percona XtraDB Cluster SST startup with Google Compute Engine snapshots</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28748</guid>
    <link>http://www.percona.com/blog/2015/03/03/improving-percona-xtradb-cluster-sst-startup-google-cloud-engine-snapshots/</link>
    <description>As the need for information grows so does the size of data we need to keep in our databases. SST is unavoidable for spinning up new nodes in a PXC cluster and when datasets reach the &amp;#8220;terra-byte&amp;#8221; range this becomes ever more cumbersome requiring many hours for a new node to synchronize.More often that not, it is necessary to implement custom &amp;#8220;wsrep_sst&amp;#8221; scripts or resort to manual synchronization processes. Luckily cloud providers provide convenient methods to leverage disk snapshots that can be used to quickly transfer data between nodes.This article deals with the actions needed to perform a snapshot on Google&amp;#8217;s Compute Engine (GCE) infrastructure. A similar method can be used on AWS EC2 instances using EBS snapshots or any other form of snapshots such as LVM, ZFS or SAN. The steps described can be used to add a new node to a PXC cluster or to avoid SST. The following procedure can also be used to take advantage of the performance benefit of GCE Snapshots. A similar procedure can be used for adding a regular slave provided the binary log co-ordinates have been captured. This article assumes your &amp;#8220;datadir&amp;#8221; is on a separate disk to your operating system partition using the &amp;#8220;ext4&amp;#8243; filesystem:Select a suitable &amp;#8220;donor&amp;#8221; node, we will use &amp;#8220;node1&amp;#8243; for this example.Stop the MySQL service on &amp;#8220;node1&amp;#8243; or perform a FTWRL with the MySQL service running on a node which is in &amp;#8220;desync/donor&amp;#8221; mode# Take the snapshot from a stopped instance
[root@node1 /] service mysql stop &amp;amp; tail -f /var/log/mysql/error.log
 
# OR alternatively take the snapshot from a 'desynced' node
 
### desync from cluster replication
mysql&amp;gt; set global wsrep_desync=ON; 
 
### get FTWRL
mysql&amp;gt; flush tables with read lock;While the MySQL service is down on &amp;#8220;node1&amp;#8243; or the FTWRL is held create a snapshot in the Google Developer Console for the disk or using the GCE API (* this assumes that the datadir is located in a separate standalone disk). This part of the process takes around 15 minutes for a 3.5 TB disk.gcloud compute disks snapshot node1-datadir-disk --snapshot-name node1-datadir-disk-snapshot-1As soon as the snapshot has completed start the MySQL service on &amp;#8220;node1&amp;#8243; (verifying the node has successfully joined the cluster) or release the FTWRL# Depending on the steps followed in step 1 either start MySQL on node1
[root@node1 /] service mysql start &amp;amp; tail -f /var/log/mysql/error.log
 
# OR alternatively release the FTWRL and &quot;sync&quot; the node
 
### release FTWRL
mysql&amp;gt; unlock tables;
 
### if there is high load on the cluster monitor wsrep_local_recv_queue 
### until it reaches 0 before running the following command to rejoin 
### the cluster replication (otherwise it can be run immediately after
### releasing the FTWRL):
mysql&amp;gt; set global wsrep_desync=OFF; ***** IMPORTANT NOTE: In case &amp;#8220;node1&amp;#8243; is unable to rejoin the cluster or requires an SST you will need to re-create the snapshot from another node or after SST completes.Now connect to the &amp;#8220;joiner&amp;#8221; node, we will use &amp;#8220;node2&amp;#8243; for this example.Unmount the existing disk from &amp;#8220;node2&amp;#8243; for this example (assuming MySQL service is not running else stop the MySQL service first)[root@node2 /] umount /var/lib/mysqlDetach and delete the disk containing the MySQL datadir from the &amp;#8220;node2&amp;#8243; instance in the Google Developer Console or using the GCE APIgcloud compute instances detach-disk node2 --disk node2-datadir-disk
gcloud compute disks delete node2-datadir-diskCreate and attach a new disk to the &amp;#8220;node2&amp;#8243; instance in the Google Developer Console or using the GCE API using the snapshot you created in step 3. This part of the process takes around 10 minutes for a 3.5 TB diskgcloud compute disks create node2-datadir-disk --source-snapshot node1-datadir-disk-snapshot-1
gcloud compute instance attach-disk node2 --disk node2-datadir-disk[ *** LVM only step *** ]: If you are using LVM the device will not show up in this list until you have activated the Volume Group (&amp;#8220;vg_mysql_data&amp;#8221; in this example)# this command will report the available volume groups
[root@node2 /] vgscan
  Reading all physical volumes.  This may take a while...
  Found volume group &quot;vg_mysql_data&quot; using metadata type lvm2
 
# this command will report the available logical volumes, you should see the LV INACTIVE now
[root@node2 /] lvscan
  INACTIVE            '/dev/vg_mysql_data/lv_mysql' [20.00 TiB] inherit
 
# this command will activate all logical volumes within the volume group
[root@node2 /] vgchange -ay vg_mysql_data
 
# this command will report the available logical volumes, you should see the LV ACTIVE now
[root@node2 /] lvscan
  ACTIVE            '/dev/vg_mysql_data/lv_mysql' [20.00 TiB]After the device has been added it should show up on the &amp;#8220;node2&amp;#8243; operating system &amp;#8211; you can retrieve the new UUID using the following command (in case you have mounted using &amp;#8220;/dev/disk/by-name&amp;#8221; and the name of the new disk is the same as the previous you do not need to update &amp;#8220;/etc/fstab&amp;#8221; e.g. this holds true for VM instances created using the Percona XtraDB click-to-deploy installer)[root@node2 /] ls -l /dev/disk/by-uuid/
total 0
lrwxrwxrwx 1 root root 10 Feb 14 15:56 4ad2d22b-500a-4ad2-b929-12f38347659c -&amp;gt; ../../sda1
lrwxrwxrwx 1 root root 10 Feb 19 03:12 9e48fefc-960c-456f-95c9-9d893bcafc62 -&amp;gt; ../../dm-0   # This is the 'new' disk You can now proceed to adding the new UUID you retrieved in step 9 to &amp;#8220;/etc/fstab&amp;#8221; (unless you are using &amp;#8220;/dev/disk/by-name&amp;#8221; with the same disk name) and mount the new disk[root@node2 /] vi /etc/fstab
...
UUID=9e48fefc-960c-456f-95c9-9d893bcafc62 /var/lib/mysql ext4 defaults,noatime 0 0
...
 
[root@node2 /] mount -aVerify the data is mounted correctly and the ownership of the data directory and sub-contents are using the correct UID / GID for the MySQL user on the destination system (although this is usually OK, it is good to do a quick check)[root@node2 /] ls -lhtR /var/lib/mysql/You are now ready to start MySQL and verify that the node has in fact initialised with IST (provided you have sufficient &amp;#8220;gcache&amp;#8221; available there shouldn&amp;#8217;t be any other issues)[root@node2 /] service mysql start &amp;amp; tail -f /var/log/mysql/error.logThe Percona XtraDB Click-to-deploy tool can be used for automated deployments and further details on creating a cluster on Google Compute Engine using this method can be found in Jay Janssen&amp;#8217;s post, &amp;#8220;Google Compute Engine adds Percona XtraDB Cluster to click-to-deploy process.&amp;#8221;&amp;nbsp; The post Improving Percona XtraDB Cluster SST startup with Google Compute Engine snapshots appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<article><div><p>As the need for information grows so does the size of data we need to keep in our databases. SST is unavoidable for spinning up new nodes in a <a href="http://www.percona.com/software/percona-xtradb-cluster" target="_blank">PXC</a> cluster and when datasets reach the &#8220;terra-byte&#8221; range this becomes ever more cumbersome requiring many hours for a new node to synchronize.</p><p>More often that not, it is necessary to implement custom &#8220;wsrep_sst&#8221; scripts or resort to manual synchronization processes. Luckily cloud providers provide convenient methods to leverage disk snapshots that can be used to quickly transfer data between nodes.</p><p>This article deals with the actions needed to perform a snapshot on Google&#8217;s Compute Engine (GCE) infrastructure. A similar method can be used on AWS EC2 instances using EBS snapshots or any other form of snapshots such as LVM, ZFS or SAN. The steps described can be used to add a new node to a PXC cluster or to avoid SST. The following procedure can also be used to take advantage of the performance benefit of GCE Snapshots. A similar procedure can be used for adding a regular slave provided the binary log co-ordinates have been captured. This article assumes your &#8220;datadir&#8221; is on a separate disk to your operating system partition using the &#8220;ext4&#8243; filesystem:</p><ol><li>Select a suitable &#8220;donor&#8221; node, we will use &#8220;node1&#8243; for this example.</li><li>Stop the MySQL service on &#8220;node1&#8243; or perform a FTWRL with the MySQL service running on a node which is in &#8220;desync/donor&#8221; mode<br
/><pre># Take the snapshot from a stopped instance
[root@node1 /] service mysql stop &amp; tail -f /var/log/mysql/error.log
 
# OR alternatively take the snapshot from a 'desynced' node
 
### desync from cluster replication
mysql&gt; set global wsrep_desync=ON; 
 
### get FTWRL
mysql&gt; flush tables with read lock;</pre></li><li>While the MySQL service is down on &#8220;node1&#8243; or the FTWRL is held create a snapshot in the Google Developer Console for the disk or using the GCE API (* this assumes that the datadir is located in a separate standalone disk). This part of the process takes around 15 minutes for a 3.5 TB disk.<br
/><pre>gcloud compute disks snapshot node1-datadir-disk --snapshot-name node1-datadir-disk-snapshot-1</pre></li><li>As soon as the snapshot has completed start the MySQL service on &#8220;node1&#8243; (verifying the node has successfully joined the cluster) or release the FTWRL<br
/><pre># Depending on the steps followed in step 1 either start MySQL on node1
[root@node1 /] service mysql start &amp; tail -f /var/log/mysql/error.log
 
# OR alternatively release the FTWRL and "sync" the node
 
### release FTWRL
mysql&gt; unlock tables;
 
### if there is high load on the cluster monitor wsrep_local_recv_queue 
### until it reaches 0 before running the following command to rejoin 
### the cluster replication (otherwise it can be run immediately after
### releasing the FTWRL):
mysql&gt; set global wsrep_desync=OFF;</pre><br
/> <strong>***** IMPORTANT NOTE:</strong> In case &#8220;node1&#8243; is unable to rejoin the cluster or requires an SST you will need to re-create the snapshot from another node or after SST completes.</li><li>Now connect to the &#8220;joiner&#8221; node, we will use &#8220;node2&#8243; for this example.</li><li>Unmount the existing disk from &#8220;node2&#8243; for this example (assuming MySQL service is not running else stop the MySQL service first)<div><div><div><pre>[root@node2 /] umount /var/lib/mysql</pre></div></div></div></li><li>Detach and delete the disk containing the MySQL datadir from the &#8220;node2&#8243; instance in the Google Developer Console or using the GCE API<br
/><pre>gcloud compute instances detach-disk node2 --disk node2-datadir-disk
gcloud compute disks delete node2-datadir-disk</pre></li><li>Create and attach a new disk to the &#8220;node2&#8243; instance in the Google Developer Console or using the GCE API using the snapshot you created in step 3. This part of the process takes around 10 minutes for a 3.5 TB disk<br
/><pre>gcloud compute disks create node2-datadir-disk --source-snapshot node1-datadir-disk-snapshot-1
gcloud compute instance attach-disk node2 --disk node2-datadir-disk</pre></li><li>[ *** LVM only step *** ]: If you are using LVM the device will not show up in this list until you have activated the Volume Group (&#8220;vg_mysql_data&#8221; in this example)<div><div><div><pre># this command will report the available volume groups
[root@node2 /] vgscan
  Reading all physical volumes.  This may take a while...
  Found volume group "vg_mysql_data" using metadata type lvm2
 
# this command will report the available logical volumes, you should see the LV INACTIVE now
[root@node2 /] lvscan
  INACTIVE            '/dev/vg_mysql_data/lv_mysql' [20.00 TiB] inherit
 
# this command will activate all logical volumes within the volume group
[root@node2 /] vgchange -ay vg_mysql_data
 
# this command will report the available logical volumes, you should see the LV ACTIVE now
[root@node2 /] lvscan
  ACTIVE            '/dev/vg_mysql_data/lv_mysql' [20.00 TiB]</pre></div></div></div></li><li>After the device has been added it should show up on the &#8220;node2&#8243; operating system &#8211; you can retrieve the new UUID using the following command (in case you have mounted using &#8220;/dev/disk/by-name&#8221; and the name of the new disk is the same as the previous you do not need to update &#8220;/etc/fstab&#8221; e.g. this holds true for VM instances created using the Percona XtraDB click-to-deploy installer)<div><div><div><pre>[root@node2 /] ls -l /dev/disk/by-uuid/
total 0
lrwxrwxrwx 1 root root 10 Feb 14 15:56 4ad2d22b-500a-4ad2-b929-12f38347659c -&gt; ../../sda1
lrwxrwxrwx 1 root root 10 Feb 19 03:12 9e48fefc-960c-456f-95c9-9d893bcafc62 -&gt; ../../dm-0   # This is the 'new' disk</pre></div></div></div></li><li> You can now proceed to adding the new UUID you retrieved in step 9 to &#8220;/etc/fstab&#8221; (unless you are using &#8220;/dev/disk/by-name&#8221; with the same disk name) and mount the new disk<div><div><div><pre>[root@node2 /] vi /etc/fstab
...
UUID=9e48fefc-960c-456f-95c9-9d893bcafc62 /var/lib/mysql ext4 defaults,noatime 0 0
...
 
[root@node2 /] mount -a</pre></div></div></div></li><li>Verify the data is mounted correctly and the ownership of the data directory and sub-contents are using the correct UID / GID for the MySQL user on the destination system (although this is usually OK, it is good to do a quick check)<div><div><div><pre>[root@node2 /] ls -lhtR /var/lib/mysql/</pre></div></div></div></li><li>You are now ready to start MySQL and verify that the node has in fact initialised with IST (provided you have sufficient &#8220;gcache&#8221; available there shouldn&#8217;t be any other issues)<div><div><div><pre>[root@node2 /] service mysql start &amp; tail -f /var/log/mysql/error.log</pre></div></div></div></li></ol></div><p>The Percona XtraDB Click-to-deploy tool can be used for automated deployments and further details on creating a cluster on Google Compute Engine using this method can be found in Jay Janssen&#8217;s post, &#8220;<a href="http://www.percona.com/blog/2014/11/21/google-compute-engine-adds-percona-xtradb-cluster-to-click-to-deploy-process/" target="_blank">Google Compute Engine adds Percona XtraDB Cluster to click-to-deploy process</a>.&#8221;</p><p>&nbsp;</p> </article><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/03/03/improving-percona-xtradb-cluster-sst-startup-google-cloud-engine-snapshots/">Improving Percona XtraDB Cluster SST startup with Google Compute Engine snapshots</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989113&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989113&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 14:36:00 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>Cloud and MySQL</category>
    <category>MySQL</category>
    <category>Percona XtraDB Cluster</category>
    <category>Google Compute Engine</category>
    <category>Google Developer Console</category>
    <category>Nik Vyzas</category>
    <category>Primary</category>
    <category>pxc</category>
    <category>SST</category>
  </item>

  <item>
    <title>MySQL Character encoding – part 2</title>
    <guid isPermaLink="false">http://www.psce.com/blog/?p=3000</guid>
    <link>http://www.psce.com/blog/2015/03/03/mysql-character-encoding-part-2/</link>
    <description>In MySQL Character encoding &amp;#8211; part 1 we stated that the myriad of ways in which character encoding can be controlled can lead to many situations where your data may not be available as expected.
UTF8 was designed on a placemat in a New Jersey diner one night in September or so 1992.

Setting MySQL Client and Server Character encoding.
Lets restart MySQL with the correct setting for our purpose, UTF8. Here we can see the setting in the MySQL configuration file, in this case /etc/mysql/my.cnf.
character-set-server = utf8

This change is then reflected in the session and global variables once the instance is restarted with the new configuration parameter.
mysql&amp;gt; SELECT @@global.character_set_server, @@session.character_set_client;
+-------------------------------+--------------------------------+
| @@global.character_set_server | @@session.character_set_client | 
+-------------------------------+--------------------------------+
| utf8                          | utf8                           | 
+-------------------------------+--------------------------------+ 
1 row in set (0.00 sec)

Now we have verified the server and client are set to use UTF8, we can go ahead, continue developing our application and create a new table people.
mysql&amp;gt; CREATE TABLE people (first_name VARCHAR(30) NOT NULL, 
                             last_name VARCHAR(30) NOT NULL); 
Query OK, 0 rows affected (0.13 sec)

Now let&amp;#8217;s enter some data into the new table, which was created with the server and client configured for UTF8.


Something appears to have gone terribly wrong, the accent in Maciek&amp;#8217;s surname now appears as a question mark.
mysql&amp;gt; SELECT @@session.character_set_server, @@session.character_set_client; 
+--------------------------------+--------------------------------+
| @@session.character_set_server | @@session.character_set_client | 
+--------------------------------+--------------------------------+
| utf8                           | utf8                           | 
+--------------------------------+--------------------------------+ 
1 row in set (0.00 sec)

The database settings are still UTF8, this should have worked.
mysql&amp;gt; USE fosdem;
mysql&amp;gt; SHOW CREATE TABLE people\G
*************************** 1. row ***************************
Table: people
Create Table: CREATE TABLE `people` (
`first_name` varchar(30) NOT NULL,
`last_name` varchar(30) NOT NULL
) ENGINE=InnoDB DEFAULT CHARSET=latin1 1 row in set (0.00 sec)

Looking at the table, we see that despite being created under a server set to use UTF8, it appears to be set to use latin1.
How can this be?, Let&amp;#8217;s look at the session settings.
￼mysql&amp;gt; SHOW SESSION VARIABLES LIKE 'character_set_%'; 
+--------------------------+----------------------------+ 
| Variable_name            | Value                      | 
+--------------------------+----------------------------+
| character_set_client     | utf8                       |
| character_set_connection | utf8                       |
| character_set_database   | latin1                     |
| character_set_filesystem | binary                     |
| character_set_results    | utf8                       |
| character_set_server     | utf8                       |
| character_set_system     | utf8                       |
| character_sets_dir       | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+ 
8 rows in set (0.00 sec)

We can see the server and client values are as expected, but database is set to something else.
￼mysql&amp;gt; SHOW CREATE DATABASE fosdem\G 
*************************** 1. row ***************************
Database: fosdem
Create Database: CREATE DATABASE `fosdem` /*!40100 DEFAULT CHARACTER SET latin1 */ 
1 row in set (0.00 sec)

Since the database was created when the server was set to latin1 it inherited that charset setting, which persists even when the server setting changes.
Can we fix this?
mysql&amp;gt; SET NAMES utf8;
Query OK, 0 rows affected (0.00 sec)
mysql&amp;gt; SELECT last_name, HEX(last_name) FROM people; 
+------------+----------------------+
| last_name  | HEX(last_name)       | 
+------------+----------------------+
| Lemon      | 4C656D6F6E           | 
| Müller     | 4DFC6C6C6572         | 
| Dobrza?ski | 446F62727A613F736B69 | 
+------------+----------------------+ 
3 rows in set (0.00 sec)

mysql&amp;gt; SET NAMES latin2;
Query OK, 0 rows affected (0.00 sec)
mysql&amp;gt; SELECT last_name, HEX(last_name) FROM people; 
+------------+----------------------+
| last_name  | HEX(last_name)       | 
+------------+----------------------+
| Lemon      | 4C656D6F6E           | 
| Müller     | 4DFC6C6C6572         | 
| Dobrza?ski | 446F62727A613F736B69 | 
+------------+----------------------+ 
3 rows in set (0.00 sec)

Unfortunately, no matter how I try to read the data, 0x3F is &amp;#8216;?&amp;#8217;, so the &amp;#8216;ń&amp;#8217; has been lost forever. Therefore it may not be enough to reconfigure the server, as a mismatch between client and server can permanently break data, due to the implicit conversion inside the MySQL server.
Implicit conversions happen silently when characters of one character set are inserted into a column with a different character set. This behaviour can be controlled by SQL_MODE, which allows you force MySQL to raise an error instead.
In MySQL Character encoding &amp;#8211; part 1 we established there were a number of places you can control the character settings, now we can add a couple of important observations to our view of Character encoding settings.

Session settings

character_set_server
character_set_client
character_set_connection
character_set_database
character_set_result


Schema level Defaults &amp;#8211; Affects new tables
Table level Defaults &amp;#8211; Affects new columns
Column charsets

We have seen how a table created with no explicit charset declaration inherits the database (schema) charset, but what happens to a column when the table charset is changed?.
mysql&amp;gt; USE fosdem;
mysql&amp;gt; CREATE TABLE test (a VARCHAR(300), INDEX (a)); 
Query OK, 0 rows affected (0.62 sec)
mysql&amp;gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=latin1 
1 row in set (0.00 sec)

￼mysql&amp;gt; ALTER TABLE test DEFAULT CHARSET = utf8; 
Query OK, 0 rows affected (0.08 sec)
Records: 0 Duplicates: 0 Warnings: 0
mysql&amp;gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) CHARACTER SET latin1 DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 1 row in set (0.00 sec)

The columns in a table inherit their default charset value when the table is created, but do not change when the table is changed, however new columns added after the ALTER TABLE would inherit UTF8.
mysql&amp;gt; ALTER TABLE test ADD b VARCHAR(10); 
Query OK, 0 rows affected (0.74 sec)
Records: 0 Duplicates: 0 Warnings: 0
mysql&amp;gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) CHARACTER SET latin1 DEFAULT NULL, 
`b` varchar(10) DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8 
1 row in set (0.00 sec

What can you do if you detect inconsistencies in your MySQL Character encoding settings
First of all, keep calm and don’t start by changing something. Analyse the situation and make sure you understand what settings you have and what your application understands regarding reading and writing data from the database.
Once you detect a problem, try to assess the extent of the damage. Firstly, what is the scope of the damage and is it consistent. Are all the rows bad or is it just a subset such as the last days worth of inserts. Are all the bad rows broken in the same way or are there actually a mixture of problems affected different sets of rows. Are the rows actually repairable &amp;#8211; could be that recovering from backup and rolling forward is necessary as the inserted data has already been destroyed. Has any character mapping occurred during writes (e.g. unicode over latin1/latin1) &amp;#8211; all of this is necessary to get a good picture of where you are starting from.
Take care not to do not do any of the following:

Try to fix this table by table unless you really only have a single table. &amp;#8211; Any fix will affect the application and database as a whole, therefore fixing a single table may lead to inconsistencies and further problems elsewhere.
ALTER TABLE &amp;#8230; DEFAULT CHARSET = as it only changes the default character set for new columns.
ALTER TABLE &amp;#8230; CONVERT TO CHARACTER SET &amp;#8230; It’s not for fixing broken encoding.
ALTER TABLE &amp;#8230; MODIFY col_name &amp;#8230; CHARACTER SET &amp;#8230;

What needs to be fixed?


Schema default character set


ALTER SCHEMA fosdem DEFAULT CHARSET = utf8;


Tables with text columns: CHAR, VARCHAR, TEXT, TINYTEXT, LONGTEXT
What about ENUM?

The information schema can provide a list of candidate tables.
￼SELECT CONCAT(c.table_schema, '.', c.table_name) AS candidate_table 
FROM information_schema.columns c
WHERE c.table_schema = 'fosdem'
AND c.column_type REGEXP '^(.*CHAR|.*TEXT|ENUM)(\(.+\))?$' GROUP BY candidate_table;

You must also ensure the database and application configuration is correct also, to avoid having the newly fixed tables broken by new data being introduced incorrectly (for the settings) into the tables.
How do I fix this?
Option 1. Dump and restore (Requires downtime)
Dump the data preserving the bad configuration and drop the old database
# mysqldump -u root -p --skip-set-charset --default-character-set=latin1 fosdem &amp;gt; fosdem.sql

mysql&amp;gt; DROP SCHEMA fosdem;

Correct table definitions in the dump file by editing DEFAULT CHARSET in all CREATE TABLE statements, then create the database again and import the data.
mysql&amp;gt; CREATE SCHEMA fosdem DEFAULT CHARSET utf8;

# mysql -u root -p --default-character-set=utf8 fosdem &amp;lt; fosdem.sql

Option 2. Two step conversion (Requires downtime)
Perform a two step conversion with ALTER TABLE, converting the original encoding to VARBINARY/BLOB and then from there to the target encoding. Conversion from/to BINARY/BLOB removes character set context.


Stop applications
On each table, for each text column perform:


ALTER TABLE tbl MODIFY col_name VARBINARY(255);
ALTER TABLE tbl MODIFY col_name VARCHAR(255) CHARACTER SET utf8;

You may specify multiple columns per ALTER TABLE

Fix the problems (application and/or db configs)
Restart applications

Option 3. – Online character set fix; (Minimal downtime, Approximately 1 min)
Using pt-online-schema-change with the PSCE plugin and a small patch for pt-online-schema-change, you can convert columns online in the live database.

Start pt-online-schema-change on all tables – one by one with table rotation disabled (&amp;#8211;no-swap-tables) or drop pt-online-schema-change triggers
Wait until all tables have been converted
Stop applications
Fix the problems (application and/or db configs)
Rotate the tables – should take a minute or so
Restart applications

Currently the patch to pt-online-schema-change and plugin are available on bitbucket Github.
In MySQL Character encoding part 3 we will cover the gotchas in the process of fixing broken encoding, and what best practise to follow to get it right each time you setup a new server or create a new database.</description>
    <content:encoded><![CDATA[<p>In <a title="MySQL Character encoding - part 1" href="http://www.psce.com/blog/2015/02/12/mysql-character-encoding-part-1/" target="_blank">MySQL Character encoding &#8211; part 1</a> we stated that the myriad of ways in which character encoding can be controlled can lead to many situations where your data may not be available as expected.</p>
<div><a href="http://www.cl.cam.ac.uk/~mgk25/ucs/utf-8-history.txt"><img class="wp-image-3025 size-full" src="http://www.psce.com/blog/wp-content/uploads/2015/02/640px-Summit_diner_1024x658.jpg" alt="640px-Summit_diner_1024x658" width="640" height="428" /></a><p>UTF8 was designed on a placemat in a New Jersey diner one night in September or so 1992.</p></div>
<p><span></span></p>
<p><strong><em>Setting MySQL Client and Server Character encoding.</em></strong></p>
<p>Lets restart MySQL with the correct setting for our purpose, UTF8. Here we can see the setting in the MySQL configuration file, in this case /etc/mysql/my.cnf.</p>
<pre>character-set-server = <span>utf8</span>
</pre>
<p>This change is then reflected in the session and global variables once the instance is restarted with the new configuration parameter.</p>
<pre>mysql&gt; SELECT @@global.character_set_server, @@session.character_set_client;
+-------------------------------+--------------------------------+
| @@global.character_set_server | @@session.character_set_client | 
+-------------------------------+--------------------------------+
| <span>utf8</span>                          | <span>utf8</span>                           | 
+-------------------------------+--------------------------------+ 
1 row in set (0.00 sec)
</pre>
<p>Now we have verified the server and client are set to use UTF8, we can go ahead, continue developing our application and create a new table people.</p>
<pre>mysql&gt; CREATE TABLE people (first_name VARCHAR(30) NOT NULL, 
                             last_name VARCHAR(30) NOT NULL); 
Query OK, 0 rows affected (0.13 sec)
</pre>
<p>Now let&#8217;s enter some data into the new table, which was created with the server and client configured for UTF8.</p>
<p><a href="http://www.psce.com/blog/wp-content/uploads/2015/02/charsetpeople1.png"><img class="aligncenter size-medium wp-image-3001" src="http://www.psce.com/blog/wp-content/uploads/2015/02/charsetpeople1-300x281.png" alt="charsetpeople1" width="300" height="281" /></a></p>
<p><a href="http://www.psce.com/blog/wp-content/uploads/2015/02/charsetpeople2.png"><img class="aligncenter size-medium wp-image-3003" src="http://www.psce.com/blog/wp-content/uploads/2015/02/charsetpeople2-291x300.png" alt="charsetpeople2" width="291" height="300" /></a></p>
<p>Something appears to have gone terribly wrong, the accent in Maciek&#8217;s surname now appears as a question mark.</p>
<pre>mysql&gt; SELECT @@session.character_set_server, @@session.character_set_client; 
+--------------------------------+--------------------------------+
| @@session.character_set_server | @@session.character_set_client | 
+--------------------------------+--------------------------------+
| utf8                           | utf8                           | 
+--------------------------------+--------------------------------+ 
1 row in set (0.00 sec)
</pre>
<p>The database settings are still UTF8, this should have worked.</p>
<pre>mysql&gt; USE fosdem;
mysql&gt; SHOW CREATE TABLE people\G
*************************** 1. row ***************************
Table: people
Create Table: CREATE TABLE `people` (
`first_name` varchar(30) NOT NULL,
`last_name` varchar(30) NOT NULL
) ENGINE=InnoDB <span>DEFAULT CHARSET=latin1</span> 1 row in set (0.00 sec)
</pre>
<p>Looking at the table, we see that despite being created under a server set to use UTF8, it appears to be set to use latin1.</p>
<p>How can this be?, Let&#8217;s look at the session settings.</p>
<pre>￼mysql&gt; SHOW SESSION VARIABLES LIKE 'character_set_%'; 
+--------------------------+----------------------------+ 
| Variable_name            | Value                      | 
+--------------------------+----------------------------+
| character_set_client     | utf8                       |
| character_set_connection | utf8                       |
| character_set_database   | <span>latin1</span>                     |
| character_set_filesystem | binary                     |
| character_set_results    | utf8                       |
| character_set_server     | utf8                       |
| character_set_system     | utf8                       |
| character_sets_dir       | /usr/share/mysql/charsets/ |
+--------------------------+----------------------------+ 
8 rows in set (0.00 sec)
</pre>
<p>We can see the server and client values are as expected, but database is set to something else.</p>
<pre>￼mysql&gt; SHOW CREATE DATABASE fosdem\G 
*************************** 1. row ***************************
Database: fosdem
Create Database: CREATE DATABASE `fosdem` /*!40100 <span>DEFAULT CHARACTER SET latin1</span> */ 
1 row in set (0.00 sec)
</pre>
<p>Since the database was created when the server was set to latin1 it inherited that charset setting, which persists even when the server setting changes.</p>
<p><strong><em>Can we fix this?</em></strong></p>
<pre>mysql&gt; SET NAMES utf8;
Query OK, 0 rows affected (0.00 sec)
mysql&gt; SELECT last_name, HEX(last_name) FROM people; 
+------------+----------------------+
| last_name  | HEX(last_name)       | 
+------------+----------------------+
| Lemon      | 4C656D6F6E           | 
| Müller     | 4DFC6C6C6572         | 
| Dobrza?ski | 446F62727A61<span>3F</span>736B69 | 
+------------+----------------------+ 
3 rows in set (0.00 sec)
</pre>
<pre>mysql&gt; SET NAMES latin2;
Query OK, 0 rows affected (0.00 sec)
mysql&gt; SELECT last_name, HEX(last_name) FROM people; 
+------------+----------------------+
| last_name  | HEX(last_name)       | 
+------------+----------------------+
| Lemon      | 4C656D6F6E           | 
| Müller     | 4DFC6C6C6572         | 
| Dobrza?ski | 446F62727A61<span>3F</span>736B69 | 
+------------+----------------------+ 
3 rows in set (0.00 sec)
</pre>
<p>Unfortunately, no matter how I try to read the data, 0x3F is &#8216;?&#8217;, so the &#8216;ń&#8217; has been lost forever. Therefore it may not be enough to reconfigure the server, as a mismatch between client and server can permanently break data, due to the implicit conversion inside the MySQL server.</p>
<p>Implicit conversions happen silently when characters of one character set are inserted into a column with a different character set. This behaviour can be controlled by SQL_MODE, which allows you force MySQL to raise an error instead.</p>
<p>In <a title="MySQL Character encoding - part 1" href="http://www.psce.com/blog/2015/02/12/mysql-character-encoding-part-1/" target="_blank">MySQL Character encoding &#8211; part 1</a> we established there were a number of places you can control the character settings, now we can add a couple of important observations to our view of Character encoding settings.</p>
<ul>
<li>Session settings
<ul>
<li>character_set_server</li>
<li>character_set_client</li>
<li>character_set_connection</li>
<li>character_set_database</li>
<li>character_set_result</li>
</ul>
</li>
<li>Schema level <span>Defaults &#8211; Affects new tables</span></li>
<li>Table level <span>Defaults &#8211; Affects new columns</span></li>
<li>Column charsets</li>
</ul>
<p>We have seen how a table created with no explicit charset declaration inherits the database (schema) charset, but what happens to a column when the table charset is changed?.</p>
<pre>mysql&gt; USE fosdem;
mysql&gt; CREATE TABLE test (a VARCHAR(300), INDEX (a)); 
Query OK, 0 rows affected (0.62 sec)
mysql&gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=<span>latin1</span> 
1 row in set (0.00 sec)
</pre>
<pre>￼mysql&gt; ALTER TABLE test DEFAULT CHARSET = <span>utf8</span>; 
Query OK, 0 rows affected (0.08 sec)
Records: 0 Duplicates: 0 Warnings: 0
mysql&gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) CHARACTER SET <span>latin1</span> DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=<span>utf8</span> 1 row in set (0.00 sec)
</pre>
<p>The columns in a table inherit their default charset value when the table is created, but do not change when the table is changed, however new columns added after the ALTER TABLE would inherit UTF8.</p>
<pre>mysql&gt; ALTER TABLE test ADD b VARCHAR(10); 
Query OK, 0 rows affected (0.74 sec)
Records: 0 Duplicates: 0 Warnings: 0
mysql&gt; SHOW CREATE TABLE test\G 
*************************** 1. row ***************************
Table: test
Create Table: CREATE TABLE `test` (
`a` varchar(300) CHARACTER SET <span>latin1</span> DEFAULT NULL, 
`b` varchar(10) DEFAULT NULL,
KEY `a` (`a`)
) ENGINE=InnoDB DEFAULT CHARSET=<span>utf8</span> 
1 row in set (0.00 sec
</pre>
<p><strong><em>What can you do if you detect inconsistencies in your MySQL Character encoding settings</em></strong></p>
<p>First of all, keep calm and don’t start by changing something. Analyse the situation and make sure you understand what settings you have and what your application understands regarding reading and writing data from the database.</p>
<p>Once you detect a problem, try to assess the extent of the damage. Firstly, what is the scope of the damage and is it consistent. Are all the rows bad or is it just a subset such as the last days worth of inserts. Are all the bad rows broken in the same way or are there actually a mixture of problems affected different sets of rows. Are the rows actually repairable &#8211; could be that recovering from backup and rolling forward is necessary as the inserted data has already been destroyed. Has any character mapping occurred during writes (e.g. unicode over latin1/latin1) &#8211; all of this is necessary to get a good picture of where you are starting from.</p>
<p>Take care not to do not do any of the following:</p>
<ul>
<li>Try to fix this table by table unless you really only have a single table. &#8211; Any fix will affect the application and database as a whole, therefore fixing a single table may lead to inconsistencies and further problems elsewhere.</li>
<li>ALTER TABLE &#8230; DEFAULT CHARSET = as it only changes the default character set for new columns.</li>
<li>ALTER TABLE &#8230; CONVERT TO CHARACTER SET &#8230; It’s not for fixing broken encoding.</li>
<li>ALTER TABLE &#8230; MODIFY col_name &#8230; CHARACTER SET &#8230;</li>
</ul>
<p><strong><em>What needs to be fixed?</em></strong></p>
<ul>
<ul>
<li>Schema default character set</li>
</ul>
</ul>
<pre>ALTER SCHEMA fosdem DEFAULT CHARSET = utf8;
</pre>
<ul>
<li>Tables with text columns: CHAR, VARCHAR, TEXT, TINYTEXT, LONGTEXT</li>
<li>What about ENUM?</li>
</ul>
<p>The information schema can provide a list of candidate tables.</p>
<pre>￼SELECT CONCAT(c.table_schema, '.', c.table_name) AS candidate_table 
FROM information_schema.columns c
WHERE c.table_schema = 'fosdem'
AND c.column_type REGEXP '^(.*CHAR|.*TEXT|ENUM)(\(.+\))?$' GROUP BY candidate_table;
</pre>
<p>You must also ensure the database and application configuration is correct also, to avoid having the newly fixed tables broken by new data being introduced incorrectly (for the settings) into the tables.</p>
<p><strong><em>How do I fix this?</em></strong></p>
<p><strong>Option 1</strong>. Dump and restore (Requires downtime)</p>
<p>Dump the data preserving the bad configuration and drop the old database</p>
<pre># mysqldump -u root -p --skip-set-charset --default-character-set=latin1 fosdem &gt; fosdem.sql
</pre>
<pre>mysql&gt; DROP SCHEMA fosdem;
</pre>
<p>Correct table definitions in the dump file by editing DEFAULT CHARSET in all CREATE TABLE statements, then create the database again and import the data.</p>
<pre>mysql&gt; CREATE SCHEMA fosdem DEFAULT CHARSET utf8;
</pre>
<pre># mysql -u root -p --default-character-set=utf8 fosdem &lt; fosdem.sql
</pre>
<p><strong>Option 2.</strong> Two step conversion (Requires downtime)</p>
<p>Perform a two step conversion with ALTER TABLE, converting the original encoding to VARBINARY/BLOB and then from there to the target encoding. Conversion from/to BINARY/BLOB removes character set context.</p>
<ol>
<ol>
<li>Stop applications</li>
<li>On each table, for each text column perform:</li>
</ol>
</ol>
<pre>ALTER TABLE tbl MODIFY col_name VARBINARY(255);
ALTER TABLE tbl MODIFY col_name VARCHAR(255) CHARACTER SET utf8;
</pre>
<p>You may specify multiple columns per ALTER TABLE</p>
<ol>
<li>Fix the problems (application and/or db configs)</li>
<li>Restart applications</li>
</ol>
<p><strong>Option 3.</strong> – Online character set fix; (Minimal downtime, Approximately 1 min)</p>
<p>Using pt-online-schema-change with the PSCE plugin and a small patch for pt-online-schema-change, you can convert columns online in the live database.</p>
<ol>
<li>Start pt-online-schema-change on all tables – one by one with table rotation disabled (&#8211;no-swap-tables) or drop pt-online-schema-change triggers</li>
<li>Wait until all tables have been converted</li>
<li>Stop applications</li>
<li>Fix the problems (application and/or db configs)</li>
<li>Rotate the tables – should take a minute or so</li>
<li>Restart applications</li>
</ol>
<p>Currently the patch to pt-online-schema-change and plugin are available on <del><a title="pt-online-schema-change patch and plugin" href="https://bitbucket.org/psce/mysql-toolkit/src">bitbucket</a></del> <a title="github.com/PSCE/mysql-toolkit" href="https://github.com/PSCE/mysql-toolkit">Github</a>.</p>
<p>In MySQL Character encoding part 3 we will cover the gotchas in the process of fixing broken encoding, and what best practise to follow to get it right each time you setup a new server or create a new database.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989109&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989109&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 09:46:51 +0000</pubDate>
    <dc:creator>dba square</dc:creator>
    <category>Conference</category>
    <category>Development with MySQL</category>
    <category>Managing MySQL</category>
    <category>MySQL</category>
    <category>Varia</category>
    <category>character encoding</category>
    <category>conference</category>
    <category>configuration</category>
    <category>development</category>
    <category>fosdem</category>
    <category>latin1</category>
    <category>managing mysql</category>
    <category>Schema</category>
    <category>utf8</category>
  </item>

  <item>
    <title>MySQL Cluster on my &amp;quot;Windows computer&amp;quot;</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-14455177.post-631670145990926431</guid>
    <link>http://mikaelronstrom.blogspot.com/2015/03/mysql-cluster-on-my-windows-computer.html</link>
    <description>It's been some time since I wrote my last blog. As usual this means that I havebeen busy developing new things. Most of my blogs are about describing newdevelopments that happened in the MySQL Server, MySQL Cluster, MySQLpartitioning and other areas I have been busy developing in. For the last year I havebeen quite busy in working with MySQL Cluster 7.4, the newest cluster release. Asusual we have been able to add some performance improvements. But forMySQL Cluster 7.4 the goal has also been to improve quality. There are a numberof ways that one can improve quality. One can improve quality of a cluster by makingit faster to restart as problems appear. One can also improve it by improving codequality. We have done both of those things.In order to improve my own possibilities to test my new developments I decided toinvest in a &quot;Windows computer&quot;. This means a computer that I house in my window inmy living room :)There is some new very interesting computers that you can buy from Intel today thatworks perfectly as test platform for a distributed database like MySQL Cluster. Itis called Intel NUC. It is actually a computer that you need to assemble on your own,it comes equipped with a dual core Intel i5-4250U CPU that runs at 1.3GHz and canrun up to Turbo frequency of 2.3 GHz (the current generation has been upgraded toIntel i5-5250U processors). To this computer you can buy up to 16 GByte of memoryand you can add a 2.5&quot; SSD drive and it's even possible to also have a M.2 SSD cardin addition to the 2.5&quot; SSD drive. Personally I thought it was enough to have one SSDdrive of 250GB.So this computer is amazingly small, but still amazingly capable. I have it placed inthe window in front of my desk behind our TV set. It is so quiet that I can't evenhear it when all the rest of the equipment is shut off.Still it is capable enough to run our daily test suite with a 4 data node setup. Thistest suite runs more than 400 different test cases where we test node restarts, systemrestarts, index scans, backups, pk lookups and so forth. One such test run takes13 hours, so it is nice to be able to have this running on such a small box that canrun in the background without me having to interfere at all and without it making anynoise.So it's amazing how scalable the MySQL Cluster SW is, I can run test suites with a4 data node setup on a box that fits in the palm of my hand. At the same I can runbenchmarks using 100 2-socket servers that requires probably 4-5 racks of computersand that achieves 200M reads per second.Here is a little description of how you can setup a similar box to be runningdaily test runs for MySQL Cluster if ever you decide that you want to try todevelop a new feature for MySQL Cluster.1) At first the test suite requires a ndb_cpcd process to be running. This processtakes care of starting and stopping the MySQL Cluster processes.To do this do the following:1) Create a new directory, I called mine /home/mikael/cpcdIn this directory create a minor script that starts the ndb_cpcd.It contains the following command:ndb_cpcd --work-dir=/home/mikael/cpcd --logfile=/home/mikael/cpcd/cpcd.logFor it to run you need to compile MySQL Cluster and copy ndb_cpcd to e.g./usr/local/bin. This binary doesn't really change very often, so you canhave one compiled and need not change it. I call this script start_ndb_cpcd.sh.Then you start the ndb_cpcd in one window using./start_ndb_cpcd.sh2) In order to run the test suite I created a directory I called/home/mikael/mysql_clones/autotest_runThis is where I run the test suite from. For this I need to two files.The first is the autotest-boot.sh which is found in the MySQL Cluster sourcein the place storage/ndb/test/run-test/autotest-boot.sh.In addition I create here the configuration file used by this autotest-boot.shscript, it's called autotest.conf.In my case this file contains:install_dir=&quot;/home/mikael/mysql_clones/autotest_install&quot;build_dir=&quot;/home/mikael/mysql_clones/autotest_build&quot;git_local_repo=&quot;/home/mikael/mysql_clones/test_74&quot;git_remote_repo=&quot;/home/mikael/mysql_git&quot;base_dir=&quot;/home/mikael/mysql_clones/autotest_results&quot;target=&quot;x86_64_Linux&quot;hosts=&quot;mikael1 mikael1 mikael1 mikael1 mikael1 mikael1&quot;report=WITH_NDB_JAVA_DEFAULT=0WITH_NDB_NODEJS=0export WITH_NDB_JAVA_DEFAULT WITH_NDB_NODEJSMAKEFLAGS=&quot;-j7&quot;export MAKEFLAGSinstall_dir is the place where the build of the MySQL Cluster source is installed.build_dir is the place where the build of the MySQL Cluster is placed.git_local_repo is a git branch of MySQL Cluster 7.4.git_remote_repo is a git repo of the entire MySQL source.base_dir is the directory where the results of the test run are placed in acompressed tarball.target is the computer and OS, in my case a x86_64 running Linux.hosts is the hosts I use, there should be 6 hosts here, in my case they are all the samehost which is called mikael1.Finally I have report set to nothingand in order to avoid having to build the Java API and NodeJS APIs I set theWITH_NDB_JAVA_DEFAULT=0 and WITH_NDB_NODEJS=0.Finally I set the MAKEFLAGS to get a good parallelism in building MySQL Cluster.In order to run I need to have git installed, CMake as well and possibly somemore things. If one uses an older git version (like I do), then one needs tochange the git command in autotest-boot.sh a little bit.Finally one needs to add a new file to the MySQL branch from where you run,/home/mikael/mysql_clones/test_74 in my case. This file is called in mycase /home/mikael/mysql_clones/test_74/storage/ndb/test/run-test/conf-mikael1.cnf.The autotest-boot.sh creates the config file of the cluster from this file.In my case this file contains:# Copyright (c) 2015, Oracle and/or its affiliates. All rights reserved.## This program is free software; you can redistribute it and/or modify# it under the terms of the GNU General Public License as published by# the Free Software Foundation; version 2 of the License.## This program is distributed in the hope that it will be useful,# but WITHOUT ANY WARRANTY; without even the implied warranty of# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. &amp;nbsp;See the# GNU General Public License for more details.## You should have received a copy of the GNU General Public License# along with this program; if not, write to the Free Software# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 &amp;nbsp;USA[atrt]basedir = CHOOSE_dirbaseport = 14000#clusters = .4nodeclusters = .2nodefix-nodeid=1mt = 0[ndb_mgmd][mysqld]innodbskip-grant-tablessocket=mysql.sockdefault-storage-engine=myisam[client]protocol=tcp[cluster_config.2node]ndb_mgmd = CHOOSE_host1#ndbd = CHOOSE_host2,CHOOSE_host3,CHOOSE_host4,CHOOSE_host5ndbd = CHOOSE_host2,CHOOSE_host3ndbapi= CHOOSE_host1,CHOOSE_host1,CHOOSE_host1#mysqld = CHOOSE_host1,CHOOSE_host6mysqld = CHOOSE_host1,CHOOSE_host4NoOfReplicas = 2IndexMemory = 100MDataMemory = 500MBackupMemory = 64MMaxNoOfConcurrentScans = 100MaxNoOfSavedMessages= 5NoOfFragmentLogFiles = 4FragmentLogFileSize = 32MODirect=1MaxNoOfAttributes=2000Checksum=1SharedGlobalMemory=256MDiskPageBufferMemory=256M#FileSystemPath=/home/mikael/autotest#FileSystemPathDataFiles=/home/mikael/autotest#FileSystemPathUndoFiles=/home/mikael/autotestInitialLogfileGroup=undo_buffer_size=64M;undofile01.dat:256M;undofile02.dat:256MInitialTablespace=datafile01.dat:256M;datafile02.dat:256MTimeBetweenWatchDogCheckInitial=60000[cluster_config.ndbd.1.2node]TwoPassInitialNodeRestartCopy=1[cluster_config.ndbd.3.2node]TwoPassInitialNodeRestartCopy=1In the current setup it uses 2 data nodes, but it can also run easily with 4 datanodes by simply changing a few lines above.mt = 0 means that I always run with the ndbd process which is the smallest manner to runMySQL Cluster data nodes where all blocks runs within one thread. It is possible to runthem in up to more than 50 threads, but in this machine it makes more sense to use ndbd.The name of the file should be conf-HOST.cnf where you replace HOST with the hostname ofyour test computer.Finally in my case I also changed one line instorage/ndb/test/run-test/atrt-gather-results.shas# &amp;nbsp; &amp;nbsp;rsync -a --exclude='BACKUP' --exclude='ndb_*_fs' &quot;$1&quot; .&amp;nbsp; &amp;nbsp; rsync -a --exclude='BACKUP' &quot;$1&quot; .The idea is that I should also get the file system of the cluster reportedas part of the result tarball. This increases the size of the result tarballsignificantly, but if one is looking for bugs that happen in writing of REDOlogs, UNDO logs, checkpoints and so forth, then this is required to be ableto find the problem.Finally we have now come to the point where we need to execute theactual test run, we place ourselves in the autotest_run directoryand execute the command:./autotest-boot.sh --clone=mysql-5.6-cluster-7.4 daily-basicDuring the test execution one can look into autotest_install, there isa directory there starting with run that contains the current test runningand if a test fails for some reason it will create a result.number directorythere where you get the log information from the failure, successful testcases doesn't get any logs produced. The file log.txt contains the currenttest being executed.Finally the test executed for daily-basic are defined in the file:/home/mikael/mysql_clones/test_74/storage/ndb/test/run-test/daily-basic-tests.txtSo by adding or removing tests from this file you can add your own test cases,most of the tests are defined in the/home/mikael/mysql_clones/test_74/storage/ndb/test/ndbapi directory.Good luck in trying out this test environment. Remember that any changes to filesin the test_74 directory also requires to be commited in git since the test_74directory is cloned off using git commands.</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on">It's been some time since I wrote my last blog. As usual this means that I have<br />been busy developing new things. Most of my blogs are about describing new<br />developments that happened in the MySQL Server, MySQL Cluster, MySQL<br />partitioning and other areas I have been busy developing in. For the last year I have<br />been quite busy in working with MySQL Cluster 7.4, the newest cluster release. As<br />usual we have been able to add some performance improvements. But for<br />MySQL Cluster 7.4 the goal has also been to improve quality. There are a number<br />of ways that one can improve quality. One can improve quality of a cluster by making<br />it faster to restart as problems appear. One can also improve it by improving code<br />quality. We have done both of those things.<br /><br />In order to improve my own possibilities to test my new developments I decided to<br />invest in a "Windows computer". This means a computer that I house in my window in<br />my living room :)<br /><br />There is some new very interesting computers that you can buy from Intel today that<br />works perfectly as test platform for a distributed database like MySQL Cluster. It<br />is called Intel NUC. It is actually a computer that you need to assemble on your own,<br />it comes equipped with a dual core Intel i5-4250U CPU that runs at 1.3GHz and can<br />run up to Turbo frequency of 2.3 GHz (the current generation has been upgraded to<br />Intel i5-5250U processors). To this computer you can buy up to 16 GByte of memory<br />and you can add a 2.5" SSD drive and it's even possible to also have a M.2 SSD card<br />in addition to the 2.5" SSD drive. Personally I thought it was enough to have one SSD<br />drive of 250GB.<br /><br />So this computer is amazingly small, but still amazingly capable. I have it placed in<br />the window in front of my desk behind our TV set. It is so quiet that I can't even<br />hear it when all the rest of the equipment is shut off.<br /><br />Still it is capable enough to run our daily test suite with a 4 data node setup. This<br />test suite runs more than 400 different test cases where we test node restarts, system<br />restarts, index scans, backups, pk lookups and so forth. One such test run takes<br />13 hours, so it is nice to be able to have this running on such a small box that can<br />run in the background without me having to interfere at all and without it making any<br />noise.<br /><br />So it's amazing how scalable the MySQL Cluster SW is, I can run test suites with a<br />4 data node setup on a box that fits in the palm of my hand. At the same I can run<br />benchmarks using 100 2-socket servers that requires probably 4-5 racks of computers<br />and that achieves 200M reads per second.<br /><br />Here is a little description of how you can setup a similar box to be running<br />daily test runs for MySQL Cluster if ever you decide that you want to try to<br />develop a new feature for MySQL Cluster.<br /><br />1) At first the test suite requires a ndb_cpcd process to be running. This process<br />takes care of starting and stopping the MySQL Cluster processes.<br /><br />To do this do the following:<br />1) Create a new directory, I called mine /home/mikael/cpcd<br />In this directory create a minor script that starts the ndb_cpcd.<br />It contains the following command:<br />ndb_cpcd --work-dir=/home/mikael/cpcd --logfile=/home/mikael/cpcd/cpcd.log<br /><br />For it to run you need to compile MySQL Cluster and copy ndb_cpcd to e.g.<br />/usr/local/bin. This binary doesn't really change very often, so you can<br />have one compiled and need not change it. I call this script start_ndb_cpcd.sh.<br /><br />Then you start the ndb_cpcd in one window using<br />./start_ndb_cpcd.sh<br /><br />2) In order to run the test suite I created a directory I called<br />/home/mikael/mysql_clones/autotest_run<br /><br />This is where I run the test suite from. For this I need to two files.<br />The first is the autotest-boot.sh which is found in the MySQL Cluster source<br />in the place storage/ndb/test/run-test/autotest-boot.sh.<br />In addition I create here the configuration file used by this autotest-boot.sh<br />script, it's called autotest.conf.<br /><br />In my case this file contains:<br /><br />install_dir="/home/mikael/mysql_clones/autotest_install"<br />build_dir="/home/mikael/mysql_clones/autotest_build"<br />git_local_repo="/home/mikael/mysql_clones/test_74"<br />git_remote_repo="/home/mikael/mysql_git"<br />base_dir="/home/mikael/mysql_clones/autotest_results"<br />target="x86_64_Linux"<br />hosts="mikael1 mikael1 mikael1 mikael1 mikael1 mikael1"<br />report=<br />WITH_NDB_JAVA_DEFAULT=0<br />WITH_NDB_NODEJS=0<br />export WITH_NDB_JAVA_DEFAULT WITH_NDB_NODEJS<br />MAKEFLAGS="-j7"<br />export MAKEFLAGS<br /><br />install_dir is the place where the build of the MySQL Cluster source is installed.<br /><br />build_dir is the place where the build of the MySQL Cluster is placed.<br /><br />git_local_repo is a git branch of MySQL Cluster 7.4.<br /><br />git_remote_repo is a git repo of the entire MySQL source.<br /><br />base_dir is the directory where the results of the test run are placed in a<br />compressed tarball.<br /><br />target is the computer and OS, in my case a x86_64 running Linux.<br /><br />hosts is the hosts I use, there should be 6 hosts here, in my case they are all the same<br />host which is called mikael1.<br /><br />Finally I have report set to nothing<br /><br />and in order to avoid having to build the Java API and NodeJS APIs I set the<br />WITH_NDB_JAVA_DEFAULT=0 and WITH_NDB_NODEJS=0.<br />Finally I set the MAKEFLAGS to get a good parallelism in building MySQL Cluster.<br /><br />In order to run I need to have git installed, CMake as well and possibly some<br />more things. If one uses an older git version (like I do), then one needs to<br />change the git command in autotest-boot.sh a little bit.<br /><br />Finally one needs to add a new file to the MySQL branch from where you run,<br />/home/mikael/mysql_clones/test_74 in my case. This file is called in my<br />case /home/mikael/mysql_clones/test_74/storage/ndb/test/run-test/conf-mikael1.cnf.<br /><br />The autotest-boot.sh creates the config file of the cluster from this file.<br />In my case this file contains:<br /><br /># Copyright (c) 2015, Oracle and/or its affiliates. All rights reserved.<br />#<br /># This program is free software; you can redistribute it and/or modify<br /># it under the terms of the GNU General Public License as published by<br /># the Free Software Foundation; version 2 of the License.<br />#<br /># This program is distributed in the hope that it will be useful,<br /># but WITHOUT ANY WARRANTY; without even the implied warranty of<br /># MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. &nbsp;See the<br /># GNU General Public License for more details.<br />#<br /># You should have received a copy of the GNU General Public License<br /># along with this program; if not, write to the Free Software<br /># Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 &nbsp;USA<br /><br />[atrt]<br />basedir = CHOOSE_dir<br />baseport = 14000<br />#clusters = .4node<br />clusters = .2node<br />fix-nodeid=1<br />mt = 0<br /><br />[ndb_mgmd]<br /><br />[mysqld]<br />innodb<br />skip-grant-tables<br />socket=mysql.sock<br />default-storage-engine=myisam<br /><br />[client]<br />protocol=tcp<br /><br />[cluster_config.2node]<br />ndb_mgmd = CHOOSE_host1<br />#ndbd = CHOOSE_host2,CHOOSE_host3,CHOOSE_host4,CHOOSE_host5<br />ndbd = CHOOSE_host2,CHOOSE_host3<br />ndbapi= CHOOSE_host1,CHOOSE_host1,CHOOSE_host1<br />#mysqld = CHOOSE_host1,CHOOSE_host6<br />mysqld = CHOOSE_host1,CHOOSE_host4<br /><br />NoOfReplicas = 2<br />IndexMemory = 100M<br />DataMemory = 500M<br />BackupMemory = 64M<br />MaxNoOfConcurrentScans = 100<br />MaxNoOfSavedMessages= 5<br />NoOfFragmentLogFiles = 4<br />FragmentLogFileSize = 32M<br />ODirect=1<br />MaxNoOfAttributes=2000<br />Checksum=1<br /><br />SharedGlobalMemory=256M<br />DiskPageBufferMemory=256M<br />#FileSystemPath=/home/mikael/autotest<br />#FileSystemPathDataFiles=/home/mikael/autotest<br />#FileSystemPathUndoFiles=/home/mikael/autotest<br />InitialLogfileGroup=undo_buffer_size=64M;undofile01.dat:256M;undofile02.dat:256M<br />InitialTablespace=datafile01.dat:256M;datafile02.dat:256M<br />TimeBetweenWatchDogCheckInitial=60000<br /><br />[cluster_config.ndbd.1.2node]<br />TwoPassInitialNodeRestartCopy=1<br /><br />[cluster_config.ndbd.3.2node]<br />TwoPassInitialNodeRestartCopy=1<br /><br />In the current setup it uses 2 data nodes, but it can also run easily with 4 data<br />nodes by simply changing a few lines above.<br />mt = 0 means that I always run with the ndbd process which is the smallest manner to run<br />MySQL Cluster data nodes where all blocks runs within one thread. It is possible to run<br />them in up to more than 50 threads, but in this machine it makes more sense to use ndbd.<br /><br />The name of the file should be conf-HOST.cnf where you replace HOST with the hostname of<br />your test computer.<br /><br />Finally in my case I also changed one line in<br />storage/ndb/test/run-test/atrt-gather-results.sh<br /><br />as<br /><br /># &nbsp; &nbsp;rsync -a --exclude='BACKUP' --exclude='ndb_*_fs' "$1" .<br />&nbsp; &nbsp; rsync -a --exclude='BACKUP' "$1" .<br /><br />The idea is that I should also get the file system of the cluster reported<br />as part of the result tarball. This increases the size of the result tarball<br />significantly, but if one is looking for bugs that happen in writing of REDO<br />logs, UNDO logs, checkpoints and so forth, then this is required to be able<br />to find the problem.<br /><br />Finally we have now come to the point where we need to execute the<br />actual test run, we place ourselves in the autotest_run directory<br />and execute the command:<br /><br />./autotest-boot.sh --clone=mysql-5.6-cluster-7.4 daily-basic<br /><br />During the test execution one can look into autotest_install, there is<br />a directory there starting with run that contains the current test running<br />and if a test fails for some reason it will create a result.number directory<br />there where you get the log information from the failure, successful test<br />cases doesn't get any logs produced. The file log.txt contains the current<br />test being executed.<br /><br />Finally the test executed for daily-basic are defined in the file:<br />/home/mikael/mysql_clones/test_74/storage/ndb/test/run-test/daily-basic-tests.txt<br /><br />So by adding or removing tests from this file you can add your own test cases,<br />most of the tests are defined in the<br />/home/mikael/mysql_clones/test_74/storage/ndb/test/ndbapi directory.<br /><br />Good luck in trying out this test environment. Remember that any changes to files<br />in the test_74 directory also requires to be commited in git since the test_74<br />directory is cloned off using git commands.<br /><div><br /></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989107&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989107&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 08:03:00 +0000</pubDate>
    <dc:creator>Mikael Ronstr&amp;ouml;m</dc:creator>
  </item>

  <item>
    <title>MySQL Tech Tour in Oslo Norway on March the 17th!</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-8445678881921638771.post-7385500637471219889</guid>
    <link>http://mysql-nordic.blogspot.com/2015/03/mysql-tech-tour-in-oslo-norway-on-march.html</link>
    <description>MySQL Tech Tour: Out of the Box MySQL High Availability - Performance - ScalabilityMarch 17, 2015, Oslo, NorwayDid you know that the new MySQL Fabric delivers High Availability with automatic failure detection and failover? And that MySQL Fabric also enables scale-out with automated data sharding? Do you know how to take advantage of the MySQL SYS Schema?Join us for this free MySQL Tech Tour to learn straight from the source how you can benefit from Oracle’s latest MySQL innovations. Our technical experts will help you understand how to take advantage of the wide range of new features and enhancements available in MySQL Fabric, MySQL 5.6, MySQL Cluster and other MySQL solutions. They will share tips &amp;amp; tricks to help you get the most of your database. You will also discover what’s coming next in MySQL MySQL 5.7.Agenda:08:30 – 09:00 Registration &amp;amp; Welcome09:00 – 09:30 Introduction and Latest News&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Morten Andersen Oracle MySQL Technology Sales&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant09:30 – 09:50 MySQL EE and Enterprise Monitor demo&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Morten Andersen Oracle MySQL Technology Sales Rep&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant10:00 – 10:40 MySQL Roadmap&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant10:40 – 11:00 Coffee break11:00 – 11:40 MySQL Fabric&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant11:40 – 12:20 Why Oracle Linux for MySQL deployments?&amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; Discover how you can benefit from Oracle Linux and Oracle VM12:20 – 12:30 Q&amp;amp;A and ClosingDon’t miss this opportunity to learn from the experts. Join us at a location near you!Register here</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on">MySQL Tech Tour: Out of the Box MySQL High Availability - Performance - Scalability<br />March 17, 2015, Oslo, Norway<br /><br />Did you know that the new MySQL Fabric delivers High Availability with automatic failure detection and failover? And that MySQL Fabric also enables scale-out with automated data sharding? Do you know how to take advantage of the MySQL SYS Schema?<br /><br />Join us for this free MySQL Tech Tour to learn straight from the source how you can benefit from Oracle’s latest MySQL innovations. Our technical experts will help you understand how to take advantage of the wide range of new features and enhancements available in MySQL Fabric, MySQL 5.6, MySQL Cluster and other MySQL solutions. They will share tips &amp; tricks to help you get the most of your database. You will also discover what’s coming next in MySQL MySQL 5.7.<br /><br />Agenda:<br />08:30 – 09:00<span> </span>Registration &amp; Welcome<br />09:00 – 09:30<span> </span>Introduction and Latest News<br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Morten Andersen Oracle MySQL Technology Sales</i><br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant</i><br />09:30 – 09:50<span> </span>MySQL EE and Enterprise Monitor demo<br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Morten Andersen Oracle MySQL Technology Sales Rep</i><br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant</i><br />10:00 – 10:40<span> </span>MySQL Roadmap<br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant</i><br />10:40 – 11:00<span> </span>Coffee break<br />11:00 – 11:40<span> </span>MySQL Fabric<br /><i>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Ted Wennmark Oracle MySQL Principal Sales Consultant</i><br />11:40 – 12:20<span> </span>Why Oracle Linux for MySQL deployments?<br />&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Discover how you can benefit from Oracle Linux and Oracle VM<br />12:20 – 12:30<span> </span>Q&amp;A and Closing<br /><br />Don’t miss this opportunity to learn from the experts. Join us at a location near you!<br /><br />Register <a href="http://eventreg.oracle.com/profile/web/index.cfm?PKWebId=0x1886529711" target="_blank">here</a></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989106&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989106&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 07:18:00 +0000</pubDate>
    <dc:creator>Ted Wennmark</dc:creator>
  </item>

  <item>
    <title>Table and tablespace encryption on MariaDB 10.1.3</title>
    <guid isPermaLink="false">https://blog.mariadb.org/?p=2680</guid>
    <link>https://blog.mariadb.org/table-and-tablespace-encryption-on-mariadb-10-1-3/</link>
    <description>Introduction
For the moment, the only engines that fully support encryption are XtraDB and InnoDB. The Aria storage engine also supports encryption, but only for temporary tables.
MariaDB supports 2 different way to encrypt data in InnoDB/XtraDB:

Table encryption: Only tables which you create with PAGE_ENCRYPTION=1 are encrypted. This feature was created by eperi.
Tablespace encryption: Everything is encrypted (including log files). This feature was created by Google and is based on their MySQL branch.

InnoDB Table Level Encryption
Table level encryption means that you choose which tables to encrypt. This allows you to balance security with speed. To use table encryption, you have to:

Set the value of encryption-algorithm to the algorithm of your choice.
Load the file-key-management-plugin (or similar)
Define the location of key file
Create keys

Example:# Table level encryption configuration
plugin-load-add=file_key_management_plugin.so
file-key-management-plugin
file-key-management-plugin-filename=/mnt/dfs/keys.txt
encryption-algorithm=aes_ctrKeys can be generated using OpenSSL with following commandshell&amp;amp;gt;openssl enc -aes-256-ctr -k mypass -P -md sha1
salt=BFA606C6079DAD33
key=3DB1F43A606DA6ADF4AEB25B44A5E5FE2126EDEACF5AF8DF7B982D8143191936
iv =21C4592A16C870DD47B162F8959E562FThe key file is a text file containing an key id, the hex-encoded iv and the hex-encoded key. Example keys.txt using above generated key:1;21C4592A16C870DD47B162F8959E562F;3DB1F43A606DA6ADF4AEB25B44A5E5FE2126EDEACF5AF8DF7B982D8143191936After this is it up to database designer to select tables that contain sensitive data for encryption. Encryption can be enabled to table in table creation time or using ALTER TABLE. As an example:CREATE TABLE customer(CUSTOMER_ID BIGINT NOT NULL PRIMARY KEY, CUSTOMER_NAME VARCHAR(80), CUSTOMER_CREDITCARD VARCHAR(20)) ENGINE=InnoDB page_encryption=1 page_encryption_key=1;
ALTER TABLE sales page_encryption=1 page_encryption_key=1;In table encryption currently keys can&amp;#8217;t be changed but used key can be changed using ALTER TABLE. If no key identifier is provided a default key is used. Default key can be set either on my.cnf withinnodb_default_page_encryption_key=4or dynamically using global setting:set global innodb_default_page_encryption_key=4;Default key is used e.g.create table t1(a int not null primary key) engine=innodb page_encryption=1;
InnoDB Tablespace Encryption
In tablespace encryption all InnoDB tables are encrypted. Additionally, you may encrypt InnoDB log files, Aria tables (ROW_FORMAT=PAGE) and Aria temporary tables. To use tablespace encryption, you have to:

 Set the value of encryption-algorithm to the algorithm of your choice.
Load the example-key-management-plugin (or similar)

Example:# Tablespace encryption configuration
encryption-algorithm=aes_ctr
innodb-encrypt-tables
plugin-load-add=example_key_management_plugin.so
example_key_management_plugin
# encrypt Aria tables
aria
aria-encrypt-tables
# encrypt tmp tables
encrypt-tmp-disk-tables
# encrypt InnoDB log files
innodb-encrypt-log
# key rotation
innodb-encryption-threads=4
innodb-encryption-rotate-key-age=1800In tablespace encryption keys are not static. Instead so called key rotation is used. In key rotation used encryption key is changed if key used on a page is older than innodb-encryption-rotate-key-age seconds.
InnoDB Tablespace Scrubbing
Scrubbing means that there is a background process that regularly scans through all tables and upgrades the encryption keys for the pages. This happens either as part of purge (non compressed) or scrubbing by scanning whole tablespaces (added into key rotation threads). Purge is a a type of garbage collection that InnoDB internally runs to improve performance. Configuration for this feature might look as follows:# InnoDB Tablespace scrubbing 
innodb-immediate_scrub_data_uncompressed
innodb-background-scrub-data-uncompressed
innodb-background-scrub-data-compressed
# check if spaces needs scrubbing every 500 seconds
innodb_background_scrub_data_check_interval=500
# scrub spaces that were last scrubbed longer than 1800 seconds
innodb_background_scrub_data_interval=1800
Performance Impact
Encrypting the tables or tablespaces naturally have some effect on overall performance of the system. Naturally, the amount of performance effect encryption has is dependent on used hardware, workload and used encryption method. Goal of this section is to give some indication how much effect on performance there is when table encryption is used or when tablespace encryption is used when compared to setup where no encryption is used.
All experiments where conducted on Intel Xeon E5-2690 @ 2.9GHz CPU containing 2 sockets with 8 cores each using hyper threading, thus 32 total cores and Linux 3.4.12 with 132G main memory. The database is stored on a Fusion-io ioDrive2 Duo 2.41TB Firmware v7.2.5, rev 110646, Driver 3.3.4 build 5833069. The database filesystem is using NVMFS and all test logs and outputs are stored on second ioDrive using EXT4. We use On-Line Transaction Processing (OLTP) benchmark from Percona https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql. This TPC-C like workload involves a mix of five concurrent transaction types executed on-line or queued for deferred execution. The database is comprised of nine tables with a wide range of record and population sizes. Results are measured in terms of transactions per minute (tpmC). We will use 1000 warehouses producing ~100G database and buffer pool size 50G, so that full database does not fit to buffer pool. Additionally, we will use only InnoDB plugin as a storage engine. Finally, we use 3 hour measure time.
In the first graph we compare the resulting tpmC results on normal InnoDB tables (unencrypted tables), page encrypted tables and tablespace encryption (google encrypted on graph).

&amp;nbsp;
In the second graph we compare the number of New Order transactions/second with InnoDB tables (unencrypted tables), page encrypted tables and tablespace encryption (google encrypted on graph)

Conclusions
MariaDB Corporation would like to thank eperi and Google for their contributions to MariaDB.</description>
    <content:encoded><![CDATA[<h1>Introduction</h1>
<p>For the moment, the only engines that fully support encryption are XtraDB and InnoDB. The Aria storage engine also supports encryption, but only for temporary tables.</p>
<p>MariaDB supports 2 different way to encrypt data in InnoDB/XtraDB:</p>
<ol>
<li>Table encryption: Only tables which you create with PAGE_ENCRYPTION=1 are encrypted. This feature was created by <a href="http://eperi.de/en/">eperi</a>.</li>
<li>Tablespace encryption: Everything is encrypted (including log files). This feature was created by <a href="http://www.google.com/about/">Google</a> and is based on their <a href="https://code.google.com/p/google-mysql/">MySQL branch</a>.</li>
</ol>
<h2>InnoDB Table Level Encryption</h2>
<p>Table level encryption means that you choose which tables to encrypt. This allows you to balance security with speed. To use table encryption, you have to:</p>
<ul>
<li>Set the value of encryption-algorithm to the algorithm of your choice.</li>
<li>Load the file-key-management-plugin (or similar)</li>
<li>Define the location of key file</li>
<li>Create keys</li>
</ul>
<p>Example:</p><pre># Table level encryption configuration
plugin-load-add=file_key_management_plugin.so
file-key-management-plugin
file-key-management-plugin-filename=/mnt/dfs/keys.txt
encryption-algorithm=aes_ctr</pre><p>Keys can be generated using OpenSSL with following command</p><pre>shell&amp;gt;openssl enc -aes-256-ctr -k mypass -P -md sha1
salt=BFA606C6079DAD33
key=3DB1F43A606DA6ADF4AEB25B44A5E5FE2126EDEACF5AF8DF7B982D8143191936
iv =21C4592A16C870DD47B162F8959E562F</pre><p>The key file is a text file containing an key id, the hex-encoded iv and the hex-encoded key. Example keys.txt using above generated key:</p><pre>1;21C4592A16C870DD47B162F8959E562F;3DB1F43A606DA6ADF4AEB25B44A5E5FE2126EDEACF5AF8DF7B982D8143191936</pre><p>After this is it up to database designer to select tables that contain sensitive data for encryption. Encryption can be enabled to table in table creation time or using ALTER TABLE. As an example:</p><pre>CREATE TABLE customer(CUSTOMER_ID BIGINT NOT NULL PRIMARY KEY, CUSTOMER_NAME VARCHAR(80), CUSTOMER_CREDITCARD VARCHAR(20)) ENGINE=InnoDB page_encryption=1 page_encryption_key=1;
ALTER TABLE sales page_encryption=1 page_encryption_key=1;</pre><p>In table encryption currently keys can&#8217;t be changed but used key can be changed using ALTER TABLE. If no key identifier is provided a default key is used. Default key can be set either on my.cnf with</p><pre>innodb_default_page_encryption_key=4</pre><p>or dynamically using global setting:</p><pre>set global innodb_default_page_encryption_key=4;</pre><p>Default key is used e.g.</p><pre>create table t1(a int not null primary key) engine=innodb page_encryption=1;</pre><p></p>
<h2>InnoDB Tablespace Encryption</h2>
<p>In tablespace encryption all InnoDB tables are encrypted. Additionally, you may encrypt InnoDB log files, Aria tables (ROW_FORMAT=PAGE) and Aria temporary tables. To use tablespace encryption, you have to:</p>
<ul>
<li> Set the value of encryption-algorithm to the algorithm of your choice.</li>
<li>Load the example-key-management-plugin (or similar)</li>
</ul>
<p>Example:</p><pre># Tablespace encryption configuration
encryption-algorithm=aes_ctr
innodb-encrypt-tables
plugin-load-add=example_key_management_plugin.so
example_key_management_plugin
# encrypt Aria tables
aria
aria-encrypt-tables
# encrypt tmp tables
encrypt-tmp-disk-tables
# encrypt InnoDB log files
innodb-encrypt-log
# key rotation
innodb-encryption-threads=4
innodb-encryption-rotate-key-age=1800</pre><p>In tablespace encryption keys are not static. Instead so called key rotation is used. In key rotation used encryption key is changed if key used on a page is older than innodb-encryption-rotate-key-age seconds.</p>
<h1>InnoDB Tablespace Scrubbing</h1>
<p>Scrubbing means that there is a background process that regularly scans through all tables and upgrades the encryption keys for the pages. This happens either as part of purge (non compressed) or scrubbing by scanning whole tablespaces (added into key rotation threads). Purge is a a type of garbage collection that InnoDB internally runs to improve performance. Configuration for this feature might look as follows:</p><pre># InnoDB Tablespace scrubbing 
innodb-immediate_scrub_data_uncompressed
innodb-background-scrub-data-uncompressed
innodb-background-scrub-data-compressed
# check if spaces needs scrubbing every 500 seconds
innodb_background_scrub_data_check_interval=500
# scrub spaces that were last scrubbed longer than 1800 seconds
innodb_background_scrub_data_interval=1800</pre><p></p>
<h1>Performance Impact</h1>
<p>Encrypting the tables or tablespaces naturally have some effect on overall performance of the system. Naturally, the amount of performance effect encryption has is dependent on used hardware, workload and used encryption method. Goal of this section is to give some indication how much effect on performance there is when table encryption is used or when tablespace encryption is used when compared to setup where no encryption is used.</p>
<p>All experiments where conducted on Intel Xeon E5-2690 @ 2.9GHz CPU containing 2 sockets with 8 cores each using hyper threading, thus 32 total cores and Linux 3.4.12 with 132G main memory. The database is stored on a Fusion-io ioDrive2 Duo 2.41TB Firmware v7.2.5, rev 110646, Driver 3.3.4 build 5833069. The database filesystem is using NVMFS and all test logs and outputs are stored on second ioDrive using EXT4. We use On-Line Transaction Processing (OLTP) benchmark from Percona <a href="https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql">https://code.launchpad.net/~percona-dev/perconatools/tpcc-mysql</a>. This TPC-C like workload involves a mix of five concurrent transaction types executed on-line or queued for deferred execution. The database is comprised of nine tables with a wide range of record and population sizes. Results are measured in terms of transactions per minute (tpmC). We will use 1000 warehouses producing ~100G database and buffer pool size 50G, so that full database does not fit to buffer pool. Additionally, we will use only InnoDB plugin as a storage engine. Finally, we use 3 hour measure time.</p>
<p>In the first graph we compare the resulting tpmC results on normal InnoDB tables (unencrypted tables), page encrypted tables and tablespace encryption (google encrypted on graph).</p>
<p><a href="https://blog.mariadb.org/wp-content/uploads/2015/02/encryption_tpcc22.jpeg"><img class="alignnone size-full wp-image-2705" src="https://blog.mariadb.org/wp-content/uploads/2015/02/encryption_tpcc22.jpeg" alt="encryption_tpcc2" width="800" height="600" /></a></p>
<p>&nbsp;</p>
<p>In the second graph we compare the number of New Order transactions/second with InnoDB tables (unencrypted tables), page encrypted tables and tablespace encryption (google encrypted on graph)</p>
<p><a href="https://blog.mariadb.org/wp-content/uploads/2015/02/encryption_tpcc3.jpeg"><img class="alignnone size-full wp-image-2704" src="https://blog.mariadb.org/wp-content/uploads/2015/02/encryption_tpcc3.jpeg" alt="encryption_tpcc" width="800" height="600" /></a></p>
<h1>Conclusions</h1>
<p>MariaDB Corporation would like to thank eperi and Google for their contributions to MariaDB.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989105&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989105&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 06:19:24 +0000</pubDate>
    <dc:creator>MariaDB</dc:creator>
    <category>Development</category>
    <category>General</category>
    <category>InnoDB</category>
    <category>XtraDB</category>
  </item>

  <item>
    <title>March 17 Webinar: How Indexes Work in MySQL</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/03/03/upcoming-webinar-how-indexes-works-in-mysql/</guid>
    <link>https://vividcortex.com/blog/2015/03/03/upcoming-webinar-how-indexes-works-in-mysql/</link>
    <description>MySQL offers a few different types of indexes and uses them in a variety of ways. There’s a lot to know about the various kinds of indexes and how they interact with the storage engines, and it’s all very important for query optimization. A few examples are listed below:


  
    The “leftmost prefix rule”
  
  
    Clustered primary indexes versus secondary indexes
  
  
    B-Tree and hash
  
  
    New types of indexes such as LSM and Fractal Trees
  
  
    Newer features in the query optimizer and 
executor in MySQL 5.6 and 5.7
  


You will leave this webinar with a better understanding of how MySQL and its storage engines use indexes to speed up queries, and how you can improve query performance with basic and advanced index optimizations.
Please register here, and no worries if you cannot attend the webinar. We will send you a recording.</description>
    <content:encoded><![CDATA[<p>MySQL offers a few different types of indexes and uses them in a variety of ways. There’s a lot to know about the various kinds of indexes and how they interact with the storage engines, and it’s all very important for query optimization. A few examples are listed below:</p>

<ul>
  <li>
    <p>The “leftmost prefix rule”</p>
  </li>
  <li>
    <p>Clustered primary indexes versus secondary indexes</p>
  </li>
  <li>
    <p>B-Tree and hash</p>
  </li>
  <li>
    <p>New types of indexes such as LSM and Fractal Trees</p>
  </li>
  <li>
    <p>Newer features in the query optimizer and 
executor in MySQL 5.6 and 5.7</p>
  </li>
</ul>

<p>You will leave this webinar with a better understanding of how MySQL and its storage engines use indexes to speed up queries, and how you can improve query performance with basic and advanced index optimizations.
Please register <a href="https://vividcortex.com/webinars/mysql-indexes?utm_source=site&amp;utm_medium=blog">here</a>, and no worries if you cannot attend the webinar. We will send you a recording.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989114&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989114&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Access Shard-Query with the MySQL client without using MySQL proxy</title>
    <guid isPermaLink="false">http://shardquery.com/?p=256</guid>
    <link>http://shardquery.com/2015/03/02/access-shard-query-with-the-mysql-client-without-using-mysql-proxy/</link>
    <description>One of the great features of Shard-Query is the ability to use MySQL proxy to access resultsets transparently. While this is a great tool, many people have expressed reservations about using MySQL Proxy, an alpha component in their production environment.
I recognize that this is a valid concern, and have implemented an alternate method of retrieving resultsets directly in the MySQL client, without using a proxy. This means that any node can easily act as the &amp;#8220;head&amp;#8221; node without any extra daemon, instead of having to run many proxies.
The sq_helper() routine has been checked into the git repository and is available now.
The function takes a few parameters:

sql to run
shard-query schema name (empty string or null for default schema)
schema to store temp table in
temp table name (where results are sent to)
return result (boolean, 1 returns result to client, 0 does not return the result)
drop table (boolean, 1 drops the table at the end of the procedure, 0 does not)

mysql&amp;gt; call shard_query.sq_helper(&quot;select * from dim_date limit 2&quot;, &quot;&quot;, 'test','testtab',1,1)\G
*************************** 1. row ***************************
         D_DateKey: 19911231
            D_Date: December 31, 1991
       D_DayOfWeek: Wednesday
           D_Month: December
            D_Year: 1991
    D_YearMonthNum: 199112
       D_YearMonth: Dec1991
    D_DayNumInWeek: 4
   D_DayNumInMonth: 31
    D_DayNumInYear: 365
  D_MonthNumInYear: 12
   D_WeekNumInYear: 53
   D_SellingSeason: Christmas
 D_LastDayInWeekFl: 0
D_LastDayInMonthFl: 0
       D_HolidayFl: 0
       D_WeekDayFl: 1
*************************** 2. row ***************************
         D_DateKey: 19920101
            D_Date: January 1, 1992
       D_DayOfWeek: Thursday
           D_Month: January
            D_Year: 1992
    D_YearMonthNum: 199201
       D_YearMonth: Jan1992
    D_DayNumInWeek: 5
   D_DayNumInMonth: 1
    D_DayNumInYear: 1
  D_MonthNumInYear: 1
   D_WeekNumInYear: 1
   D_SellingSeason: Winter
 D_LastDayInWeekFl: 0
D_LastDayInMonthFl: 1
       D_HolidayFl: 1
       D_WeekDayFl: 1
2 rows in set (0.07 sec)

Important:
The only requirement is the Gearman UDF (which is also required by the proxy). Don&amp;#8217;t forget to call gman_server_set(&amp;#8230;) with your gearman server, or this function won&amp;#8217;t work.   </description>
    <content:encoded><![CDATA[<p>One of the great features of Shard-Query is the ability to use MySQL proxy to access resultsets transparently. While this is a great tool, many people have expressed reservations about using MySQL Proxy, an alpha component in their production environment.</p>
<p>I recognize that this is a valid concern, and have implemented an alternate method of retrieving resultsets directly in the MySQL client, without using a proxy. This means that any node can easily act as the &#8220;head&#8221; node without any extra daemon, instead of having to run many proxies.</p>
<p>The <strong>sq_helper</strong>() routine has been checked into the git repository and is available now.</p>
<p>The function takes a few parameters:</p>
<ul>
<li>sql to run</li>
<li>shard-query schema name (empty string or null for default schema)</li>
<li>schema to store temp table in</li>
<li>temp table name (where results are sent to)</li>
<li>return result (boolean, 1 returns result to client, 0 does not return the result)</li>
<li>drop table (boolean, 1 drops the table at the end of the procedure, 0 does not)</li>
</ul>
<pre>mysql&gt; call shard_query.sq_helper("select * from dim_date limit 2", "", 'test','testtab',1,1)\G
*************************** 1. row ***************************
         D_DateKey: 19911231
            D_Date: December 31, 1991
       D_DayOfWeek: Wednesday
           D_Month: December
            D_Year: 1991
    D_YearMonthNum: 199112
       D_YearMonth: Dec1991
    D_DayNumInWeek: 4
   D_DayNumInMonth: 31
    D_DayNumInYear: 365
  D_MonthNumInYear: 12
   D_WeekNumInYear: 53
   D_SellingSeason: Christmas
 D_LastDayInWeekFl: 0
D_LastDayInMonthFl: 0
       D_HolidayFl: 0
       D_WeekDayFl: 1
*************************** 2. row ***************************
         D_DateKey: 19920101
            D_Date: January 1, 1992
       D_DayOfWeek: Thursday
           D_Month: January
            D_Year: 1992
    D_YearMonthNum: 199201
       D_YearMonth: Jan1992
    D_DayNumInWeek: 5
   D_DayNumInMonth: 1
    D_DayNumInYear: 1
  D_MonthNumInYear: 1
   D_WeekNumInYear: 1
   D_SellingSeason: Winter
 D_LastDayInWeekFl: 0
D_LastDayInMonthFl: 1
       D_HolidayFl: 1
       D_WeekDayFl: 1
2 rows in set (0.07 sec)
</pre>
<p><strong>Important:</strong><br />
The only requirement is the Gearman UDF (which is also required by the proxy). Don&#8217;t forget to call gman_server_set(&#8230;) with your gearman server, or this function won&#8217;t work.</p><br />  <a rel="nofollow" href="http://feeds.wordpress.com/1.0/gocomments/shardquery.wordpress.com/256/"><img alt="" border="0" src="http://feeds.wordpress.com/1.0/comments/shardquery.wordpress.com/256/" /></a> <img alt="" border="0" src="http://pixel.wp.com/b.gif?host=shardquery.com&amp;blog=52522344&amp;post=256&amp;subd=shardquery&amp;ref=&amp;feed=1" width="1" height="1" /><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989103&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989103&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 00:24:57 +0000</pubDate>
    <dc:creator>Justin Swanhart</dc:creator>
    <category>Announcements</category>
    <category>Hints and tips</category>
    <category>client</category>
    <category>json</category>
    <category>procedure</category>
    <category>proxy</category>
    <category>sq_helper</category>
    <category>udf</category>
  </item>

  <item>
    <title>#DBHangOps 03/05/15 -- Group Replication, Multithreaded Replication, and more!</title>
    <guid isPermaLink="false">http://www.dbhangops.net/2015/03/03/dbhangops-030515-group-replication-multi-threaded-replication-and-more</guid>
    <link>http://www.dbhangops.net/2015/03/03/dbhangops-030515-group-replication-multi-threaded-replication-and-more</link>
    <description>#DBHangOps 03/05/15 -- Group Replication, Multithreaded Replication, and more!

Hello everybody!

Join in #DBHangOps this Thursday, March, 05, 2015 at 11:00am pacific (19:00 GMT), to participate in the discussion about:


Group Replication
Multithreaded Replication
Operational learnings with GTID
New MySQL 5.7 defaults from Morgan Tocker


You can check out the event page at https://plus.google.com/events/cjbmf109r6d7isr715iupigsrq4 on Thursday to participate.

As always, you can still watch the #DBHangOps twitter search, the @DBHangOps twitter feed, or this blog post to get a link for the google hangout on Thursday!

See all of you on Thursday!

You can catch a livestream at:</description>
    <content:encoded><![CDATA[<h1>#DBHangOps 03/05/15 -- Group Replication, Multithreaded Replication, and more!</h1>

<p>Hello everybody!</p>

<p>Join in #DBHangOps this Thursday, <strong>March, 05, 2015 at 11:00am pacific (19:00 GMT)</strong>, to participate in the discussion about:</p>

<ul>
<li>Group Replication</li>
<li>Multithreaded Replication</li>
<li>Operational learnings with GTID</li>
<li>New MySQL 5.7 defaults from Morgan Tocker</li>
</ul>

<p>You can check out the event page at <a href="https://plus.google.com/events/cjbmf109r6d7isr715iupigsrq4">https://plus.google.com/events/cjbmf109r6d7isr715iupigsrq4</a> on Thursday to participate.</p>

<p>As always, you can still watch the <a href="https://twitter.com/search/realtime?q=%23DBHangOps">#DBHangOps twitter search</a>, the <a href="https://twitter.com/dbhangops">@DBHangOps</a> twitter feed, or this blog post to get a link for the google hangout on Thursday!</p>

<p>See all of you on Thursday!</p>

<p>You can catch a livestream at:</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989122&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989122&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 03 Mar 2015 00:00:00 +0000</pubDate>
    <dc:creator>Geoffrey Anderson</dc:creator>
  </item>

  <item>
    <title>The ocelotgui debugger</title>
    <guid isPermaLink="false">http://ocelot.ca/blog/?p=503</guid>
    <link>http://ocelot.ca/blog/blog/2015/03/02/the-ocelotgui-debugger/</link>
    <description>I have merged a debugger for MySQL/MariaDB stored procedures and functions into our GUI client and posted the source and binaries on github. It allows breakpoint, clear, continue, next, skip, step, tbreakpoint, and variable displays. Features which are rare or missing in other debuggers include:
its current platform is Linux;
it allows breakpoints on functions which are invoked within SQL statements;
it never changes existing stored procedures or functions;
it is integrated with a general GUI client;
it allows commands like gdb and allows menu items / shortcut keys like ddd;
it is available on github as C++ source with GPL licence.
It's alpha and it's fragile but it works. Here is a demo.
Start the client and connect to a running server, as root. Actually the required privileges are merely for creation of certain objects and SUPER, but I'm making this simple.

Type the statement: DELIMITER //  (For all typed-in instructions, execute by typing carriage-return after the delimiter or by clicking Run|Execute.)
Create a function fdemo which updates and returns a counter.

Create a procedure pdemo which contains a loop of &quot;INSERT INTO tdemo VALUES (fdemo())&quot; statements.

Type the statement: DELIMITER ;

Type the statement: $install;

Type the statement: $setup fdemo, pdemo;

Type the statement: $debug pdemo;
After this statement is executed a tabbed widget appears. The first line of pdemo is highlighted. There is always a breakpoint before the first line.

Click on the Debug menu to see what options are available. Debugger instructions may be done via the menu, via Alt keys, or via command-line statements. For example to enter a Next instruction one may now click Debug|Next, or type Alt+3, or type the statement: &quot;$next;&quot;.

Enter a Next instruction repeatedly, watching how the highlighted line changes, until the INSERT line is highlighted.

Enter a Step instruction. After this is executed, the function now comes into the foreground.

Enter three more Step (or Next) instructions. After these are executed, the procedure now is in the foreground again.

Set a breakpoint for the final executable line of the procedure. This can be done by clicking over the line number, or by moving the cursor to the line and then clicking Debug|Breakpoint, or by typing the statement &quot;$breakpoint pdemo 9;&quot;. After this is executed, line 9 of pdemo has a small red mark showing that it has a breakpoint.

Enter a Continue instruction. After this is executed, watch the debugger highlight hop around 100 times as it moves through the loop, and finally settle on line 9.

Type the statements &quot;$refresh variables;&quot; and &quot;select old_value, value from xxxmdbug.variables&quot;. After this is executed, the result-set widget will contain the old value of variable i (99) and the current value (100), This is the way that one examines DECLAREd variables (there are other statements for user variables and server variables).

Enter an Exit instruction. This stops the debugging session, so the effects of the earlier $debug instruction are cancelled. The effects of the earlier $install and $setup instructions are not cancelled, so they will not have to be repeated for the next debugging session involving pdemo and fdemo.

Thus ends our demo. If you would like to confirm it: an introduction for how ocelotgui in general works is in the earlier blog post An open-source MySQL/MariaDB client on Linux&quot; and the source + executable download files for version 0.3.0 are on github.com/ocelot-inc/ocelotgui.
If there is anything Ocelot can do to make your next debugging trip more enjoyable, please leave a comment. If you have private thoughts, please write to pgulutzan at ocelot.ca.</description>
    <content:encoded><![CDATA[<p>I have merged a <a href="https://launchpad.net/mdbug">debugger for MySQL/MariaDB stored procedures and functions</a> into our GUI client and posted the source and binaries on github. It allows breakpoint, clear, continue, next, skip, step, tbreakpoint, and variable displays. Features which are rare or missing in other debuggers include:<br />
its current platform is Linux;<br />
it allows breakpoints on functions which are invoked within SQL statements;<br />
it never changes existing stored procedures or functions;<br />
it is integrated with a general GUI client;<br />
it allows commands like gdb and allows menu items / shortcut keys like ddd;<br />
it is available on github as C++ source with GPL licence.</p>
<p>It's alpha and it's fragile but it works. Here is a demo.</p>
<p>Start the client and connect to a running server, as root. Actually the required privileges are merely for creation of certain objects and SUPER, but I'm making this simple.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/connection_dialog_box.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/connection_dialog_box.jpg" alt="connection_dialog_box" width="417" height="557" class="alignnone size-full wp-image-504" /></a></p>
<p>Type the statement: DELIMITER //  (For all typed-in instructions, execute by typing carriage-return after the delimiter or by clicking Run|Execute.)<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/delimiter_1.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/delimiter_1.jpg" alt="delimiter_1" width="1440" height="900" class="alignnone size-full wp-image-506" /></a></p>
<p>Create a function fdemo which updates and returns a counter.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/fdemo1.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/fdemo1.jpg" alt="fdemo" width="1440" height="900" class="alignnone size-full wp-image-508" /></a></p>
<p>Create a procedure pdemo which contains a loop of "INSERT INTO tdemo VALUES (fdemo())" statements.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/pdemo.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/pdemo.jpg" alt="pdemo" width="1440" height="900" class="alignnone size-full wp-image-509" /></a></p>
<p>Type the statement: DELIMITER ;<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/delimiter_2.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/delimiter_2.jpg" alt="delimiter_2" width="1440" height="900" class="alignnone size-full wp-image-510" /></a></p>
<p>Type the statement: $install;<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/install.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/install.jpg" alt="install" width="1440" height="900" class="alignnone size-full wp-image-511" /></a></p>
<p>Type the statement: $setup fdemo, pdemo;<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/setup.png"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/setup.png" alt="setup" width="1440" height="900" class="alignnone size-full wp-image-512" /></a></p>
<p>Type the statement: $debug pdemo;<br />
After this statement is executed a tabbed widget appears. The first line of pdemo is highlighted. There is always a breakpoint before the first line.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/debug.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/debug.jpg" alt="debug" width="1440" height="900" class="alignnone size-full wp-image-513" /></a></p>
<p>Click on the Debug menu to see what options are available. Debugger instructions may be done via the menu, via Alt keys, or via command-line statements. For example to enter a Next instruction one may now click Debug|Next, or type Alt+3, or type the statement: "$next;".<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/menu.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/menu.jpg" alt="menu" width="1440" height="900" class="alignnone size-full wp-image-514" /></a></p>
<p>Enter a Next instruction repeatedly, watching how the highlighted line changes, until the INSERT line is highlighted.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/next.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/next.jpg" alt="next" width="1440" height="900" class="alignnone size-full wp-image-515" /></a></p>
<p>Enter a Step instruction. After this is executed, the function now comes into the foreground.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/step.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/step.jpg" alt="step" width="1440" height="900" class="alignnone size-full wp-image-516" /></a></p>
<p>Enter three more Step (or Next) instructions. After these are executed, the procedure now is in the foreground again.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/three_steps.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/three_steps.jpg" alt="three_steps" width="1440" height="900" class="alignnone size-full wp-image-517" /></a></p>
<p>Set a breakpoint for the final executable line of the procedure. This can be done by clicking over the line number, or by moving the cursor to the line and then clicking Debug|Breakpoint, or by typing the statement "$breakpoint pdemo 9;". After this is executed, line 9 of pdemo has a small red mark showing that it has a breakpoint.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/breakpoint.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/breakpoint.jpg" alt="breakpoint" width="1440" height="900" class="alignnone size-full wp-image-518" /></a></p>
<p>Enter a Continue instruction. After this is executed, watch the debugger highlight hop around 100 times as it moves through the loop, and finally settle on line 9.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/continue.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/continue.jpg" alt="continue" width="1440" height="900" class="alignnone size-full wp-image-519" /></a></p>
<p>Type the statements "$refresh variables;" and "select old_value, value from xxxmdbug.variables". After this is executed, the result-set widget will contain the old value of variable i (99) and the current value (100), This is the way that one examines DECLAREd variables (there are other statements for user variables and server variables).<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/select.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/select.jpg" alt="select" width="1440" height="900" class="alignnone size-full wp-image-520" /></a></p>
<p>Enter an Exit instruction. This stops the debugging session, so the effects of the earlier $debug instruction are cancelled. The effects of the earlier $install and $setup instructions are not cancelled, so they will not have to be repeated for the next debugging session involving pdemo and fdemo.<br />
<a href="http://ocelot.ca/blog/wp-content/uploads/2015/03/exit.jpg"><img src="http://ocelot.ca/blog/wp-content/uploads/2015/03/exit.jpg" alt="exit" width="1440" height="900" class="alignnone size-full wp-image-521" /></a></p>
<p>Thus ends our demo. If you would like to confirm it: an introduction for how ocelotgui in general works is in the earlier blog post <a href="http://ocelot.ca/blog/blog/2014/08/19/an-open-source-mysqlmariadb-gui-client-on-linux">An open-source MySQL/MariaDB client on Linux"</a> and the source + executable download files for version 0.3.0 are on <a href="https://github.com/ocelot-inc/ocelotgui">github.com/ocelot-inc/ocelotgui</a>.</p>
<p>If there is anything Ocelot can do to make your next debugging trip more enjoyable, please leave a comment. If you have private thoughts, please write to pgulutzan at ocelot.ca.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989099&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989099&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 18:17:05 +0000</pubDate>
    <dc:creator>Peter Gulutzan</dc:creator>
    <category>MySQL / MariaDB</category>
  </item>

  <item>
    <title>Bad Benchmarketing and the Bar Chart</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-8043938871710850997.post-9200339677114730708</guid>
    <link>http://www.acmebenchmarking.com/2015/03/benchmarketing-charts-hurt-us-all.html</link>
    <description>Technical conferences are flooded with visual [mis]representations of a particular product's performance, compression, cost effectiveness, micro-transactions per flux-capacitor, or whatever two-axis comparison someone dreams up. Lets be honest, benchmarketers like to believe we all suffer from innumeracy. The Merriam-Webster dictionary defines innumeracy as follows:innumeracy (noun): marked by an ignorance of mathematics and the scientific approach Mark Callaghan has been a long time advocate of explaining benchmark results, but that's not the point of the bar chart. Oh no, the bar chart only exists to catch your eye and draw you into the booth for further conversation.I was attending a large name-brand conference in 2014. A well-known hardware vendor was presenting one of the keynotes. A few slides into the deck and it was &quot;Benchmark Time!&quot;, so up came the following bar chart.The visualization of their data is quite dramatic. Their product, the &quot;us&quot; bar, delivering a substantially higher &quot;% Improvement&quot; over their competitions, the &quot;them&quot; bar. On a quick glance your mind tells you, &quot;wow, their product is almost 3x better than the competition&quot;. And a quick glance is all you get because presenters typically spend less than 60 seconds per slide, even less in a keynote. I've been waiting to catch this type of benchmarketer in the wild, so I quickly pulled out my phone and took a picture.Lets break down the events that led to the above bar chart. Long before the keynote the vendor in question asked someone on their technical staff to create a scenario (benchmark) comparing their product to the competition. The request usually includes something like the following, &quot;Make sure we are measurably better than them, but not so much that people won't believe it&quot;. So the technical resource goes away, creates the benchmark, executes it, and presents the following data to the Marketing department.us     58.5them   49.0Now keep in mind, the &quot;us&quot; number of 58.5 is only 19.3% higher than the &quot;them&quot; number of 49.0. A 20% improvement in an important system metric might be huge for certain use-cases, but its not that compelling for general consumption, especially during a keynote. So marketing gets to work with the &quot;data&quot;, which almost seems silly given that the data consists of exactly two numbers.Now any good Marketer will generally fire up Microsoft Excel and see what they can do with this data. Indeed it is almost comical to call it data, this scenario is actually just 2 values.First up is what I call the purely scientific graph. Setting the y-axis range to the possible values (lets use 0 to 100 for this scenario) creates the following graph.Needless to say, this chart doesn't make the cut. There is too little visual difference between the two bars.So next up is an attempt to re-chart the data with a still scientific approach, what I like to call the &quot;we are the best&quot; chart.The goal of this chart is to set something slightly larger than our value as the maximum y value but keep the minimum value at 0, thus making the difference between &quot;us&quot; and &quot;them&quot; more apparent. As with the prior graph, this one is rejected as our awesomeness is not properly conveyed.So it's time to get extreme, and create the &quot;world domination&quot; graph. I've never seen one of these in the wild, but it's just a matter of time.This graph uses a value slightly larger than the &quot;us&quot; as the maximum y-axis value and something slightly smaller than the &quot;them&quot; as the minimum y-axis value. The results are stunning, we're talking order-of-magnitude improvements now. Well done!At this point the presentation starts coming together with with the above slide. Inevitably someone in engineering walks by a printer, sees the chart, and freaks out. Engineering and Marketing negotiate a peaceful settlement and we end up with the chart at the top of this blog. Not ideal, but certainly better than what might have been presented. Benchmarketing for-the-win!So that's it, hopefully that explains the process. Perhaps you're now a little better prepared to question what you see, and question you should. Don't be innumerate.I want to create a page on AcmeBenchmarking with a Benchmarketing Hall of Fame, so please send along any pictures or URLs of the bad benchmarketing you've seen.&amp;nbsp;Also get involved in the comments. Any Marketers have a contrary opinion?</description>
    <content:encoded><![CDATA[<span>Technical conferences are flooded with visual [mis]representations of a particular product's performance, compression, cost effectiveness, micro-transactions per flux-capacitor, or whatever two-axis comparison someone dreams up. </span><span>Lets be honest, benchmarketers like to believe we all suffer from <a href="http://www.merriam-webster.com/dictionary/innumeracy">innumeracy</a>. </span><br /><span><br /></span><span>The <a href="http://www.merriam-webster.com/">Merriam-Webster dictionary</a> defines innumeracy as follows:</span><br /><blockquote><span><i><span>innumeracy (noun): marked by an ignorance of mathematics and the scientific approach </span></i></span></blockquote><span><a href="http://smalldatum.blogspot.com/">Mark Callaghan</a> has been a long time advocate of <a href="http://smalldatum.blogspot.com/2014/06/benchmarketing.html">explaining benchmark results</a>, but that's not the point of the bar chart. Oh no, the bar chart only exists to catch your eye and draw you into the booth for further conversation.</span><br /><span><br /></span><span>I was attending a large name-brand conference in 2014. A well-known hardware vendor was presenting one of the keynotes. A few slides into the deck and it was "Benchmark Time!", so up came the following bar chart.</span><br /><div><span></span></div><div><a href="http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png" imageanchor="1"><img border="0" src="http://2.bp.blogspot.com/-keoyMEm27u4/VPRm_h33YJI/AAAAAAAAB-E/Q89pNWP-CuE/s1600/benchmarketing-graphs-01-original.png" /></a></div><span>The visualization of their data is quite dramatic. Their product, the "us" bar, delivering a substantially higher "% Improvement" over their competitions, the "them" bar. On a quick glance your mind tells you, "wow, their product is almost 3x better than the competition". And a quick glance is all you get because presenters typically spend less than 60 seconds per slide, even less in a keynote. I've been waiting to catch this type of benchmarketer in the wild, so I quickly pulled out my phone and took a picture.</span><br /><span><br /></span><span>Lets break down the events that led to the above bar chart. Long before the keynote the vendor in question asked someone on their technical staff to create a scenario (benchmark) comparing their product to the competition. The request usually includes something like the following, "Make sure we are measurably better than them, but not so much that people won't believe it". So the technical resource goes away, creates the benchmark, executes it, and presents the following data to the Marketing department.</span><br /><br /><pre><code>us     58.5<br />them   49.0</code></pre><span><br /></span><span>Now keep in mind, the "us" number of 58.5 <i><b>is only 19.3% higher</b></i> than the "them" number of 49.0. A 20% improvement in an important system metric might be huge for certain use-cases, but its not that compelling for general consumption, especially during a keynote. So marketing gets to work with the "data", <i>which almost seems silly given that the data consists of exactly two numbers</i>.</span><br /><span><br /></span><span>Now any good Marketer will generally fire up Microsoft Excel and see what they can do with this data. <i>Indeed it is almost comical to call it data, this scenario is actually just 2 values.</i></span><br /><span><br /></span><span>First up is what I call the purely scientific graph. Setting the y-axis range to the possible values (lets use 0 to 100 for this scenario) creates the following graph.</span><br /><span></span><br /><div><a href="http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png" imageanchor="1"><img border="0" src="http://3.bp.blogspot.com/-LC5J9w8kREw/VPRsIIDsIaI/AAAAAAAAB-U/RfjhYNNHQiY/s1600/benchmarketing-graphs-04-0-to-100.png" /></a></div><span>Needless to say, this chart doesn't make the cut. There is too little visual difference between the two bars.</span><br /><span><br /></span><span>So next up is an attempt to re-chart the data with a still scientific approach, what I like to call the "we are the best" chart.</span><br /><span></span><br /><div><a href="http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png" imageanchor="1"><img border="0" src="http://2.bp.blogspot.com/-BiDfiuIf9lQ/VPRsvT78dII/AAAAAAAAB-c/ahUCfGgqIqQ/s1600/benchmarketing-graphs-03-0-to-65.png" /></a></div><span>The goal of this chart is to set something slightly larger than our value as the maximum y value but keep the minimum value at 0, thus making the difference between "us" and "them" more apparent. As with the prior graph, this one is rejected as our awesomeness is not properly conveyed.</span><br /><span><br /></span><span>So it's time to get extreme, and create the "world domination" graph. I've never seen one of these in the wild, but it's just a matter of time.</span><br /><span></span><br /><div><a href="http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png" imageanchor="1"><img border="0" src="http://1.bp.blogspot.com/-B4mai6IhVwg/VPRtXt6bY4I/AAAAAAAAB-k/IeM-bhsssBw/s1600/benchmarketing-graphs-02-48-to-59.png" /></a></div><span>This graph uses a value slightly larger than the "us" as the maximum y-axis value and something slightly smaller than the "them" as the minimum y-axis value. The results are stunning, we're talking order-of-magnitude improvements now. <b>Well done!</b></span><br /><span><br /></span><span>At this point the presentation starts coming together with with the above slide. Inevitably someone in engineering walks by a printer, sees the chart, and freaks out. Engineering and Marketing negotiate a peaceful settlement and we end up with the chart at the top of this blog. Not ideal, but certainly better than what might have been presented. Benchmarketing for-the-win!</span><br /><br /><span>So that's it, hopefully that explains the process. Perhaps you're now a little better prepared to question what you see, and question you should. Don't be innumerate.</span><br /><br /><span>I want to create a page on AcmeBenchmarking with a Benchmarketing Hall of Fame, so please send along any pictures or URLs of the bad benchmarketing </span><span>you've seen.&nbsp;</span><br /><br /><span>Also get involved in the comments. Any Marketers have a contrary opinion?</span><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989095&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989095&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 16:58:00 +0000</pubDate>
    <dc:creator>Tim Callaghan</dc:creator>
    <category>benchmark</category>
    <category>benchmarketing</category>
    <category>marketing</category>
    <category>mongodb</category>
    <category>mysql</category>
  </item>

  <item>
    <title>Emulating MySQL roles with the Percona PAM plugin and proxy users</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28785</guid>
    <link>http://www.percona.com/blog/2015/03/02/emulating-roles-percona-pam-plugin-proxy-users/</link>
    <description>From time to time people wonder how to implement roles in MySQL. This can be useful for companies having to deal with many user accounts or for companies with tight security requirements (PCI or HIPAA for instance). Roles do not exist in regular MySQL but here is an example on how to emulate them using Percona Server, the PAM plugin and proxy users.The goalSay we have 2 databases: db1 and db2, and we want to be able to create 3 roles:db1_dev: can read and write on db1 only.db2_dev: can read and write on db2 only.stats: can read on db1 and db2For each role, we will create one user: joe (db1_dev), mike (db2_dev) and tom (stats).Setting up the Percona PAM pluginThe Percona PAM plugin is distributed with Percona Server 5.5 and 5.6. I will be using Percona Server 5.6 in this post and I will authenticate users with /etc/shadow. As explained here, the setup is easy:Make sure /etc/shadow can be read by the mysql user:# chgrp mysql /etc/shadow
# chmod g+r /etc/shadowInstall the plugin:mysql&amp;gt; INSTALL PLUGIN auth_pam SONAME 'auth_pam.so';Create a /etc/pam.d/mysqld file containing:auth       required     pam_warn.so
auth       required     pam_unix.so audit
account    required     pam_unix.so auditTinkering with the permissions of /etc/shadow may a security concern. Authenticating users against an LDAP server may be a better option. The configuration of the PAM plugin is similar (replace pam_unix.so with pam_ldap.so and forget the part about /etc/shadow).Testing authentication with the PAM pluginNow let&amp;#8217;s create a user:# adduser test_pam
# passwd test_pam
mysql&amp;gt; GRANT ALL PRIVILEGES ON db1.* TO test_pam@localhost IDENTIFIED WITH auth_pam;And let&amp;#8217;s check that the authentication is working as we expect:mysql -utest_pam -p
Enter password:
mysql&amp;gt; show grants;
+-----------------------------------------------------------+
| Grants for test_pam@localhost                             |
+-----------------------------------------------------------+
| GRANT USAGE ON *.* TO 'test_pam'@'localhost'              |
| GRANT ALL PRIVILEGES ON `db1`.* TO 'test_pam'@'localhost' |
+-----------------------------------------------------------+That works! We can delete the user and go to the next step.Creating proxy userThe key to emulate a role is to create a MySQL account for which nobody will know the password (then nobody will be able to use it directly). Instead we will use the PROXY privilege to make sure we map an anonymous account that will match any incoming user to the right MySQL user.So the first step is to create an anonymous user:mysql&amp;gt; CREATE USER ''@'' IDENTIFIED WITH auth_pam AS 'mysqld, pam_db1=db1_dev, pam_db2=db2_dev, pam_stats=stats';The goal of this user is simply to map Unix users in the pam_db1 group to the db1_dev MySQL user, Unix users in the pam_db2 group to the db2_dev MySQL user and Unix users in the pam_stats group to the stats MySQL user.Creating the proxied accountsNow we can create the MySQL users corresponding to each of the roles we want to create:mysql&amp;gt; GRANT SELECT, INSERT ON db1.* TO 'db1_dev'@localhost IDENTIFIED BY 'XXXXX';
mysql&amp;gt; GRANT PROXY ON 'db1_dev'@'localhost' TO ''@'';
mysql&amp;gt; GRANT SELECT, INSERT ON db2.* TO 'db2_dev'@localhost IDENTIFIED BY 'YYYYY';
mysql&amp;gt; GRANT PROXY ON 'db2_dev'@'localhost' TO ''@'';
mysql&amp;gt; GRANT SELECT ON db1.* TO 'stats'@localhost IDENTIFIED BY 'ZZZZZ';
mysql&amp;gt; GRANT SELECT ON db2.* TO 'stats'@localhost;
mysql&amp;gt; GRANT PROXY ON 'stats'@'localhost' TO ''@'';Creating the Unix user accountsThe last step is to create the Unix users joe, mike and tom and assign them the correct group:# useradd joe
# passwd joe
# groupadd pam_db1
# usermod -g pam_db1 joe
# useradd mike
# passwd mike
# groupadd pam_db2
# usermod -g pam_db2 mike
# useradd tom
# passwd tom
# groupadd pam_stats
# usermod -g pam_stats tomAgain you may prefer using an LDAP server to avoid creating the users at the OS level.Testing it out!Let&amp;#8217;s try to connect as mike:# mysql -umike -p
Enter password:
mysql&amp;gt; show grants;
+----------------------------------------------------------------------------------------------------------------+
| Grants for db2_dev@localhost                                                                                   |
+----------------------------------------------------------------------------------------------------------------+
| GRANT USAGE ON *.* TO 'db2_dev'@'localhost' IDENTIFIED BY PASSWORD '*C1DDB6E980040762275B29A316FD993B4A19C108' |
| GRANT SELECT, INSERT ON `db2`.* TO 'db2_dev'@'localhost'                                                       |
+----------------------------------------------------------------------------------------------------------------+Not bad!AlternativesThe Percona PAM plugin is not the only option to use roles:MariaDB 10 supports roles from version 10.0.5Oracle distributes a PAM plugin for MySQL 5.5 and MySQL 5.6 as part of the MySQL Enterprise subscriptionSecurich is a set of stored procedures that has many features regarding user managementGoogle has been offering support for roles through its google-mysql-tools for a long time.ConclusionEven if they are not officially supported, roles can be emulated with an authentication plugin and a proxy user. Let&amp;#8217;s hope that roles will be added in MySQL 5.7!The post Emulating MySQL roles with the Percona PAM plugin and proxy users appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p>From time to time people wonder how to implement roles in MySQL. This can be useful for companies having to deal with many user accounts or for companies with tight security requirements (PCI or HIPAA for instance). Roles do not exist in regular MySQL but here is an example on how to emulate them using Percona Server, the <a href="http://www.percona.com/doc/percona-pam-for-mysql/" target="_blank">PAM plugin</a> and proxy users.</p><h2>The goal</h2><p>Say we have 2 databases: db1 and db2, and we want to be able to create 3 roles:</p><ul><li>db1_dev: can read and write on db1 only.</li><li>db2_dev: can read and write on db2 only.</li><li>stats: can read on db1 and db2</li></ul><p>For each role, we will create one user: joe (db1_dev), mike (db2_dev) and tom (stats).</p><h2>Setting up the Percona PAM plugin</h2><p>The Percona PAM plugin is distributed with Percona Server 5.5 and 5.6. I will be using <a href="http://www.percona.com/software/percona-server" target="_blank">Percona Server 5.6</a> in this post and I will authenticate users with <code>/etc/shadow</code>. As explained <a href="http://www.percona.com/blog/2013/08/14/getting-percona-pam-to-work-with-percona-server-its-client-apps/" target="_blank">here</a>, the setup is easy:</p><ul><li>Make sure <code>/etc/shadow</code> can be read by the mysql user:<br
/><pre># chgrp mysql /etc/shadow
# chmod g+r /etc/shadow</pre></li><li>Install the plugin:<br
/><pre>mysql&gt; INSTALL PLUGIN auth_pam SONAME 'auth_pam.so';</pre></li><li>Create a <code>/etc/pam.d/mysqld</code> file containing:<br
/><pre>auth       required     pam_warn.so
auth       required     pam_unix.so audit
account    required     pam_unix.so audit</pre></li></ul><p>Tinkering with the permissions of <code>/etc/shadow</code> may a security concern. Authenticating users against an LDAP server may be a better option. The configuration of the PAM plugin is similar (replace <code>pam_unix.so</code> with <code>pam_ldap.so</code> and forget the part about <code>/etc/shadow</code>).</p><h2>Testing authentication with the PAM plugin</h2><p>Now let&#8217;s create a user:</p><pre># adduser test_pam
# passwd test_pam
mysql&gt; GRANT ALL PRIVILEGES ON db1.* TO test_pam@localhost IDENTIFIED WITH auth_pam;</pre><p>And let&#8217;s check that the authentication is working as we expect:</p><pre>mysql -utest_pam -p
Enter password:
mysql&gt; show grants;
+-----------------------------------------------------------+
| Grants for test_pam@localhost                             |
+-----------------------------------------------------------+
| GRANT USAGE ON *.* TO 'test_pam'@'localhost'              |
| GRANT ALL PRIVILEGES ON `db1`.* TO 'test_pam'@'localhost' |
+-----------------------------------------------------------+</pre><p>That works! We can delete the user and go to the next step.</p><h2>Creating proxy user</h2><p>The key to emulate a role is to create a MySQL account for which nobody will know the password (then nobody will be able to use it directly). Instead we will use the <code>PROXY</code> privilege to make sure we map an anonymous account that will match any incoming user to the right MySQL user.</p><p>So the first step is to create an anonymous user:</p><pre>mysql&gt; CREATE USER ''@'' IDENTIFIED WITH auth_pam AS 'mysqld, pam_db1=db1_dev, pam_db2=db2_dev, pam_stats=stats';</pre><p>The goal of this user is simply to map Unix users in the pam_db1 group to the db1_dev MySQL user, Unix users in the pam_db2 group to the db2_dev MySQL user and Unix users in the pam_stats group to the stats MySQL user.</p><h2>Creating the proxied accounts</h2><p>Now we can create the MySQL users corresponding to each of the roles we want to create:</p><pre>mysql&gt; GRANT SELECT, INSERT ON db1.* TO 'db1_dev'@localhost IDENTIFIED BY 'XXXXX';
mysql&gt; GRANT PROXY ON 'db1_dev'@'localhost' TO ''@'';
mysql&gt; GRANT SELECT, INSERT ON db2.* TO 'db2_dev'@localhost IDENTIFIED BY 'YYYYY';
mysql&gt; GRANT PROXY ON 'db2_dev'@'localhost' TO ''@'';
mysql&gt; GRANT SELECT ON db1.* TO 'stats'@localhost IDENTIFIED BY 'ZZZZZ';
mysql&gt; GRANT SELECT ON db2.* TO 'stats'@localhost;
mysql&gt; GRANT PROXY ON 'stats'@'localhost' TO ''@'';</pre><p></p><h2>Creating the Unix user accounts</h2><p>The last step is to create the Unix users joe, mike and tom and assign them the correct group:</p><pre># useradd joe
# passwd joe
# groupadd pam_db1
# usermod -g pam_db1 joe
# useradd mike
# passwd mike
# groupadd pam_db2
# usermod -g pam_db2 mike
# useradd tom
# passwd tom
# groupadd pam_stats
# usermod -g pam_stats tom</pre><p>Again you may prefer using an LDAP server to avoid creating the users at the OS level.</p><h2>Testing it out!</h2><p>Let&#8217;s try to connect as mike:</p><pre># mysql -umike -p
Enter password:
mysql&gt; show grants;
+----------------------------------------------------------------------------------------------------------------+
| Grants for db2_dev@localhost                                                                                   |
+----------------------------------------------------------------------------------------------------------------+
| GRANT USAGE ON *.* TO 'db2_dev'@'localhost' IDENTIFIED BY PASSWORD '*C1DDB6E980040762275B29A316FD993B4A19C108' |
| GRANT SELECT, INSERT ON `db2`.* TO 'db2_dev'@'localhost'                                                       |
+----------------------------------------------------------------------------------------------------------------+</pre><p>Not bad!</p><h2>Alternatives</h2><p>The Percona PAM plugin is not the only option to use roles:</p><ul><li>MariaDB 10 supports <a rel="nofollow" href="https://mariadb.com/kb/en/mariadb/roles-overview/" target="_blank">roles</a> from version 10.0.5</li><li>Oracle distributes a <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/pam-authentication-plugin.html" target="_blank">PAM plugin</a> for MySQL 5.5 and MySQL 5.6 as part of the MySQL Enterprise subscription</li><li><a rel="nofollow" href="http://securich.com/" target="_blank">Securich</a> is a set of stored procedures that has many features regarding user management</li><li>Google has been offering support for roles through its <a rel="nofollow" href="https://code.google.com/p/google-mysql-tools/wiki/MysqlRoles" target="_blank">google-mysql-tools</a> for a long time.</li></ul><h2>Conclusion</h2><p>Even if they are not officially supported, roles can be emulated with an authentication plugin and a proxy user. Let&#8217;s hope that roles will be added in MySQL 5.7!</p><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/03/02/emulating-roles-percona-pam-plugin-proxy-users/">Emulating MySQL roles with the Percona PAM plugin and proxy users</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989096&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989096&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 16:50:12 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>Insight for DBAs</category>
    <category>MySQL</category>
    <category>Percona Server</category>
    <category>HIPAA</category>
    <category>MySQL roles</category>
    <category>PCI</category>
    <category>Percona PAM plugin</category>
    <category>Primary</category>
    <category>proxy users</category>
    <category>Stephane Combaudon</category>
  </item>

  <item>
    <title>200M reads per second in MySQL Cluster 7.4</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-14455177.post-7982876796177480655</guid>
    <link>http://mikaelronstrom.blogspot.com/2015/03/200m-reads-per-second-in-mysql-cluster.html</link>
    <description>By courtesy of Intel we had access to a very large cluster of Intel servers for a fewweeks. We took the opportunity to see the improvements of the Intelservers in the new Haswell implementation on the Intel Xeon chips. We also tookthe opportunity to see how far we can now scale flexAsynch, the NoSQL benchmarkwe've developed for testing MySQL Cluster.Last time we tested we were using MySQL Cluster 7.2 and the main bottleneckthen was that the API nodes could not push through more than around 300k readsper second and we have a limit of up to 255 nodes in total. This meant that wewere able to reach a bit more than 70M reads per second using MySQL Cluster 7.2.In MySQL Cluster 7.3 we improved the handling of thread contention in the NDB APIwhich means that we are now able to process much more traffic per API node.In MySQL Cluster 7.4 we also improved the execution in the NDB API receiveprocessing, and we also improved the handling of scans and PK lookups in the datanodes. This meant that now each API node can process more than1M reads per second. This is very good throughput given that each read containsabout 150 bytes. So this means that each socket can handle more than 1Gb/second.To describe what we achieved we'll first describe the HW involved.The machines had 2 sockets with Intel E5-2697 v3 processors. These areHaswell-based Intel Xeon that have 14 cores and 28 CPU threads per CPU socket.Thus a total of 28 cores and 56 CPU threads in each server operating at 2.6GHz basefrequency and a turbo frequency of 3.6GHz. The machines were equipped with64 GByte of memory each. They had an Infiniband connection anda gigabit ethernet port for communication.The communication to the outside was actually limited by the Infiniband interrupthandling. The Infiniband interrupt handling was set up to be latency-optimisedwhich results in higher interrupt rates. We did however manage to push theflexAsynch such that this limitiation was very minor, it limited the performanceloss to within 10% of the maximum performance available.We started testing using just 2 data nodes with 2 replicas. In this test we were ableto reach 13.94M reads per second. Using 4 data nodes we reached28.53M reads per second. Using 8 data nodes we were able to scale it almostlinearly up to 55.30M reads per second. We managed to continue thealmost linear scaling even up to 24 data nodes where we achieved156.5M reads per second. We also achieved 104.7M reads per second on a16-node cluster and 131.7M reads on a 20-node cluster. Finally we took thebenchmark to 32 data nodes where we were able to achieve a new record of205.6M reads per second.The configuration we used in most of these tests had:&amp;nbsp;12 LDM threads, non-HT&amp;nbsp;12 TC threads, HT&amp;nbsp;2 send threads, non-HT&amp;nbsp;8 receive threads, HTwhere HT means that we used both CPU threads in a core and non-HT meantthat we only used one thread per CPU core.We also tried with 20 LDM threads HT, which gave similar results to 12 LDMthreads non-HT. Finally we had threads for replication, main, io and other activitiesthat were not used much in those benchmarks.We compared the improvement of Haswell versus Ivy Bridge (Intel Xeon v2) serversby running a similar configuration with 24 data nodes. With Ivy Bridge(which had 12 cores per socket and thus 24 cores and 48 CPU threads in total) wereached 117.0M reads per second and with Haswell we reached156.5M reads per second. So this is a 33.8% improvement. Important to note hereis that Haswell was slightly limited by the interrupt handling of Infinibandwhereas the Ivy Bridge servers were not &amp;nbsp;imited by this. So the real difference isprobably more in the order of 40-45%.At 24 nodes we tested scaling on number of API nodes. We started at 1 API machineusing 4 API node connections. This gave 4.24M reads per second. We then tried with3 API machines using a total of 12 API node connections where we achieved12.84M reads per second. We then added 3 machines at a time with 12 new APIconnections and this added more than 12M reads per second giving 62.71M readsper second at 15 API machines, 122.8M reads per second at 30 API machines andlinear scaling continued until 37 API machines where we achieved 156.5M readsper second. The best results was achieved at 37 API machines where we achieved156.5M reads per second. Performance of 40 API machines was about the same as at37 API machines at 156.0M reads per second. The performance was saturated heresince the interrupt handling could not handle more packets per second. Evenwithout this the data node was close to saturating the CPUs for both the LDM andthe TC threads and the send threads.Running with clusters like this is interesting. The bottlenecks can be more trickyto find than the normal case. One must remember that running a benchmark with37 API machines and 24 data nodes where each machine has 28 CPU cores, thusmore than 1000 CPU cores are involved, it requires understanding a complexqueueing network.What is interesting here is that the queueing network behaves best if there is somewell behaved bottleneck in the system. This bottleneck ensures that the flowthrough the remainder of the system behaves well. However in some cases wherethere is no bottleneck in the system one can enter into a wave of increasing anddecreasing performance. We have all experienced this type ofbehaviour of queueing networks while being stuck in car queues.What we discovered is that MySQL Cluster can enter such waves if the config doesn'thave any natural bottlenecks. What happens here is that the data nodes are able tosend results back to the API nodes in an eager fashion. This means that the API nodesreceives many small packets to process. Since small packets takes longer to processper byte compared to large packets this has the consequence that the API node slowsdown. This in turn means that the benchmark slows down. After a while the data nodesstarts sending larger packets again to speed things up and again it hits too eagersending.To handle this we introduced a new configuration parameter MaxSendDelay inMySQL Cluster 7.4. This parameter ensures that we are not so eager in sendingresponses back to the API nodes. We will send immediately if there is no othercompeting traffic, but if there is other competing traffic, we will delay sendinga bit to ensure that we're sending larger packets. One can say that we'reintroducing an artificial bottleneck into the send part. This artificial bottleneckcan in some cases improve throughput by as much as 100% and &amp;nbsp;more.The conclusion is that MySQL Cluster 7.4 using the new Haswell computers iscapable of stunning performance. It can deliver 205.6M reads per second ofrecords a bit larger than 100 bytes, thus providing a data flow of more than20 GBytes per second of key lookups or 12.3 billion reads per minute.</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on">By courtesy of Intel we had access to a very large cluster of Intel servers for a few<br />weeks. We took the opportunity to see the improvements of the Intel<br />servers in the new Haswell implementation on the Intel Xeon chips. We also took<br />the opportunity to see how far we can now scale flexAsynch, the NoSQL benchmark<br />we've developed for testing MySQL Cluster.<br /><br />Last time we tested we were using MySQL Cluster 7.2 and the main bottleneck<br />then was that the API nodes could not push through more than around 300k reads<br />per second and we have a limit of up to 255 nodes in total. This meant that we<br />were able to reach a bit more than 70M reads per second using MySQL Cluster 7.2.<br /><br />In MySQL Cluster 7.3 we improved the handling of thread contention in the NDB API<br />which means that we are now able to process much more traffic per API node.<br />In MySQL Cluster 7.4 we also improved the execution in the NDB API receive<br />processing, and we also improved the handling of scans and PK lookups in the data<br />nodes. This meant that now each API node can process more than<br />1M reads per second. This is very good throughput given that each read contains<br />about 150 bytes. So this means that each socket can handle more than 1Gb/second.<br /><br />To describe what we achieved we'll first describe the HW involved.<br />The machines had 2 sockets with Intel E5-2697 v3 processors. These are<br />Haswell-based Intel Xeon that have 14 cores and 28 CPU threads per CPU socket.<br />Thus a total of 28 cores and 56 CPU threads in each server operating at 2.6GHz base<br />frequency and a turbo frequency of 3.6GHz. The machines were equipped with<br />64 GByte of memory each. They had an Infiniband connection and<br />a gigabit ethernet port for communication.<br /><br />The communication to the outside was actually limited by the Infiniband interrupt<br />handling. The Infiniband interrupt handling was set up to be latency-optimised<br />which results in higher interrupt rates. We did however manage to push the<br />flexAsynch such that this limitiation was very minor, it limited the performance<br />loss to within 10% of the maximum performance available.<br /><br />We started testing using just 2 data nodes with 2 replicas. In this test we were able<br />to reach 13.94M reads per second. Using 4 data nodes we reached<br />28.53M reads per second. Using 8 data nodes we were able to scale it almost<br />linearly up to 55.30M reads per second. We managed to continue the<br />almost linear scaling even up to 24 data nodes where we achieved<br />156.5M reads per second. We also achieved 104.7M reads per second on a<br />16-node cluster and 131.7M reads on a 20-node cluster. Finally we took the<br />benchmark to 32 data nodes where we were able to achieve a new record of<br />205.6M reads per second.<br /><br /><div><a href="http://4.bp.blogspot.com/-r-Y3wqeS_wI/VPR9waEf0NI/AAAAAAAAARI/z5kvTYZg-LE/s1600/200-Million-NoSQL-QPS.png" imageanchor="1"><img border="0" src="http://4.bp.blogspot.com/-r-Y3wqeS_wI/VPR9waEf0NI/AAAAAAAAARI/z5kvTYZg-LE/s1600/200-Million-NoSQL-QPS.png" height="379" width="640" /></a></div><br /><br />The configuration we used in most of these tests had:<br />&nbsp;12 LDM threads, non-HT<br />&nbsp;12 TC threads, HT<br />&nbsp;2 send threads, non-HT<br />&nbsp;8 receive threads, HT<br />where HT means that we used both CPU threads in a core and non-HT meant<br />that we only used one thread per CPU core.<br /><br />We also tried with 20 LDM threads HT, which gave similar results to 12 LDM<br />threads non-HT. Finally we had threads for replication, main, io and other activities<br />that were not used much in those benchmarks.<br /><br />We compared the improvement of Haswell versus Ivy Bridge (Intel Xeon v2) servers<br />by running a similar configuration with 24 data nodes. With Ivy Bridge<br />(which had 12 cores per socket and thus 24 cores and 48 CPU threads in total) we<br />reached 117.0M reads per second and with Haswell we reached<br />156.5M reads per second. So this is a 33.8% improvement. Important to note here<br />is that Haswell was slightly limited by the interrupt handling of Infiniband<br />whereas the Ivy Bridge servers were not &nbsp;imited by this. So the real difference is<br />probably more in the order of 40-45%.<br /><br />At 24 nodes we tested scaling on number of API nodes. We started at 1 API machine<br />using 4 API node connections. This gave 4.24M reads per second. We then tried with<br />3 API machines using a total of 12 API node connections where we achieved<br />12.84M reads per second. We then added 3 machines at a time with 12 new API<br />connections and this added more than 12M reads per second giving 62.71M reads<br />per second at 15 API machines, 122.8M reads per second at 30 API machines and<br />linear scaling continued until 37 API machines where we achieved 156.5M reads<br />per second. The best results was achieved at 37 API machines where we achieved<br />156.5M reads per second. Performance of 40 API machines was about the same as at<br />37 API machines at 156.0M reads per second. The performance was saturated here<br />since the interrupt handling could not handle more packets per second. Even<br />without this the data node was close to saturating the CPUs for both the LDM and<br />the TC threads and the send threads.<br /><br />Running with clusters like this is interesting. The bottlenecks can be more tricky<br />to find than the normal case. One must remember that running a benchmark with<br />37 API machines and 24 data nodes where each machine has 28 CPU cores, thus<br />more than 1000 CPU cores are involved, it requires understanding a complex<br />queueing network.<br /><br />What is interesting here is that the queueing network behaves best if there is some<br />well behaved bottleneck in the system. This bottleneck ensures that the flow<br />through the remainder of the system behaves well. However in some cases where<br />there is no bottleneck in the system one can enter into a wave of increasing and<br />decreasing performance. We have all experienced this type of<br />behaviour of queueing networks while being stuck in car queues.<br /><br />What we discovered is that MySQL Cluster can enter such waves if the config doesn't<br />have any natural bottlenecks. What happens here is that the data nodes are able to<br />send results back to the API nodes in an eager fashion. This means that the API nodes<br />receives many small packets to process. Since small packets takes longer to process<br />per byte compared to large packets this has the consequence that the API node slows<br />down. This in turn means that the benchmark slows down. After a while the data nodes<br />starts sending larger packets again to speed things up and again it hits too eager<br />sending.<br /><br />To handle this we introduced a new configuration parameter MaxSendDelay in<br />MySQL Cluster 7.4. This parameter ensures that we are not so eager in sending<br />responses back to the API nodes. We will send immediately if there is no other<br />competing traffic, but if there is other competing traffic, we will delay sending<br />a bit to ensure that we're sending larger packets. One can say that we're<br />introducing an artificial bottleneck into the send part. This artificial bottleneck<br />can in some cases improve throughput by as much as 100% and &nbsp;more.<br /><br />The conclusion is that MySQL Cluster 7.4 using the new Haswell computers is<br />capable of stunning performance. It can deliver 205.6M reads per second of<br />records a bit larger than 100 bytes, thus providing a data flow of more than<br />20 GBytes per second of key lookups or 12.3 billion reads per minute.</div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989094&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989094&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 16:23:00 +0000</pubDate>
    <dc:creator>Mikael Ronstr&amp;ouml;m</dc:creator>
  </item>

  <item>
    <title>Quick bulk load of data into InnoDB</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-8445678881921638771.post-3632817187856812807</guid>
    <link>http://mysql-nordic.blogspot.com/2015/03/quick-bulk-load-of-data-into-innodb.html</link>
    <description>Some weeks back I helped a customer lower time to bulk-load data into MySQL, they where at the time using a MySQL dumpfile (containing SQL statements) to populate their tables during nightly jobs.By using LOAD DATA INFILE command and creating secondary indexes after bulk-load of data load time went down by a factor of almost 2x.My test environment:DB: MySQL 5.6.23OS: Ubuntu 14.04HW: My Toshiba Portege laptop with 2 cores and SSD diskCommands/tables used in tests:CREATE TABLE t1PK (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32)) ENGINE=InnoDB;CREATE TABLE t1 (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32) UNIQUE KEY) ENGINE=InnoDB;LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1 FIELDS TERMINATED BY ',';I created a file named 1000000 that contains two columns and 1 millon rows:$ head -5 10000000,test01,test12,test23,test34,test4.....$ wc -l 10000001000000 1000000Next step is to run some test, if you do not have a BBWC on your disksystem you might consider setting&amp;nbsp;innodb_flush_log_at_trx_commit to 2 during bulk-load of data.First let's create the full table and import the 1000000 file to get a baseline:mysql&amp;gt;&amp;nbsp;set global innodb_flush_log_at_trx_commit=2;mysql&amp;gt;&amp;nbsp;CREATE TABLE t1 (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32) UNIQUE KEY) ENGINE=InnoDB;mysql&amp;gt; LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1 FIELDS TERMINATED BY ',';mysql&amp;gt; set global innodb_flush_log_at_trx_commit=1;Result: Loaded 1.000.000 rows in 4.3 seconds (average from 10 runs)Next let's try to load the file into table with only PK and then add index afterwards:mysql&amp;gt;&amp;nbsp;set global innodb_flush_log_at_trx_commit=2;mysql&amp;gt;&amp;nbsp;CREATE TABLE t1PK (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32)) ENGINE=InnoDB;mysql&amp;gt; LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1PK FIELDS TERMINATED BY ',';mysql&amp;gt; ALTER TABLE t1PK ADD INDEX(art);mysql&amp;gt; set global innodb_flush_log_at_trx_commit=1;Result: Loaded&amp;nbsp;1.000.000 rows in 2.1 seconds (average from 10 runs) and another 1.9 seconds to add secondary index, total time 4 seconds in average. This improvement is due to&amp;nbsp;InnoDB Fast Index CreationConclusion:In my environment it took almost 6 seconds to load a dumpfile (using innodb_flush_log_at_trx_commit=2) of table t1 into mysql, this will depend on how many keys you have and many other factors but as you can see using LOAD DATA INFILE command is a fast way to load data into MySQL!</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on">Some weeks back I helped a customer lower time to bulk-load data into MySQL, they where at the time using a MySQL dumpfile (containing SQL statements) to populate their tables during nightly jobs.<br /><br />By using <i>LOAD DATA INFILE</i> command and creating secondary indexes after bulk-load of data load time went down by a factor of almost 2x.<br /><br /><b>My test environment:</b><br />DB: MySQL 5.6.23<br />OS: Ubuntu 14.04<br />HW: My Toshiba Portege laptop with 2 cores and SSD disk<br /><br /><b>Commands/tables used in tests:</b><br /><table><tbody><tr><td><span>CREATE TABLE t1PK (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32)) ENGINE=InnoDB;</span><br /><span>CREATE TABLE t1 (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32) UNIQUE KEY) ENGINE=InnoDB;</span><br /><span>LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1 FIELDS TERMINATED BY ',';</span></td></tr></tbody></table><br />I created a file named 1000000 that contains two columns and 1 millon rows:<br /><table><tbody><tr><td><span>$ head -5 1000000</span><br /><span>0,test0</span><br /><span>1,test1</span><br /><span>2,test2</span><br /><span>3,test3</span><br /><span>4,test4</span><br /><span>.....</span><br /><span><br /></span><span>$ wc -l 1000000</span><br /><span>1000000 1000000</span></td></tr></tbody></table><br />Next step is to run some test, if you do not have a BBWC on your disksystem you might consider setting&nbsp;innodb_flush_log_at_trx_commit to 2 during bulk-load of data.<br /><br /><b>First let's create the full table and import the 1000000 file to get a baseline:</b><br /><table><tbody><tr><td><span>mysql&gt;&nbsp;</span><span>set global innodb_flush_log_at_trx_commit=2;</span><br /><span>mysql&gt;&nbsp;</span><span>CREATE TABLE t1 (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32) UNIQUE KEY) ENGINE=InnoDB;</span><br /><span>mysql&gt; LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1 FIELDS TERMINATED BY ',';</span><br /><span>mysql&gt; set global innodb_flush_log_at_trx_commit=1;</span></td></tr></tbody></table><br /><b>Result:</b> Loaded 1.000.000 rows in <b>4.3 seconds</b> (average from 10 runs)<br /><div><br /></div><div><b>Next let's try to load the file into table with only PK and then add index afterwards:</b></div><div><table><tbody><tr><td><span>mysql&gt;&nbsp;set global innodb_flush_log_at_trx_commit=2;</span><br /><span>mysql&gt;&nbsp;CREATE TABLE t1PK (i BIGINT UNSIGNED PRIMARY KEY, art VARCHAR(32)) ENGINE=InnoDB;</span><br /><span>mysql&gt; LOAD DATA INFILE '/home/ted/labb/load-data/1000000' INTO TABLE test.t1PK FIELDS TERMINATED BY ',';</span><br /><span>mysql&gt; ALTER TABLE t1PK ADD INDEX(art);</span><br /><span>mysql&gt; set global innodb_flush_log_at_trx_commit=1;</span></td></tr></tbody></table></div><div><b>Result:</b> Loaded&nbsp;1.000.000 rows in 2.1 seconds (average from 10 runs) and another 1.9 seconds to add secondary index, total time <b>4 seconds</b> in average. This improvement is due to&nbsp;<a href="http://dev.mysql.com/doc/refman/5.5/en/innodb-create-index.html" target="_blank">InnoDB Fast Index Creation</a></div><div><br /></div><div><b>Conclusion:</b></div><div>In my environment it took almost <b>6 seconds to load a dumpfile </b>(using innodb_flush_log_at_trx_commit=2) of table t1 into mysql, this will depend on how many keys you have and many other factors but as you can see using <i>LOAD DATA INFILE</i> command is a fast way to load data into MySQL!</div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989090&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989090&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 09:05:00 +0000</pubDate>
    <dc:creator>Ted Wennmark</dc:creator>
  </item>

  <item>
    <title>Temp Tables, Filesorts, UTF8, VARCHAR, and Schema Design in MySQL</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/03/02/temp-tables-filesorts-utf8-varchar-schema-design-mysql/</guid>
    <link>https://vividcortex.com/blog/2015/03/02/temp-tables-filesorts-utf8-varchar-schema-design-mysql/</link>
    <description>The other day we were doing some peer review on a schema change at VividCortex and the topic of character set, column length, and the potential downsides of using utf8 came up. I knew from experience that there are some hidden gotchas with this, and usually I’ll just say a couple words and send a link with further reading. But Google turned up nothing, so I am writing this post to try to help fix that.

TL;DR version is that when MySQL can’t use an index for operations such as ORDER BY, it may allocate fixed-length memory buffers large enough to hold the worst-case values, and the same might apply to temporary tables on disk. This can be seen in EXPLAIN as “Using filesort; using temporary” and similar. And when this happens, you might end up making MySQL do gigabytes worth of work to finish a query on a table that’s only a fraction of that size.

Let’s see why this can happen.

The Theory of UTF8

In theory, you can “just use utf8 for everything,” and there is lots of advice on the Internet suggesting you should. There are good reasons for this. If you use a 1-byte character set, such as the default latin1, and put multibyte data into it (which is very easy to do), you can end up with quite a mess that’s pretty hard to undo. Lots of funny-looking characters can result. You know, those little diamonds with a question mark in the middle, or squares with four little digits inside instead of a character.

In theory, there’s no downside to using utf8 for ASCII data. The “normal” characters all fit in one byte in utf8 anyway, so the bytes end up being the same, regardless of the character set. You’d only get multi-byte characters when you go outside the ASCII range. (I’m being a bit vague with terms like ASCII to avoid the verbosity of being precise. Please correct me, or ask questions, if I’m taking too much of a shortcut.)

And in theory, you can use VARCHAR for everything, too. A VARCHAR begins with a little header that says how long the value is. For VARCHAR(255), for example, there will be 2 bytes that say how long the value is, followed by the data itself. If you use, say, VARCHAR(255) for a column, even if it’s not really going to store values that long, you theoretically pay no extra cost.

So in theory, this is a fine table definition:

CREATE TABLE `t1` (
  `a` varchar(255) CHARACTER SET utf8 NOT NULL,
  `b` varchar(255) CHARACTER SET utf8 NOT NULL,
  KEY `a` (`a`)
)


Now let’s talk about the reality.

Temporary Tables and Filesorts

Suppose you write the following query against the table shown above:

SELECT a FROM t1 WHERE a &amp;gt; 'abc' ORDER BY a LIMIT 20;


MySQL can execute this query by looking into the index on the a column, finding the first value that’s greater than “abc”, and reading the next 20 values. The index is already sorted, so the ORDER BY is automatically satisfied. This is excellent.

The difficulty comes when, for example, you ORDER BY a different column.

SELECT a FROM t1 WHERE a &amp;gt; 'abc' ORDER BY b LIMIT 20;


In this case, MySQL will have to use a so-called “filesort.” Filesort doesn’t really mean files are sorted. It should be called “sort, which may use a file if it overflows the buffer.”

MySQL has a couple of sort algorithms. These are covered in some detail in High Performance MySQL, especially in Chapter 6 under Sort Optimizations. The manual also discusses them, as does Sergey Petrunia’s blog post. In brief, though, MySQL does a sort by putting values into a buffer in memory. The size of this buffer is specified by the sort_buffer_size server variable. If all the values fit into the buffer, MySQL sorts in-memory; if not, then it writes to temp files on disk and does a merge-sort.

This is where the gotchas start to appear and theory diverges from reality in two important ways:


  VARCHAR isn’t variable-length anymore. The table’s on-disk storage may be variable, but the values are stored in fixed-length in memory, at their full possible width. Ditto for the temporary files.
  utf8 isn’t single-byte anymore. The fixed-length values are created large enough to accomodate rows that are all 3-byte characters.


So your “hello, world” row that consumes 2+12 bytes on disk suddenly consumes 2+(3*255)=767 bytes in memory for the sort. Or on disk, if there are too many rows to fit into the sort buffer.

It can potentially get worse than this, too. The dreaded “Using filesort” is bad enough, but it could be “Using temporary; Using filesort” which should strike fear into your heart. This means MySQL is creating a temporary table for part of your query. It might use an in-memory temporary table with the MEMORY storage engine, which pads rows to full-length worst-case; it might also use on-disk MyISAM storage for the temp table too. There’s more about this in High Performance MySQL as well.

Summary

This is a fairly involved topic, with lots of good reading in the manual, the Internet, and The Book Whose Name I Shall Not Keep Repeating.

Details aside, the point is it’s fairly easy (and not uncommon) to create a situation where your temp partition fills up with 10GB of data from a single innocent-looking query against a table that’s a fraction of that size on disk. All because of utf8 and/or long VARCHAR lengths!

My advice is generally to consider things like character sets and VARCHAR length limits in three ways:


  Constraints on what data can/should be stored.
  Accomodation for the data you want to store; make sure your database can hold what you want to put into it.
  Hints and worst-case bounds for the query execution process, which will sometimes be pessimistic/greedy and do the work the schema indicates might be needed, rather than the work that’s actually needed for the data that’s stored.


There’s some balance between overly tight schema constraints, which might force you to do an ALTER in the future (yuck, especially on large tables) versus overly loose constraints, which might surprise you in a bad way. It comes a bit from experience and unless you have a crystal ball, even with experience you’ll get bitten sometimes!

A relevant tweet I saw today:

Bad schemas &amp;lt; no schemas &amp;lt; good schemas - @jaykreps #StrataHadoop #Avro #Kafka&amp;mdash; Tom White (@tom_e_white) February 19, 2015


Happy schema optimizing!

Thumbnail Cred</description>
    <content:encoded><![CDATA[<p>The other day we were doing some peer review on a schema change at VividCortex and the topic of character set, column length, and the potential downsides of using utf8 came up. I knew from experience that there are some hidden gotchas with this, and usually I’ll just say a couple words and send a link with further reading. But Google turned up nothing, so I am writing this post to try to help fix that.</p>

<p>TL;DR version is that when MySQL can’t use an index for operations such as ORDER BY, it may allocate fixed-length memory buffers large enough to hold the worst-case values, and the same might apply to temporary tables on disk. This can be seen in EXPLAIN as “Using filesort; using temporary” and similar. And when this happens, you might end up making MySQL do gigabytes worth of work to finish a query on a table that’s only a fraction of that size.</p>

<p>Let’s see why this can happen.</p>

<h3>The Theory of UTF8</h3>

<p>In theory, you can “just use utf8 for everything,” and there is lots of advice on the Internet suggesting you should. There are good reasons for this. If you use a 1-byte character set, such as the default latin1, and put multibyte data into it (which is very easy to do), you can end up with quite a mess that’s pretty hard to undo. Lots of funny-looking characters can result. You know, those little diamonds with a question mark in the middle, or squares with four little digits inside instead of a character.</p>

<p>In theory, there’s no downside to using utf8 for ASCII data. The “normal” characters all fit in one byte in utf8 anyway, so the bytes end up being the same, regardless of the character set. You’d only get multi-byte characters when you go outside the ASCII range. (I’m being a bit vague with terms like ASCII to avoid the verbosity of being precise. Please correct me, or ask questions, if I’m taking too much of a shortcut.)</p>

<p>And in theory, you can use VARCHAR for everything, too. A VARCHAR begins with a little header that says how long the value is. For VARCHAR(255), for example, there will be 2 bytes that say how long the value is, followed by the data itself. If you use, say, VARCHAR(255) for a column, even if it’s not really going to store values that long, you theoretically pay no extra cost.</p>

<p>So in theory, this is a fine table definition:</p>

<pre><code>CREATE TABLE `t1` (
  `a` varchar(255) CHARACTER SET utf8 NOT NULL,
  `b` varchar(255) CHARACTER SET utf8 NOT NULL,
  KEY `a` (`a`)
)
</code></pre>

<p>Now let’s talk about the reality.</p>

<h3>Temporary Tables and Filesorts</h3>

<p>Suppose you write the following query against the table shown above:</p>

<pre><code>SELECT a FROM t1 WHERE a &gt; 'abc' ORDER BY a LIMIT 20;
</code></pre>

<p>MySQL can execute this query by looking into the index on the <code>a</code> column, finding the first value that’s greater than “abc”, and reading the next 20 values. The index is already sorted, so the ORDER BY is automatically satisfied. This is excellent.</p>

<p>The difficulty comes when, for example, you ORDER BY a different column.</p>

<pre><code>SELECT a FROM t1 WHERE a &gt; 'abc' ORDER BY b LIMIT 20;
</code></pre>

<p>In this case, MySQL will have to use a so-called “filesort.” Filesort doesn’t really mean files are sorted. It should be called “sort, which may use a file if it overflows the buffer.”</p>

<p>MySQL has a couple of sort algorithms. These are covered in some detail in High Performance MySQL, especially in Chapter 6 under Sort Optimizations. The <a href="http://dev.mysql.com/doc/refman/5.0/en/order-by-optimization.html">manual</a> also discusses them, as does <a href="http://s.petrunia.net/blog/?p=24">Sergey Petrunia’s blog post</a>. In brief, though, MySQL does a sort by putting values into a buffer in memory. The size of this buffer is specified by the <code>sort_buffer_size</code> server variable. If all the values fit into the buffer, MySQL sorts in-memory; if not, then it writes to temp files on disk and does a merge-sort.</p>

<p>This is where the gotchas start to appear and theory diverges from reality in two important ways:</p>

<ol>
  <li>VARCHAR isn’t variable-length anymore. The table’s on-disk storage may be variable, but the values are stored in fixed-length in memory, at their full possible width. Ditto for the temporary files.</li>
  <li>utf8 isn’t single-byte anymore. The fixed-length values are created large enough to accomodate rows that are all 3-byte characters.</li>
</ol>

<p>So your “hello, world” row that consumes 2+12 bytes on disk suddenly consumes 2+(3*255)=767 bytes in memory for the sort. Or on disk, if there are too many rows to fit into the sort buffer.</p>

<p>It can potentially get worse than this, too. The dreaded “Using filesort” is bad enough, but it could be “Using temporary; Using filesort” which should strike fear into your heart. This means MySQL is creating a temporary table for part of your query. It might use an in-memory temporary table with the MEMORY storage engine, which pads rows to full-length worst-case; it might also use on-disk MyISAM storage for the temp table too. There’s more about this in High Performance MySQL as well.</p>

<h3>Summary</h3>

<p>This is a fairly involved topic, with lots of good reading in the manual, the Internet, and The Book Whose Name I Shall Not Keep Repeating.</p>

<p>Details aside, the point is it’s fairly easy (and not uncommon) to create a situation where your temp partition fills up with 10GB of data from a single innocent-looking query against a table that’s a fraction of that size on disk. All because of utf8 and/or long VARCHAR lengths!</p>

<p>My advice is generally to consider things like character sets and VARCHAR length limits in three ways:</p>

<ol>
  <li>Constraints on what data can/should be stored.</li>
  <li>Accomodation for the data you want to store; make sure your database can hold what you want to put into it.</li>
  <li>Hints and worst-case bounds for the query execution process, which will sometimes be pessimistic/greedy and do the work the schema indicates might be needed, rather than the work that’s actually needed for the data that’s stored.</li>
</ol>

<p>There’s some balance between overly tight schema constraints, which might force you to do an ALTER in the future (yuck, especially on large tables) versus overly loose constraints, which might surprise you in a bad way. It comes a bit from experience and unless you have a crystal ball, even with experience you’ll get bitten sometimes!</p>

<p>A relevant tweet I saw today:</p>

<blockquote lang="en"><p>Bad schemas &lt; no schemas &lt; good schemas - <a href="https://twitter.com/jaykreps">@jaykreps</a> <a href="https://twitter.com/hashtag/StrataHadoop?src=hash">#StrataHadoop</a> <a href="https://twitter.com/hashtag/Avro?src=hash">#Avro</a> <a href="https://twitter.com/hashtag/Kafka?src=hash">#Kafka</a></p>&mdash; Tom White (@tom_e_white) <a href="https://twitter.com/tom_e_white/status/568487477733937152">February 19, 2015</a></blockquote>


<p>Happy schema optimizing!</p>

<p><a href="https://www.flickr.com/photos/theo_reth/8477010686/in/photolist-dV5TWE-ns2xSm-fZXQN-HaBPH-8QE9w1-25ytqE-3UpnZ-7Kdmmm-rmcp21-8pA3Yy-9e35mw-6KevU2-bhrh6F-p36xBm-9g8ZYe-cKTzrQ-His6V-RP3Fv-orKxaK-bKLJh-7KA9ry-5BMQGq-5YsSfx-xf96A-ahK6ij-yNEVs-25ytqh-8RJjgm-aPJKcX-Jy9Y-fcYfcV-a6MYms-pe8HGL-dsyiG4-eHQbVh-3H14A-8MHsFb-q6pUQ9-oJfCXf-8agvvp-4pNvrT-6DRSfL-5e8nYP-9J9Y3o-fJnoF9-gokeo-ahGiAD-6tsJE-oV5c6B-4Ug1bm">Thumbnail Cred</a></p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989093&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989093&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 02 Mar 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Log Buffer #412, A Carnival of the Vanities for DBAs</title>
    <guid isPermaLink="false">http://www.pythian.com/blog/?p=72403</guid>
    <link>http://www.pythian.com/blog/log-buffer-412-a-carnival-of-the-vanities-for-dbas/</link>
    <description>This Log Buffer Edition makes it way through the realms of Oracle, SQL Server and MySQL and brings you some of the blog posts.
Oracle:
Introducing Oracle Big Data Discovery Part 3: Data Exploration and Visualization
FULL and NO_INDEX Hints
Base64 Encode / Decode with Python (or WebLogic Scripting Tool) by Frank Munz
Why I’m Excited About Oracle Integration Cloud Service – New Video
Reminder: Upgrade Database 12.1.0.1 to 12.1.0.2 by July 2015
SQL Server:
An article about how we underestimate the power of joins and degrade our query performance by not using proper joins
Most large organizations have implemented one or more big data applications. As more data accumulates internal users and analysts execute more reports and forecasts, which leads to additional queries and analysis, and more reporting.
How do you develop and deploy your database?
A database must be able to maintain and enforce the business rules and relationships in data in order to maintain the data model.
Error handling with try-catch-finally in PowerShell for SQL Server
MySQL:
MySQL Enterprise Monitor 3.0.20 has been released
MySQL Cluster 7.4 is GA!
Connector/Python 2.1.1 Alpha released with C Extension
Worrying about the ‘InnoDB: detected cycle in LRU for buffer pool (…)’ message?
MySQL Cluster 7.4 GA: 200 Million QPS, Active-Active Geographic Replication and more</description>
    <content:encoded><![CDATA[<p>This Log Buffer Edition makes it way through the realms of Oracle, SQL Server and MySQL and brings you some of the blog posts.</p>
<p><span></span><strong>Oracle:</strong></p>
<p>Introducing Oracle <a href="http://www.rittmanmead.com/2015/02/introducing-oracle-big-data-discovery-part-2-data-exploration-and-visualization/">Big</a> Data Discovery Part 3: Data Exploration and Visualization</p>
<p><a href="http://international-dba.blogspot.com/2015/02/full-and-noindex-hints.html">FULL</a> and NO_INDEX Hints</p>
<p><a href="https://blogs.oracle.com/emeapartnerweblogic/entry/base64_encode_decode_with_python">Base64</a> Encode / Decode with Python (or WebLogic Scripting Tool) by Frank Munz</p>
<p>Why I’m <a href="https://blogs.oracle.com/SOA/entry/why_i_m_excited_about">Excited</a> About Oracle Integration Cloud Service – New Video</p>
<p>Reminder: <a href="http://blogs.oracle.com/stevenChan/entry/db12_1_0_1_ecs">Upgrade</a> Database 12.1.0.1 to 12.1.0.2 by July 2015</p>
<p><strong>SQL Server:</strong></p>
<p>An <a href="http://www.sqlservercentral.com/articles/T-SQL/88443/">article</a> about how we underestimate the power of joins and degrade our query performance by not using proper joins</p>
<p>Most <a href="http://www.sqlservercentral.com/redirect/articles/122604/">large</a> organizations have implemented one or more big data applications. As more data accumulates internal users and analysts execute more reports and forecasts, which leads to additional queries and analysis, and more reporting.</p>
<p>How do you <a href="http://www.sqlservercentral.com/redirect/articles/122105/">develop</a> and deploy your database?</p>
<p>A <a href="http://www.sqlservercentral.com/redirect/articles/122601/">database</a> must be able to maintain and enforce the business rules and relationships in data in order to maintain the data model.</p>
<p>Error handling with try-catch-finally in <a href="http://www.sqlservercentral.com/redirect/articles/122603/">PowerShell</a> for SQL Server</p>
<p><strong>MySQL:</strong></p>
<p><a href="https://blogs.oracle.com/mysqlenterprise/entry/mysql_enterprise_monitor_3_018">MySQL</a> Enterprise Monitor 3.0.20 has been released</p>
<p><a href="http://www.clusterdb.com/mysql-cluster/mysql-cluster-7-4-is-ga-200-million-qps-and-active-active-geor">MySQL</a> Cluster 7.4 is GA!</p>
<p>Connector/<a href="http://geert.vanderkelen.org/mysql-connector-python-2-1-1/">Python</a> 2.1.1 Alpha released with C Extension</p>
<p>Worrying about the ‘<a href="http://www.percona.com/blog/2015/02/26/worrying-about-the-innodb-detected-cycle-in-lru-for-buffer-pool-message/">InnoDB</a>: detected cycle in LRU for buffer pool (…)’ message?</p>
<p><a href="http://dev.mysql.com/tech-resources/articles/mysql-cluster-7.4.html">MySQL</a> Cluster 7.4 GA: 200 Million QPS, Active-Active Geographic Replication and more</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989080&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989080&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Fri, 27 Feb 2015 16:58:06 +0000</pubDate>
    <dc:creator>The Pythian Group</dc:creator>
    <category>Pythian</category>
    <category>Log Buffer</category>
  </item>

  <item>
    <title>MySQL Replication and GTID-based failover - A Deep Dive into Errant Transactions</title>
    <guid isPermaLink="false">4384 at http://www.severalnines.com</guid>
    <link>http://www.severalnines.com/blog/mysql-replication-and-gtid-based-failover-deep-dive-errant-transactions</link>
    <description>For years, MySQL replication used to be based on binary log events - all a slave knew was the exact event and the exact position it just read from the master. Any single transaction from a master may have ended in different binary logs, and in different positions in these logs. It was a simple solution that came with limitations - more complex topology changes could require an admin to stop replication on the hosts involved. Or these changes could cause some other issues, e.g., a slave couldn’t be moved down the replication chain without time-consuming rebuild process (we couldn’t easily change replication from A -&amp;gt; B -&amp;gt; C to A -&amp;gt; C -&amp;gt; B without stopping replication on both B and C). We’ve all had to work around these limitations while dreaming about a global transaction identifier.
GTID was introduced along with MySQL 5.6, and brought along some major changes in the way MySQL operates. First of all, every transaction has an unique identifier which identifies it in a same way on every server. It’s not important anymore in which binary log position a transaction was recorded, all you need to know is the GTID: ‘966073f3-b6a4-11e4-af2c-080027880ca6:4’. GTID is built from two parts - the unique identifier of a server where a transaction was first executed, and a sequence number. In the above example, we can see that the transaction was executed by the server with server_uuid of ‘966073f3-b6a4-11e4-af2c-080027880ca6’ and it’s 4th transaction executed there. This information is enough to perform complex topology changes - MySQL knows which transactions have been executed and therefore it knows which transactions need to be executed next. Forget about binary logs, it’s all in the GTID.
So, where can you find GTID’s? You’ll find them in two places. On a slave, in 'show slave status;'&amp;nbsp;you’ll find two columns: Retrieved_Gtid_Set and Executed_Gtid_Set. First one covers GTID’s which were retrieved from the master via replication, the second informs about all transactions which were executed on given host - both via replication or executed locally.&amp;nbsp;
&amp;nbsp;
Setting up a Replication Cluster the easy way

We’ll use the Severalnines Configurator to automatically deploy our replication setup. First, you need to point your browser at:
http://www.severalnines.com/replication-configurator/
The first page doesn’t give you too many options so you can as well click on ‘Next’ button.


The next screen contains some options regarding operating system, where the infrastructure will be created and so forth. All of options are explained in details, one thing that may be worth commenting is the ‘Number of MySQL Slaves’. By default, the deploy scripts create a master-master pair connected by semi-sync replication. This is the smallest possible block created when you set the number of slaves to 0. Every slave that you want to create will be connected to this master-master pair.


The third screen is related to the MySQL configuration of the database nodes - you can define how ‘large’ nodes will be in terms of CPU and memory, you can also set up InnoDB buffer pool sizes and predicted workload pattern.
The last screen lets you fill in the IP addresses of the ClusterControl server and the nodes in the replication setup.&amp;nbsp;
Finally, you need to fill in your email address to which file with deployment scripts will be sent.
&amp;nbsp;
Deployment
For this blog post, let’s assume we want to create infrastructure on premises. We’ll use couple of Vagrant nodes. Deployment on EC2 may work in a slightly different way that what you see below.
When all nodes are up and running, you need to copy the tarball that you received via email, to the ClusterControl node. Next, &amp;nbsp;untar it, go to the install directory and execute the deploy.sh script:

$ tar zxf s9s-mysql-56.tar.gz
$ cd s9s-mysql-56/mysql/scripts/install/
$ ./deploy.shAt the beginning of the deploy process you’ll be asked the following:

Can you SSH from this host to all other hosts without password?
Choosing 'n' will allow you to setup shared keys. (y/n):If you have passwordless ssh set up already, you can choose ‘y’ here. In other case you’ll be asked to provide passwords for root user on all nodes to generate and distribute ssh keys.
The deployment will continue, and you should have you replication setup up and running after 15 to 20 minutes. We can now &amp;nbsp;take a look at how GTID works.

Errant transactions - &amp;nbsp;what is the issue?
As we mentioned at the beginning of this post, GTID’s brought a significant change in the way people should think about MySQL replication. It’s all about habits. Let’s say, for some reason, that an application performed a write on one of the slaves. It shouldn’t have happened but surprisingly, it happens all the time. As a result, replication stops with duplicate key error. There are couple of ways to deal with such problem. One of them would be to delete the offending row and restart replication. Other one would be to skip the binary log event and then restart replication.&amp;nbsp;

mysql&amp;gt; STOP SLAVE SQL_THREAD; SET GLOBAL sql_slave_skip_counter = 1; START SLAVE SQL_THREAD;Both ways should bring replication back to work, but they may introduce data drift so it is necessary to remember that slave consistency should be checked after such event (pt-table-checksum and pt-table-sync works well here).
If a similar problem happens while using GTID, you’ll notice some differences. Deleting the offending row may seem to fix the issue, replication should be able to commence. The other method, using sql_slave_skip_counter won’t work at all - it’ll return an error. Remember, it’s now not about binlog events, it’s all about GTID being executed or not.
Why deleting the row only ‘seems’ to fix the issue? One of the most important things to keep in mind regarding GTID is that a slave, when connecting to the master, checks if it is missing any transactions which were executed on the master. These are called errant transactions. If a slave finds such transactions, it will execute them. Let’s assume we ran following SQL to clear an offending row:

mysql&amp;gt; DELETE FROM mytable WHERE id=100;Let’s check show slave status:

                  Master_UUID: 966073f3-b6a4-11e4-af2c-080027880ca6
           Retrieved_Gtid_Set: 966073f3-b6a4-11e4-af2c-080027880ca6:1-29
            Executed_Gtid_Set: 84d15910-b6a4-11e4-af2c-080027880ca6:1,
966073f3-b6a4-11e4-af2c-080027880ca6:1-29,
And see where the 84d15910-b6a4-11e4-af2c-080027880ca6:1 comes from:

mysql&amp;gt; SHOW VARIABLES LIKE 'server_uuid'\G
*************************** 1. row ***************************
Variable_name: server_uuid
        Value: 84d15910-b6a4-11e4-af2c-080027880ca6
1 row in set (0.00 sec)As you can see, we have 29 transactions that came from the master, UUID of 966073f3-b6a4-11e4-af2c-080027880ca6 and one that was executed locally. Let’s say that at some point we failover and the master (966073f3-b6a4-11e4-af2c-080027880ca6) becomes a slave. It will check its list of executed GTID’s and will not find this one: 84d15910-b6a4-11e4-af2c-080027880ca6:1. As a result, the related SQL will be executed:

mysql&amp;gt; DELETE FROM mytable WHERE id=100;This is not something we expected… If, in the meantime, the binlog containing this transaction would be purged on the old slave, then the new slave will complain after failover:

                Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.'
&amp;nbsp;
How to detect errant transactions?
MySQL provides two functions which come in very handy when you want to compare GTID sets on different hosts.
GTID_SUBSET() takes two GTID sets and checks if the first set is a subset of the second one.
&amp;nbsp;Let’s say we have following state.
Master:

mysql&amp;gt; show master status\G
*************************** 1. row ***************************
             File: binlog.000002
         Position: 160205927
     Binlog_Do_DB:
 Binlog_Ignore_DB:
Executed_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1,
ab8f5793-b907-11e4-bebd-080027880ca6:1-2
1 row in set (0.00 sec)Slave:

mysql&amp;gt; show slave status\G
[...]
           Retrieved_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1
            Executed_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1,
ab8f5793-b907-11e4-bebd-080027880ca6:1-4We can check if the slave has any errant transactions by executing the following SQL:

mysql&amp;gt; SELECT GTID_SUBSET('8a6962d2-b907-11e4-bebc-080027880ca6:1-153,ab8f5793-b907-11e4-bebd-080027880ca6:1-4', '8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2') as is_subset\G
*************************** 1. row ***************************
is_subset: 0
1 row in set (0.00 sec)Looks like there are errant transactions. How do we identify them? We can use another function, GTID_SUBTRACT()

mysql&amp;gt; SELECT GTID_SUBTRACT('8a6962d2-b907-11e4-bebc-080027880ca6:1-153,ab8f5793-b907-11e4-bebd-080027880ca6:1-4', '8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2') as mising\G
*************************** 1. row ***************************
mising: ab8f5793-b907-11e4-bebd-080027880ca6:3-4
1 row in set (0.01 sec)Our missing GTID’s are ab8f5793-b907-11e4-bebd-080027880ca6:3-4 - those transactions were executed on the slave but not on the master.
&amp;nbsp;
How to solve issues caused by errant transactions?
There are two ways - inject empty transactions or exclude transactions from GTID history.
To inject empty transactions we can use the following SQL:

mysql&amp;gt; SET gtid_next='ab8f5793-b907-11e4-bebd-080027880ca6:3';
Query OK, 0 rows affected (0.01 sec)

mysql&amp;gt; begin ; commit;
Query OK, 0 rows affected (0.00 sec)
  
Query OK, 0 rows affected (0.01 sec)

mysql&amp;gt; SET gtid_next='ab8f5793-b907-11e4-bebd-080027880ca6:4';
Query OK, 0 rows affected (0.00 sec)

mysql&amp;gt; begin ; commit;
Query OK, 0 rows affected (0.00 sec)

Query OK, 0 rows affected (0.01 sec)

mysql&amp;gt; SET gtid_next=automatic;
Query OK, 0 rows affected (0.00 sec)This has to be executed on every host in the replication topology that does not have those GTID’s executed. If the master is available, you can inject those transactions there and let them replicate down the chain. If the master is not available (for example, it crashed), those empty transactions have to be executed on every slave. Oracle developed a tool called mysqlslavetrx which is designed to automate this process.
Another approach is to remove the GTID’s from history:
Stop slave:

mysql&amp;gt; STOP SLAVE;Print Executed_Gtid_Set on the slave:

mysql&amp;gt; SHOW MASTER STATUS\GReset GTID info:

mysql&amp;gt; RESET MASTER;Set GTID_PURGED to a correct GTID set. based on data from SHOW MASTER STATUS. You should exclude errant transactions from the set:

mysql&amp;gt; SET GLOBAL GTID_PURGED='8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2';Start slave:

mysql&amp;gt; START SLAVE\GIn every case, you should verify consistency of your slaves using pt-table-checksum and pt-table-sync (if needed) - errant transaction may result in a data drift.
Blog category: DB OpsTags: errant transactionfailovergtidMariaDBMySQLreplication</description>
    <content:encoded><![CDATA[<div><div><div property="content:encoded"><p><span>For years, MySQL replication used to be based on binary log events - all a slave knew was the exact event and the exact position it just read from the master. Any single transaction from a master may have ended in different binary logs, and in different positions in these logs. It was a simple solution that came with limitations - more complex topology changes could require an admin to stop replication on the hosts involved. Or these changes could cause some other issues, e.g., a slave couldn’t be moved down the replication chain without time-consuming rebuild process (we couldn’t easily change replication from A -&gt; B -&gt; C to A -&gt; C -&gt; B without stopping replication on both B and C). We’ve all had to work around these limitations while dreaming about a global transaction identifier.</span></p>
<p>GTID was introduced along with MySQL 5.6, and brought along some major changes in the way MySQL operates. First of all, every transaction has an unique identifier which identifies it in a same way on every server. It’s not important anymore in which binary log position a transaction was recorded, all you need to know is the GTID: ‘966073f3-b6a4-11e4-af2c-080027880ca6:4’. GTID is built from two parts - the unique identifier of a server where a transaction was first executed, and a sequence number. In the above example, we can see that the transaction was executed by the server with <var>server_uuid</var> of ‘966073f3-b6a4-11e4-af2c-080027880ca6’ and it’s 4<sup>th</sup> transaction executed there. This information is enough to perform complex topology changes - MySQL knows which transactions have been executed and therefore it knows which transactions need to be executed next. Forget about binary logs, it’s all in the GTID.</p>
<p>So, where can you find GTID’s? You’ll find them in two places. On a slave, in '<span>show slave status;'</span>&nbsp;you’ll find two columns: <var><span>Retrieved_Gtid_Set</span></var> and <var><span>Executed_Gtid_Set</span></var>. First one covers GTID’s which were retrieved from the master via replication, the second informs about all transactions which were executed on given host - both via replication or executed locally.&nbsp;</p>
<p>&nbsp;</p>
<p><span><strong>Setting up a Replication Cluster the easy way</strong></span></p>
<p><mediawrapper data=""><img alt="" class="media-element file-default" data-fid="3649" data-media-element="1" src="http://www.severalnines.com/sites/default/files/mrgt_arch.png" style="height: 428px; width: 550px;" typeof="foaf:Image" /></mediawrapper></p>
<p>We’ll use the Severalnines Configurator to automatically deploy our replication setup. First, you need to point your browser at:<br />
<a href="http://www.severalnines.com/replication-configurator/">http://www.severalnines.com/replication-configurator/</a></p>
<p>The first page doesn’t give you too many options so you can as well click on ‘Next’ button.</p>
<p><mediawrapper data=""><img alt="" class="media-element file-default" data-fid="3650" data-media-element="1" src="http://www.severalnines.com/sites/default/files/mrgt_conf1.png" style="height: 249px; width: 650px;" typeof="foaf:Image" /></mediawrapper></p>
<p>
The next screen contains some options regarding operating system, where the infrastructure will be created and so forth. All of options are explained in details, one thing that may be worth commenting is the ‘Number of MySQL Slaves’. By default, the deploy scripts create a master-master pair connected by semi-sync replication. This is the smallest possible block created when you set the number of slaves to 0. Every slave that you want to create will be connected to this master-master pair.</p>
<p><mediawrapper data=""><img alt="" class="media-element file-default" data-fid="3651" data-media-element="1" src="http://www.severalnines.com/sites/default/files/mrgt_conf2.png" style="height: 556px; width: 650px;" typeof="foaf:Image" /></mediawrapper></p>
<p>
The third screen is related to the MySQL configuration of the database nodes - you can define how ‘large’ nodes will be in terms of CPU and memory, you can also set up InnoDB buffer pool sizes and predicted workload pattern.</p>
<p>The last screen lets you fill in the IP addresses of the ClusterControl server and the nodes in the replication setup.&nbsp;</p>
<p>Finally, you need to fill in your email address to which file with deployment scripts will be sent.</p>
<p>&nbsp;</p>
<p><strong><span>Deployment</span></strong></p>
<p>For this blog post, let’s assume we want to create infrastructure on premises. We’ll use couple of Vagrant nodes. Deployment on EC2 may work in a slightly different way that what you see below.</p>
<p>When all nodes are up and running, you need to copy the tarball that you received via email, to the ClusterControl node. Next, &nbsp;untar it, go to the install directory and execute the deploy.sh script:</p>
<pre>
<code>$ tar zxf s9s-mysql-56.tar.gz
$ cd s9s-mysql-56/mysql/scripts/install/
$ ./deploy.sh</code></pre><p>At the beginning of the deploy process you’ll be asked the following:</p>
<pre>
<code>Can you SSH from this host to all other hosts without password?
Choosing 'n' will allow you to setup shared keys. (y/n):</code></pre><p>If you have passwordless ssh set up already, you can choose ‘y’ here. In other case you’ll be asked to provide passwords for root user on all nodes to generate and distribute ssh keys.</p>
<p>The deployment will continue, and you should have you replication setup up and running after 15 to 20 minutes. We can now &nbsp;take a look at how GTID works.</p>
<p>
<strong><span>Errant transactions - &nbsp;what is the issue?</span></strong></p>
<p>As we mentioned at the beginning of this post, GTID’s brought a significant change in the way people should think about MySQL replication. It’s all about habits. Let’s say, for some reason, that an application performed a write on one of the slaves. It shouldn’t have happened but surprisingly, it happens all the time. As a result, replication stops with duplicate key error. There are couple of ways to deal with such problem. One of them would be to delete the offending row and restart replication. Other one would be to skip the binary log event and then restart replication.&nbsp;</p>
<pre>
<code>mysql&gt; STOP SLAVE SQL_THREAD; SET GLOBAL sql_slave_skip_counter = 1; START SLAVE SQL_THREAD;</code></pre><p>Both ways should bring replication back to work, but they may introduce data drift so it is necessary to remember that slave consistency should be checked after such event (pt-table-checksum and pt-table-sync works well here).</p>
<p>If a similar problem happens while using GTID, you’ll notice some differences. Deleting the offending row may seem to fix the issue, replication should be able to commence. The other method, using sql_slave_skip_counter won’t work at all - it’ll return an error. Remember, it’s now not about binlog events, it’s all about GTID being executed or not.</p>
<p>Why deleting the row only ‘seems’ to fix the issue? One of the most important things to keep in mind regarding GTID is that a slave, when connecting to the master, checks if it is missing any transactions which were executed on the master. These are called errant transactions. If a slave finds such transactions, it will execute them. Let’s assume we ran following SQL to clear an offending row:</p>
<pre>
<code>mysql&gt; DELETE FROM mytable WHERE id=100;</code></pre><p>Let’s check show slave status:</p>
<pre>
<code>                  Master_UUID: 966073f3-b6a4-11e4-af2c-080027880ca6
           Retrieved_Gtid_Set: 966073f3-b6a4-11e4-af2c-080027880ca6:1-29
            Executed_Gtid_Set: 84d15910-b6a4-11e4-af2c-080027880ca6:1,
966073f3-b6a4-11e4-af2c-080027880ca6:1-29,
</code></pre><p><span>And see where the 84d15910-b6a4-11e4-af2c-080027880ca6:1 comes from:</span></p>
<pre>
<code>mysql&gt; SHOW VARIABLES LIKE 'server_uuid'\G
*************************** 1. row ***************************
Variable_name: server_uuid
        Value: 84d15910-b6a4-11e4-af2c-080027880ca6
1 row in set (0.00 sec)</code></pre><p>As you can see, we have 29 transactions that came from the master, UUID of 966073f3-b6a4-11e4-af2c-080027880ca6 and one that was executed locally. Let’s say that at some point we failover and the master (966073f3-b6a4-11e4-af2c-080027880ca6) becomes a slave. It will check its list of executed GTID’s and will not find this one: 84d15910-b6a4-11e4-af2c-080027880ca6:1. As a result, the related SQL will be executed:</p>
<pre>
<code>mysql&gt; DELETE FROM mytable WHERE id=100;</code></pre><p>This is not something we expected… If, in the meantime, the binlog containing this transaction would be purged on the old slave, then the new slave will complain after failover:</p>
<pre>
<code>                Last_IO_Error: Got fatal error 1236 from master when reading data from binary log: 'The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.'
</code></pre><h2>&nbsp;</h2>
<p><span><strong>How to detect errant transactions?</strong></span></p>
<p>MySQL provides two functions which come in very handy when you want to compare GTID sets on different hosts.</p>
<p>GTID_SUBSET() takes two GTID sets and checks if the first set is a subset of the second one.</p>
<p>&nbsp;Let’s say we have following state.<br />
Master:</p>
<pre>
<code>mysql&gt; show master status\G
*************************** 1. row ***************************
             File: binlog.000002
         Position: 160205927
     Binlog_Do_DB:
 Binlog_Ignore_DB:
Executed_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1,
ab8f5793-b907-11e4-bebd-080027880ca6:1-2
1 row in set (0.00 sec)</code></pre><p>Slave:</p>
<pre>
<code>mysql&gt; show slave status\G
[...]
           Retrieved_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1
            Executed_Gtid_Set: 8a6962d2-b907-11e4-bebc-080027880ca6:1-153,
9b09b44a-b907-11e4-bebd-080027880ca6:1,
ab8f5793-b907-11e4-bebd-080027880ca6:1-4</code></pre><p>We can check if the slave has any errant transactions by executing the following SQL:</p>
<pre>
<code>mysql&gt; SELECT GTID_SUBSET('8a6962d2-b907-11e4-bebc-080027880ca6:1-153,ab8f5793-b907-11e4-bebd-080027880ca6:1-4', '8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2') as is_subset\G
*************************** 1. row ***************************
is_subset: 0
1 row in set (0.00 sec)</code></pre><p>Looks like there are errant transactions. How do we identify them? We can use another function, GTID_SUBTRACT()</p>
<pre>
<code>mysql&gt; SELECT GTID_SUBTRACT('8a6962d2-b907-11e4-bebc-080027880ca6:1-153,ab8f5793-b907-11e4-bebd-080027880ca6:1-4', '8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2') as mising\G
*************************** 1. row ***************************
mising: ab8f5793-b907-11e4-bebd-080027880ca6:3-4
1 row in set (0.01 sec)</code></pre><p>Our missing GTID’s are ab8f5793-b907-11e4-bebd-080027880ca6:3-4 - those transactions were executed on the slave but not on the master.</p>
<p>&nbsp;</p>
<p><strong><span>How to solve issues caused by errant transactions?</span></strong></p>
<p>There are two ways - inject empty transactions or exclude transactions from GTID history.<br />
To inject empty transactions we can use the following SQL:</p>
<pre>
<code>mysql&gt; SET gtid_next='ab8f5793-b907-11e4-bebd-080027880ca6:3';
Query OK, 0 rows affected (0.01 sec)

mysql&gt; begin ; commit;
Query OK, 0 rows affected (0.00 sec)
  
Query OK, 0 rows affected (0.01 sec)

mysql&gt; SET gtid_next='ab8f5793-b907-11e4-bebd-080027880ca6:4';
Query OK, 0 rows affected (0.00 sec)

mysql&gt; begin ; commit;
Query OK, 0 rows affected (0.00 sec)

Query OK, 0 rows affected (0.01 sec)

mysql&gt; SET gtid_next=automatic;
Query OK, 0 rows affected (0.00 sec)</code></pre><p>This has to be executed on every host in the replication topology that does not have those GTID’s executed. If the master is available, you can inject those transactions there and let them replicate down the chain. If the master is not available (for example, it crashed), those empty transactions have to be executed on every slave. Oracle developed a tool called mysqlslavetrx which is designed to automate this process.</p>
<p>Another approach is to remove the GTID’s from history:</p>
<p>Stop slave:</p>
<pre>
<code>mysql&gt; STOP SLAVE;</code></pre><p>Print Executed_Gtid_Set on the slave:</p>
<pre>
<code>mysql&gt; SHOW MASTER STATUS\G</code></pre><p>Reset GTID info:</p>
<pre>
<code>mysql&gt; RESET MASTER;</code></pre><p>Set GTID_PURGED to a correct GTID set. based on data from SHOW MASTER STATUS. You should exclude errant transactions from the set:</p>
<pre>
<code>mysql&gt; SET GLOBAL GTID_PURGED='8a6962d2-b907-11e4-bebc-080027880ca6:1-153, 9b09b44a-b907-11e4-bebd-080027880ca6:1, ab8f5793-b907-11e4-bebd-080027880ca6:1-2';</code></pre><p>Start slave:</p>
<pre>
<code>mysql&gt; START SLAVE\G</code></pre><p>In every case, you should verify consistency of your slaves using pt-table-checksum and pt-table-sync (if needed) - errant transaction may result in a data drift.</p>
</div></div></div><div><h3>Blog category: </h3><ul><li><a href="http://www.severalnines.com/blog-categories/db-ops" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">DB Ops</a></li></ul></div><div><div><div><img typeof="foaf:Image" src="http://www.severalnines.com/sites/default/files/mrgt-feat.PNG" width="397" height="368" alt="" /></div></div></div><div><h3>Tags: </h3><ul><li><a href="http://www.severalnines.com/blog-tags/errant-transaction" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">errant transaction</a></li><li><a href="http://www.severalnines.com/blog-tags/failover" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">failover</a></li><li><a href="http://www.severalnines.com/blog-tags/gtid" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">gtid</a></li><li><a href="http://www.severalnines.com/blog-tags/mariadb" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MariaDB</a></li><li><a href="http://www.severalnines.com/blog-tags/mysql" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL</a></li><li><a href="http://www.severalnines.com/blog-tags/replication" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">replication</a></li></ul></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989073&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989073&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Fri, 27 Feb 2015 11:13:53 +0000</pubDate>
    <dc:creator>Severalnines</dc:creator>
  </item>

  <item>
    <title>3 handy tools to remove problematic MySQL processes</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28148</guid>
    <link>http://www.percona.com/blog/2015/02/27/3-handy-tools-to-remove-problematic-mysql-processes/</link>
    <description>DBAs often encounter situations where they need to kill queries to ensure there are no long-running queries on a MySQL server that would impact performance. Long-running queries can be the result of many factors. Fortunately, Percona Server contains some handy tools to remove problematic MySQL processes. I will highlight all of the tools via some examples in this post.pt-kill: There have been some good posts on this blog about the pt-kill tool, like this one by Arunjith Aravindan titled &amp;#8220;How a set of queries can be killed in MySQL using Percona Toolkit’s pt-kill.&amp;#8221; Let&amp;#8217;s dive into pt-kill a bit further with a few more examples. What does pt-kill do? It kills MySQL connections. Say you wanted to run pt-kill from a cronjob and then get an email on every killed process/query. Here is typical example for that.$ pt-kill --interval 1 --run-time 1 --busy-time 5 --log /path/to/kill_long_running_thread.log --match-info &quot;^(select|SELECT|Select)&quot; --kill --print --user=xxxxxx --password=xxxxxxxxxxAssume this is running from a cronjob, When pt-kill executes, it will kill queries longer than 5 seconds. By default, pt-kill runs forever &amp;#8211;run-time option tells how long pt-kill to run before exiting If &amp;#8211;interval and &amp;#8211;busy-time parameters are used together then the explicit &amp;#8211;interval value is used. Otherwise the default interval is 30 seconds. Note: this will only kill all read queries as per the &amp;#8211;match-info parameter.The above command will log all killed queries in the file referenced with the &amp;#8211;log option. If you need to be notified via email for every killed query, the command below will do it. Off-course, you need to have the system configured to send e-mail.#!/bin/bash
tail -n 0 -F /path/to/kill_long_running_thread.log | while read LOG
do
echo &quot;$LOG&quot; | mail -s &quot;pt-kill alert&quot; sample@test.com
doneYou can execute this shell script in the background within screen or with the nohup utility continuously to monitor the log file. It will send an email whenever any new killed query arrives to the referenced log file in the pt-kill command. Unfortunately, there is no option to notify-by-email in pt-kill at the moment, so this is sort of a workaround.In order to log all killed queries into a database table you will need to use the &amp;#8211;log-dsn option as per the example below.$ pt-kill --interval 1 --busy-time 1 --create-log-table --log-dsn=h=localhost,D=percona,t=kill_log --daemonize --match-info &quot;^(select|SELECT|Select)&quot; --killAll killed queries will be logged into percona.kill_log table. The &amp;#8211;daemonize option will run this command in the background forever and will kill all SELECT queries running longer than 1 second (&amp;#8211;busy-time 1). The &amp;#8211;interval option instructs pt-kill to scan processes every 1 second (&amp;#8211;interval 1).mysql&amp;gt; select * from kill_log;
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+
| kill_id | server_id | timestamp           | reason                  | kill_error | Id    | User | Host      | db   | Command | Time | State      | Info            | Time_ms |
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+
|      17 |         1 | 2015-01-10 08:38:33 | Query matches Info spec |            | 35146 | root | localhost | NULL | Query   |    0 | User sleep | SELECT SLEEP(5) |    NULL |
|      20 |         1 | 2015-01-10 08:38:34 | Query matches Info spec |            | 35223 | root | localhost | NULL | Query   |    0 | User sleep | SELECT SLEEP(5) |    NULL |
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+With the help of logging killed queries into a database tables. You can easily get all the trends/and /statistics on killed queries via SQL.By default the tool kills the oldest of the queries that would have been running for more than a given &amp;#8211;busy-time.  If you need to kill all the threads that have been busy for more than a specified &amp;#8211;busy-time, then this will do it:$ pt-kill --victims=all --busy-time=60Statement Timeout in Percona Server: The max-statement-time feature is ported from the Twitter patches. This feature can be used to limit the query execution time by specifying the timeout value in the max_statement_time variable. When the specified number of milliseconds is reached the server aborts the statement and returns the error below to the client. ERROR 1877 (70101): Query execution was interrupted, max_statement_time exceededLet me demonstrate this through another example:mysql [localhost] {msandbox} (world) &amp;gt; SET max_statement_time=1;
Query OK, 0 rows affected (0.00 sec)
mysql [localhost] {msandbox} (world) &amp;gt; show variables like 'max_statement_time';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| max_statement_time | 1     |
+--------------------+-------+
1 row in set (0.00 sec)
mysql [localhost] {msandbox} (world) &amp;gt; SELECT * FROM City WHERE District = 'abcd';
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceeded
mysql [localhost] {msandbox} (world) &amp;gt; UPDATE City SET District='abcd' WHERE ID = 2001;
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceeded
mysql [localhost] {msandbox} (world) &amp;gt; ALTER TABLE City ADD INDEX district_idx (district);
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceededAs you can see from this example statement, the timeout feature works for all statements including SELECT/DML/DDL queries.mysql [localhost] {msandbox} (world) &amp;gt; show status like 'Max_statement%';
+-------------------------------+-------+
| Variable_name                 | Value |
+-------------------------------+-------+
| Max_statement_time_exceeded   | 3     |
| Max_statement_time_set        | 19    |
| Max_statement_time_set_failed | 0     |
+-------------------------------+-------+
3 rows in set (0.00 sec)The above mentioned status variables are stats for a statement timeout feature. Max_statement_time_exceeded will inform you that the total number of statements exceeded the defined timeout. Max_statement_time_set defines the number of statements for which execution time limit was set. You can find more details in this documentation. The statement timeout feature was introduced in Percona Server 5.6. You can check if your specific version of Percona Server supports this feature or not via the have_statement_timeout variable.mysql [localhost] {msandbox} (world) &amp;gt; show global variables like 'have_statement_timeout';
+------------------------+-------+
| Variable_name          | Value |
+------------------------+-------+
| have_statement_timeout | YES   |
+------------------------+-------+
1 row in set (0.00 sec)Bugs you should be aware of:https://bugs.launchpad.net/percona-server/+bug/1388533 -&amp;gt; This affects how the feature interacts with stored procedures. If you use stored procedures, max_statement_time might not behave as you expect. https://bugs.launchpad.net/percona-server/+bug/1307432 -&amp;gt; This is documentation bug. Percona Server timeouts might not be safe for some statements like DDL and should not be used with such queries, The documentation does not reflect this. You should be very careful if you set a global statement timeout, It affects data changing queries as well. For best results set the max_statement_time variable in a session before running queries that you want to be killed if they execute too long, instead of using a global variable. https://bugs.launchpad.net/percona-server/+bug/1376934 -&amp;gt; This affects the statement timeout feature on the query level. You must set max_statement_time in a session or globally instead however, this bug is fixed in latest version i.e. Percona Server 5.6.22-72.0InnoDB Kill Idle Transactions: This feature was introduced in Percona Server 5.5. It limits the age of idle XtraDB transactions and will kill idle transactions longer than a specified threshold for innodb_kill_idle_transaction. This feature is useful when autocommit is disabled on the server side and you are relying on the application to commit transactions and want to avoid long running transactions that are uncommitted. Application logic errors sometimes leaves transactions uncommitted. Let me demonstrate it quickly through one example:mysql [localhost] {msandbox} (world) &amp;gt; show variables like 'autocommit';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| autocommit    | OFF   |
+---------------+-------+
mysql [localhost] {msandbox} (world) &amp;gt; show global variables like 'innodb_kill_idle_transaction';
+------------------------------+-------+
| Variable_name                | Value |
+------------------------------+-------+
| innodb_kill_idle_transaction | 10    |
+------------------------------+-------+
mysql [localhost] {msandbox} (world) &amp;gt; START TRANSACTION; SELECT NOW(); INSERT INTO City_backup (Name,CountryCode,District,Population) VALUES ('Karachi','PK','Sindh','1000000');
Query OK, 0 rows affected (0.00 sec)
+---------------------+
| NOW()               |
+---------------------+
| 2015-01-31 07:11:39 |
+---------------------+
1 row in set (0.00 sec)
Query OK, 1 row affected (0.01 sec)
mysql [localhost] {msandbox} (world) &amp;gt; SHOW ENGINE INNODB STATUSG
*************************** 1. row ***************************
------------
TRANSACTIONS
------------
---TRANSACTION 173076, ACTIVE 10 sec
1 lock struct(s), heap size 360, 0 row lock(s), undo log entries 1
MySQL thread id 15, OS thread handle 0x7f862e9bf700, query id 176 localhost msandbox init
SHOW ENGINE InnoDB STATUS
TABLE LOCK table `world`.`City_backup` trx id 173076 lock mode IX
----------------------------
END OF INNODB MONITOR OUTPUT
============================
mysql [localhost] {msandbox} (world) &amp;gt; SELECT NOW(); SELECT * FROM City_backup;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    16
Current database: world
+---------------------+
| NOW()               |
+---------------------+
| 2015-01-31 07:12:06 |
+---------------------+
1 row in set (0.01 sec)
Empty set (0.00 sec) Conclusion: In this post, I shared some tools that can help you get rid of long-running transactions to help ensure that you don&amp;#8217;t run into performance issues. This is one of the many good reasons to use Percona Server, which has some extra features on top of vanilla MySQL server.&amp;nbsp;The post 3 handy tools to remove problematic MySQL processes appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p><img class="alignright wp-image-28849 size-thumbnail" src="http://www.percona.com/blog/wp-content/uploads/2015/02/3-handy-tools-to-remove-problematic-MySQL-processes-150x150.jpg" alt="3 handy tools to remove problematic MySQL processes" width="150" height="150" />DBAs often encounter situations where they need to kill queries to ensure there are no long-running queries on a MySQL server that would impact performance. Long-running queries can be the result of many factors. Fortunately, <a title="Percona Server" href="http://www.percona.com/software/percona-server" target="_blank">Percona Server</a> contains some handy tools to remove problematic MySQL processes. I will highlight all of the tools via some examples in this post.</p><p><strong>pt-kill:</strong><br
/> There have been some good posts on this blog about the pt-kill tool, like this one by Arunjith Aravindan titled &#8220;<a href="http://www.percona.com/blog/2014/11/19/how-a-set-of-queries-can-be-killed-in-mysql-using-pt-kill/" target="_blank">How a set of queries can be killed in MySQL using Percona Toolkit’s pt-kill</a>.&#8221; Let&#8217;s dive into <a title="pt-kill" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html" target="_blank">pt-kill</a> a bit further with a few more examples. What does pt-kill do? It kills MySQL connections. Say you wanted to run pt-kill from a cronjob and then get an email on every killed process/query. Here is typical example for that.</p><pre>$ pt-kill --interval 1 --run-time 1 --busy-time 5 --log /path/to/kill_long_running_thread.log --match-info "^(select|SELECT|Select)" --kill --print --user=xxxxxx --password=xxxxxxxxxx</pre><p>Assume this is running from a cronjob, When pt-kill executes, it will kill queries longer than 5 seconds. By default, pt-kill runs forever <a title="--run-time" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--run-time" target="_blank">&#8211;run-time</a> option tells how long pt-kill to run before exiting If <a title="--interval" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--interval" target="_blank">&#8211;interval</a> and <a title="--busy-time" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--busy-time" target="_blank">&#8211;busy-time</a> parameters are used together then the explicit &#8211;interval value is used. Otherwise the default interval is 30 seconds. Note: this will only kill all read queries as per the <a title="--match-info" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--match-info" target="_blank">&#8211;match-info</a> parameter.</p><p>The above command will log all killed queries in the file referenced with the <a title="--log" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--log" target="_blank">&#8211;log</a> option. If you need to be notified via email for every killed query, the command below will do it. Off-course, you need to have the system configured to send e-mail.</p><pre>#!/bin/bash
tail -n 0 -F /path/to/kill_long_running_thread.log | while read LOG
do
echo "$LOG" | mail -s "pt-kill alert" sample@test.com
done</pre><p>You can execute this shell script in the background within <a title="screen" rel="nofollow" href="http://www.computerhope.com/unix/screen.htm" target="_blank">screen</a> or with the <a title="nohup" rel="nofollow" href="http://linux.die.net/man/1/nohup" target="_blank">nohup</a> utility continuously to monitor the log file. It will send an email whenever any new killed query arrives to the referenced log file in the pt-kill command. Unfortunately, there is no option to notify-by-email in pt-kill at the moment, so this is sort of a workaround.</p><p>In order to log all killed queries into a database table you will need to use the <a title="--log-dsn" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--log-dsn" target="_blank">&#8211;log-dsn</a> option as per the example below.</p><pre>$ pt-kill --interval 1 --busy-time 1 --create-log-table --log-dsn=h=localhost,D=percona,t=kill_log --daemonize --match-info "^(select|SELECT|Select)" --kill</pre><p>All killed queries will be logged into percona.kill_log table. The <a title="--daemonize" href="http://www.percona.com/doc/percona-toolkit/2.2/pt-kill.html#cmdoption-pt-kill--daemonize" target="_blank">&#8211;daemonize</a> option will run this command in the background forever and will kill all SELECT queries running longer than 1 second (&#8211;busy-time 1). The &#8211;interval option instructs pt-kill to scan processes every 1 second (&#8211;interval 1).</p><pre>mysql&gt; select * from kill_log;
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+
| kill_id | server_id | timestamp           | reason                  | kill_error | Id    | User | Host      | db   | Command | Time | State      | Info            | Time_ms |
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+
|      17 |         1 | 2015-01-10 08:38:33 | Query matches Info spec |            | 35146 | root | localhost | NULL | Query   |    0 | User sleep | SELECT SLEEP(5) |    NULL |
|      20 |         1 | 2015-01-10 08:38:34 | Query matches Info spec |            | 35223 | root | localhost | NULL | Query   |    0 | User sleep | SELECT SLEEP(5) |    NULL |
+---------+-----------+---------------------+-------------------------+------------+-------+------+-----------+------+---------+------+------------+-----------------+---------+</pre><p>With the help of logging killed queries into a database tables. You can easily get all the trends/and /statistics on killed queries via SQL.</p><p>By default the tool kills the oldest of the queries that would have been running for more than a given &#8211;busy-time.  If you need to kill all the threads that have been busy for more than a specified &#8211;busy-time, then this will do it:</p><pre>$ pt-kill --victims=all --busy-time=60</pre><p><strong>Statement Timeout in Percona Server:<br
/> </strong>The max-statement-time feature is ported from the Twitter patches. This feature can be used to limit the query execution time by specifying the timeout value in the <a title="max_statement_time" href="http://www.percona.com/doc/percona-server/5.6/management/statement_timeout.html#max_statement_time" target="_blank"><tt><span>max_statement_time</span></tt></a><span> </span>variable. When the specified number of milliseconds is reached the server<span> aborts the statement and returns the error below to the client.</span><strong><br
/> </strong></p><pre>ERROR 1877 (70101): Query execution was interrupted, max_statement_time exceeded</pre><p>Let me demonstrate this through another example:</p><pre>mysql [localhost] {msandbox} (world) &gt; SET max_statement_time=1;
Query OK, 0 rows affected (0.00 sec)
mysql [localhost] {msandbox} (world) &gt; show variables like 'max_statement_time';
+--------------------+-------+
| Variable_name      | Value |
+--------------------+-------+
| max_statement_time | 1     |
+--------------------+-------+
1 row in set (0.00 sec)
mysql [localhost] {msandbox} (world) &gt; SELECT * FROM City WHERE District = 'abcd';
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceeded
mysql [localhost] {msandbox} (world) &gt; UPDATE City SET District='abcd' WHERE ID = 2001;
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceeded
mysql [localhost] {msandbox} (world) &gt; ALTER TABLE City ADD INDEX district_idx (district);
ERROR 1885 (70101): Query execution was interrupted, max_statement_time exceeded</pre><p>As you can see from this example statement, the timeout feature works for all statements including SELECT/DML/DDL queries.</p><pre>mysql [localhost] {msandbox} (world) &gt; show status like 'Max_statement%';
+-------------------------------+-------+
| Variable_name                 | Value |
+-------------------------------+-------+
| Max_statement_time_exceeded   | 3     |
| Max_statement_time_set        | 19    |
| Max_statement_time_set_failed | 0     |
+-------------------------------+-------+
3 rows in set (0.00 sec)</pre><p>The above mentioned status variables are stats for a statement timeout feature. <a title="Max_statement_time_exceeded" href="http://www.percona.com/doc/percona-server/5.6/management/statement_timeout.html#Max_statement_time_exceeded" target="_blank">Max_statement_time_exceeded</a> will inform you that the total number of statements exceeded the defined timeout. <a title="Max_statement_time_set" href="http://www.percona.com/doc/percona-server/5.6/management/statement_timeout.html#Max_statement_time_set" target="_blank">Max_statement_time_set</a> defines the number of statements for which execution time limit was set. You can find more details in this <a title="documentation" href="http://www.percona.com/doc/percona-server/5.6/management/statement_timeout.html" target="_blank">documentation</a>. The statement timeout feature was introduced in Percona Server 5.6. You can check if your specific version of Percona Server supports this feature or not via the <a title="have_statement_timeout" href="http://www.percona.com/doc/percona-server/5.6/management/statement_timeout.html#have_statement_timeout" target="_blank">have_statement_timeout</a> variable.</p><pre>mysql [localhost] {msandbox} (world) &gt; show global variables like 'have_statement_timeout';
+------------------------+-------+
| Variable_name          | Value |
+------------------------+-------+
| have_statement_timeout | YES   |
+------------------------+-------+
1 row in set (0.00 sec)</pre><p>Bugs you should be aware of:</p><p><a title="https://bugs.launchpad.net/percona-server/+bug/1388533" rel="nofollow" href="https://bugs.launchpad.net/percona-server/+bug/1388533" target="_blank">https://bugs.launchpad.net/percona-server/+bug/1388533</a> -&gt; This affects how the feature interacts with stored procedures. If you use stored procedures, max_statement_time might not behave as you expect.<br
/> <a title="https://bugs.launchpad.net/percona-server/+bug/1307432" rel="nofollow" href="https://bugs.launchpad.net/percona-server/+bug/1307432" target="_blank">https://bugs.launchpad.net/percona-server/+bug/1307432</a> -&gt; This is documentation bug. Percona Server timeouts might not be safe for some statements like DDL and should not be used with such queries, The documentation does not reflect this. You should be very careful if you set a global statement timeout, It affects data changing queries as well. For best results set the max_statement_time variable in a session before running queries that you want to be killed if they execute too long, instead of using a global variable.<br
/> <a title="https://bugs.launchpad.net/percona-server/+bug/1376934" rel="nofollow" href="https://bugs.launchpad.net/percona-server/+bug/1376934" target="_blank">https://bugs.launchpad.net/percona-server/+bug/1376934</a> -&gt; This affects the statement timeout feature on the query level. You must set max_statement_time in a session or globally instead however, this bug is fixed in latest version i.e. <a rel="nofollow" href="https://launchpad.net/percona-server/+milestone/5.6.22-72.0">Percona Server 5.6.22-72.0</a></p><p><strong>InnoDB Kill Idle Transactions:</strong><br
/> This feature was introduced in Percona Server 5.5. It limits the age of idle XtraDB transactions and will kill idle transactions longer than a specified threshold for <a title="innodb_kill_idle_transaction" href="http://www.percona.com/doc/percona-server/5.5/management/innodb_kill_idle_trx.html#innodb_kill_idle_transaction" target="_blank">innodb_kill_idle_transaction</a>. This feature is useful when autocommit is disabled on the server side and you are relying on the application to commit transactions and want to avoid long running transactions that are uncommitted. Application logic errors sometimes leaves transactions uncommitted. Let me demonstrate it quickly through one example:</p><pre>mysql [localhost] {msandbox} (world) &gt; show variables like 'autocommit';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| autocommit    | OFF   |
+---------------+-------+
mysql [localhost] {msandbox} (world) &gt; show global variables like 'innodb_kill_idle_transaction';
+------------------------------+-------+
| Variable_name                | Value |
+------------------------------+-------+
| innodb_kill_idle_transaction | 10    |
+------------------------------+-------+
mysql [localhost] {msandbox} (world) &gt; START TRANSACTION; SELECT NOW(); INSERT INTO City_backup (Name,CountryCode,District,Population) VALUES ('Karachi','PK','Sindh','1000000');
Query OK, 0 rows affected (0.00 sec)
+---------------------+
| NOW()               |
+---------------------+
| 2015-01-31 07:11:39 |
+---------------------+
1 row in set (0.00 sec)
Query OK, 1 row affected (0.01 sec)
mysql [localhost] {msandbox} (world) &gt; SHOW ENGINE INNODB STATUSG
*************************** 1. row ***************************
------------
TRANSACTIONS
------------
---TRANSACTION 173076, ACTIVE 10 sec
1 lock struct(s), heap size 360, 0 row lock(s), undo log entries 1
MySQL thread id 15, OS thread handle 0x7f862e9bf700, query id 176 localhost msandbox init
SHOW ENGINE InnoDB STATUS
TABLE LOCK table `world`.`City_backup` trx id 173076 lock mode IX
----------------------------
END OF INNODB MONITOR OUTPUT
============================
mysql [localhost] {msandbox} (world) &gt; SELECT NOW(); SELECT * FROM City_backup;
ERROR 2006 (HY000): MySQL server has gone away
No connection. Trying to reconnect...
Connection id:    16
Current database: world
+---------------------+
| NOW()               |
+---------------------+
| 2015-01-31 07:12:06 |
+---------------------+
1 row in set (0.01 sec)
Empty set (0.00 sec)</pre><p><strong> Conclusion:</strong><br
/> In this post, I shared some tools that can help you get rid of long-running transactions to help ensure that you don&#8217;t run into performance issues. This is one of the many good reasons to use Percona Server, which has some extra features on top of vanilla MySQL server.</p><p>&nbsp;</p><div></div><div></div><div></div><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/02/27/3-handy-tools-to-remove-problematic-mysql-processes/">3 handy tools to remove problematic MySQL processes</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989069&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989069&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Fri, 27 Feb 2015 08:00:54 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>Insight for DBAs</category>
    <category>MySQL</category>
    <category>Percona Server</category>
    <category>innodb_kill_idle_transaction</category>
    <category>Muhammad Irfan</category>
    <category>Primary</category>
    <category>pt-kill</category>
  </item>

  <item>
    <title>Baron Schwartz Presents on Time-Series Data at Scale13X</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/02/27/baron-schwartz-presents-at-scale13x/</guid>
    <link>https://vividcortex.com/blog/2015/02/27/baron-schwartz-presents-at-scale13x/</link>
    <description>This past weekend we had the pleasure of attending Scale13X in Los Angeles. It was full of great conversations with solid people, and it was nice to escape the winter cold for a couple days.

Baron Schwartz presented on VivdCortex’s approach to building a time-series database in MySQL. The talk is titled Scaling VividCortex’s Big Data Systems on MySQL, and the recording and video can be seen here. If you are not automaticaly directed, Baron’s presentation  begins at 5:49:00. 

Below are the slides and a brief description to pique your interest.



In this talk, Baron Schwartz discusses VividCortex’s unique time-series data requirements and implementation, including:


  How we built a solution using MySQL and additional components such as Redis
  Why we needed more than just MySQL to meet the requirements
  The good and bad aspects of our architecture
  Performance statistics
  Thoughts for the future of our time-series data architecture


You will leave the talk with a greater understanding of MySQL’s capabilities related to time-series data.

Baron has presented a webinar on this before, and you can register for a copy of it here.</description>
    <content:encoded><![CDATA[<p>This past weekend we had the pleasure of attending <a href="http://www.socallinuxexpo.org/scale/13x">Scale13X</a> in Los Angeles. It was full of great conversations with solid people, and it was nice to escape the winter cold for a couple days.</p>

<p>Baron Schwartz presented on VivdCortex’s approach to building a time-series database in MySQL. The talk is titled Scaling VividCortex’s Big Data Systems on MySQL, and the recording and video can be seen <a href="https://www.youtube.com/watch?v=kTD1NuQXr4k&amp;t=5h49m27s">here</a>. If you are not automaticaly directed, Baron’s presentation  begins at 5:49:00. </p>

<p>Below are the slides and a brief description to pique your interest.</p>



<p>In this talk, Baron Schwartz discusses VividCortex’s unique time-series data requirements and implementation, including:</p>

<ul>
  <li>How we built a solution using MySQL and additional components such as Redis</li>
  <li>Why we needed more than just MySQL to meet the requirements</li>
  <li>The good and bad aspects of our architecture</li>
  <li>Performance statistics</li>
  <li>Thoughts for the future of our time-series data architecture</li>
</ul>

<p>You will leave the talk with a greater understanding of MySQL’s capabilities related to time-series data.</p>

<p>Baron has presented a webinar on this before, and you can register for a copy of it <a href="https://vividcortex.com/webinars/building-time-series-database-in-mysql/?utm_source=site&amp;utm_medium=blog">here</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989077&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989077&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Fri, 27 Feb 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>OurSQL Episode 205: How to Learn About MySQL</title>
    <guid isPermaLink="false">5737 at http://www.technocation.org</guid>
    <link>http://www.technocation.org/content/oursql-episode-205-how-learn-about-mysql</link>
    <description>PodcastsLearning
                          
                          
                          
                          
                          
                          In this episode, we discuss resources available for learning more about MySQL and its forks.
</description>
    <content:encoded><![CDATA[<div><div><div><a href="http://www.technocation.org/category/areas/podcasts" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Podcasts</a></div></div></div><div><div><div><a href="http://www.technocation.org/category/mysql-articles/learning" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Learning</a></div></div></div><div><div><div>
                          
                          
                          
                          
                          
                          </div></div></div><div><div><div property="content:encoded"><p>In this episode, we discuss resources available for learning more about MySQL and its forks.</p>
</div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989065&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989065&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 17:48:56 +0000</pubDate>
    <dc:creator>Technocation</dc:creator>
  </item>

  <item>
    <title>MySQL Cluster 7.4 is GA!</title>
    <guid isPermaLink="false">http://www.clusterdb.com/?p=4005</guid>
    <link>http://www.clusterdb.com/mysql-cluster/mysql-cluster-7-4-is-ga-200-million-qps-and-active-active-geor</link>
    <description>
The General Availability of MySQL Cluster 7.4 has just been announced by Oracle.
The MySQL team at Oracle are excited to announce the General Availability of MySQL Cluster 7.4, ready for production workloads.
MySQL Cluster 7.4.4 can be downloaded from mysql.com and the release notes viewed in the MySQL docs.
Figure 1 provides a summary of the enhancements delivered in this release:

Performance

200M NoSQL Reads/Sec
2.5M SQL Ops/Sec
50% Faster Reads
40% Faster Mixed


Active-Active

Active-Active Geographic Redundancy
Conflict Detection/Resolution


Management

5X Faster Maintenance Ops
Detailed Reporting



Figure 1: MySQL Cluster 7.4 content summary
The sections that follow delve into some more details.
Performance
Figure 2: Read/Writes 40% Faster than MySQL Cluster 7.3
Figure 3: Reads 50% Faster than MySQL Cluster 7.3
50% Faster Reads than MySQL Cluster 7.3
Being a scaled-out, in-memory, real-time database, MySQL Cluster performance has always been great but we continue to work on making it faster each release. In particular, we want to keep pace with the trend of having more and more cores rather than faster ones. 7.4 continues along the path of better exploiting multiple cores &amp;#8211; as can be seen from these benchmark results:

Figure 3 shows that for reads, MySQL Cluster 7.4 is 50% faster than last release
Figure 2 shows that for mixed traffic, MySQL Cluster 7.4 is 50% faster than last release

Just make sure that you&amp;#8217;re using the multi-threaded data node (ndbmtd rather than ndbd) and have configured how many threads it should use.
The tests were performed using a 48 core/96 thread machine (also demonstrating how well MySQL Cluster can now scale up with large numbers of cores).
So, the sysbench benchmark results show how much faster MySQL Cluster 7.4 is compared to the previous 2 releases (a nice apples-to-apples test but on a very small cluster without any scale-out) but we now have some even more interesting benchmark results showing just how well MySQL Cluster can scale out to deliver incredible results&amp;#8230;
200 Million NoSQL NoSQL QPS!
Figure 4: 200 Million NoSQL Queries Per Second
Using the flexAsynch benchmark tool and a pool of 32 data nodes (with each data node running on a dedicated 56 thread Intel E5-2697 v3 (Haswell) machine) the cluster was able to process 200,000,000 reads per second. These results are shown in Figure 4, where the number of data nodes are scaled on the x-axis &amp;#8211; as you&amp;#8217;ll observe, performance scales virtually linearly with the number of data nodes. Note that MySQL Cluster supports up to 48 data nodes and so there&amp;#8217;s still plenty of scope to scale out even further.
2.5 Million SQL Operation per Second
Figure 5: 2.5 Million SQL operations per second
Using a smaller cluster (just 16 data nodes running on the same 56 thread machines), the DBT2 Benchmark has been used to assess how well SQL performance scales as more data nodes are added. SQL performance was measured at 2,500,000 SQL read opeations per minute. This equates to around 5 Million Transactions Per Minutes or 2.2 Million NewOnly TPM.
As can be in Figure 5 the scaling of SQL reads is again almost linear.
Active-Active (Multi-Master) Replication
MySQL Cluster allows bi-directional replication between two (or more) clusters. Replication within each cluster is synchronous but between clusters it is asynchronous which means the following scenario is possible:

Conflict with asynchronous replication


Site A
Replication
Site B


x == 10

x == 10


x = 11

x = 20



&amp;#8211; x=11 &amp;#8211;&amp;gt;
x == 11


x==20
&amp;lt;&amp;#8211; x=20 &amp;#8211;




&amp;nbsp;
In this example a value (column for a row in a table) is set to 11 on site A and the change is queued for replication to site B. In the mean time, an application sets the value to 20 on site B and that change is queued for replication to site A. Once both sites have received and applied the replicated change from the other cluster site A contains the value 20 while site B contains 11 &amp;#8211; in other words the databases are now inconsistent.
How MySQL Cluster implements eventual consistency
There are two phases to establishing consistency between both clusters after an inconsistency has been introduced:

Detect that a conflict has happened
Resolve the inconsistency

The following animation illustrates how MySQL Cluster 7.2 detects that an inconsistency has been introduced by the asynchronous, active-active replication:
Figure 6: Detecting conflicts
While we typically consider the 2 clusters in an active-active replication configuration to be peers, in this case we designate one to be the primary and the other the secondary. Reads and writes can still be sent to either cluster but it is the responsibility of the primary to identify that a conflict has arisen and then remove the inconsistency.
A logical clock is used to identify (in relative terms) when a change is made on the primary &amp;#8211; for those who know something of the MySQL Cluster internals, we use the index of the Global Checkpoint that the update is contained in. For all tables that have this feature turned on, an extra, hidden column is automatically added on the primary &amp;#8211; this represents the value of the logical clock when the change was made.
Once the change has been applied on the primary, there is a &amp;#8220;window of conflict&amp;#8221; for the effected row(s) during which if a different change is made to the same row(s) on the secondary then there will be an inconsistency. Once the slave on the secondary has applied the change from the primary, it will send a replication event back to the slave on the primary, containing the primary&amp;#8217;s clock value associated with the changes that have just been applied on the secondary. (Remember that the clock is actually the Global Checkpoint Index and so this feature is sometimes referred to as Reflected GCI). Once the slave on the primary has received this event, it knows that all changes tagged with a clock value no later than the reflected GCI are now safe &amp;#8211; the window of conflict has closed.
If an application modifies this same row on the secondary before the replication event from the primary was applied then it will send an associated replication event to the slave on the primary before it reflects the new GCI. The slave on the primary will process this replication event and compare the clock value recorded with the effected rows with the latest reflected GCI; as the clock value for the conflicting row is higher the primary recognises that a conflict has occured and will launch the algorithm to resolve the inconsistency.
Figure 7: Options for MySQL Cluster replication conflict detection/resolution
&amp;nbsp;
After a conflict has been detected, you have the option of having the database simply report the conflict to the application or have it roll back just the conflicting row or the entire transaction and all subsequent transactions that were dependent on it.
So &amp;#8211; what&amp;#8217;s new in 7.4?

Selective tracking of transaction reads for improved transaction dependency tracking
Improved NDB Exceptions table format : Non-PK columns, operation type, transaction id, before and after values
Improved manageability: Online conflict role change, improved visibility into system state, operations, current and historic consistency

Configuring Active-Active Replication with Transactional Conflict Resolution
Figure 8: MySQL Replication Configuration
This section assumes that replication has already been set up between two clusters as shown in Figure 8. For more details on how to set up that configuration then refer to the blog: Enhanced conflict resolution with MySQL Cluster active-active replication.
To keep things simple, just two hosts are used; 192.168.56.101 contains all nodes for the primary cluster and 192.168.56.102 will contain all nodes for the secondary. A single MySQL Server in each cluster acts as both the master and the slave.
The first step is to identify the tables that need conflict detection enabling. Each of those tables then has to have an entry in the mysql.ndb_replication table where they&amp;#8217;re tagged as using the new NDB$EPOCH2_TRANS() function &amp;#8211; you could also choose to use NDB$EPOCH(), in which case only the changes to conflicting rows will be rolled-back rather than the full transactions. A few things to note:

This must be done before creating the application tables themselves
Should only be done on the primary
By default the table doesn&amp;#8217;t exist and so the very first step is to create it

PRIMARY&amp;gt; CREATE TABLE mysql.ndb_replication (
-&amp;gt;     db VARBINARY(63),
-&amp;gt;     table_name VARBINARY(63),
-&amp;gt;     server_id INT UNSIGNED,
-&amp;gt;     binlog_type INT UNSIGNED,
-&amp;gt;     conflict_fn VARBINARY(128),
-&amp;gt;     PRIMARY KEY USING HASH (db, table_name, server_id)
-&amp;gt; )   ENGINE=NDB
-&amp;gt; PARTITION BY KEY(db,table_name);

PRIMARY&amp;gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple1', 0, 0,  'NDB$EPOCH2_TRANS()');
PRIMARY&amp;gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple2', 0, 0,'NDB$EPOCH2_TRANS()');
PRIMARY&amp;gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple3', 0, 0,'NDB$EPOCH2_TRANS()');

SECONDARY&amp;gt; SELECT * FROM mysql.ndb_replication;
+-----------+------------+-----------+-------------+--------------------+
| db        | table_name | server_id | binlog_type | conflict_fn        |
+-----------+------------+-----------+-------------+--------------------+
| clusterdb | simple2    |         0 |           0 | NDB$EPOCH2_TRANS() |
| clusterdb | simple1    |         0 |           0 | NDB$EPOCH2_TRANS() |
| clusterdb | simple3    |         0 |           0 | NDB$EPOCH2_TRANS() |
+-----------+------------+-----------+-------------+--------------------+

Refer to the MySQL Cluster documentation for details on the contents of the mysql.ndb_replication table.
The next step is to define which MySQL Cluster instance is to act as the PRIMARY and which as the SECONDARY (note that the slave SQL thread must be halted to make this change):
PRIMARY&amp;gt; STOP SLAVE SQL_THREAD;
SECONDARY&amp;gt; STOP SLAVE SQL_THREAD;
PRIMARY&amp;gt; SET GLOBAL ndb_slave_conflict_role='PRIMARY';
SECONDARY&amp;gt; SET GLOBAL ndb_slave_conflict_role='SECONDARY';
SECONDARY&amp;gt; START SLAVE SQL_THREAD;
PRIMARY&amp;gt; START SLAVE SQL_THREAD;

For each of these tables you should also create an exceptions table which will record any conflicts that have resulted in changes being rolled back; the format of these tables is rigidly defined and so take care to copy the types exactly; again this only needs doing on the primary:
PRIMARY&amp;gt; CREATE DATABASE clusterdb;USE clusterdb;

PRIMARY&amp;gt; CREATE TABLE simple1$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;

PRIMARY&amp;gt; CREATE TABLE simple2$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;

PRIMARY&amp;gt; CREATE TABLE simple3$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;

Refer to the MySQL Cluster documentation for details on the format of the exception tables.
Finally, the application tables themselves can be created (this only needs doing on the primary as they&amp;#8217;ll be replicated to the secondary):
PRIMARY&amp;gt; CREATE TABLE simple1 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;
PRIMARY&amp;gt; CREATE TABLE simple2 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;
PRIMARY&amp;gt; CREATE TABLE simple3 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;

Everything is now set up and the new configuration can be tested to ensure that conflicts are detected and the correct updates are rolled back.
Testing Active-Active Replication with Transactional Conflict Resolution
The first step is to add some data to our new tables (note that at this point replication is running and so they only need to be created on the primary) and confirm that everything is replicated to the secondary:
PRIMARY&amp;gt; INSERT INTO simple1 VALUES (1,10);
PRIMARY&amp;gt; INSERT INTO simple2 VALUES (1,10);
PRIMARY&amp;gt; INSERT INTO simple3 VALUES (1,10);

SECONDARY&amp;gt; SELECT * FROM simple1;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

SECONDARY&amp;gt; SELECT * FROM simple2;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

SECONDARY&amp;gt; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

It is important that the NDB$EPOCH2_TRANS() function rolls back any transactions on the secondary that involve a conflict (as well as subsequent, dependent transactions that modify the same rows); to intentionally trigger this, the simplest approach is to stop the slave IO thread on the secondary Cluster in order to increase the size of the window of conflict (which is otherwise very short). Once the slave IO thread has been stopped a change is made to table simple1 on the primary and then the secondary makes a (conflicting) change to the same row as well as making a change to table simple2 in the same transaction. A second transaction on the primary will change a row in simple3 &amp;#8211; as it doesn&amp;#8217;t touch any rows that have been involved in a conflict then that change should stand.
SECONDARY&amp;gt; STOP SLAVE IO_THREAD;

PRIMARY&amp;gt; UPDATE simple1 SET value=13 WHERE id=1;

SECONDARY&amp;gt; BEGIN; # conflicting transaction
SECONDARY&amp;gt; UPDATE simple1 SET value=20 WHERE id=1;
SECONDARY&amp;gt; UPDATE simple2 SET value=20 WHERE id=1;
SECONDARY&amp;gt; COMMIT;
SECONDARY&amp;gt; UPDATE simple3 SET value=20 WHERE id=1; # non conflicting

SECONDARY&amp;gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

If you now check the exception tables then you can see that the primary has received the changes from the secondary and because the first transaction updated the same row in simple1 during its window of conflict it has recorded that the change needs to be rolled back &amp;#8211; this will happen as soon as the replication thread is restarted on the secondary:
PRIMARY&amp;gt; SELECT * FROM simple1$EX\G
*************************** 1. row ***************************
NDB$server_id: 9
NDB$master_server_id: 20
NDB$master_epoch: 31344671326216
NDB$count: 3
NDB$OP_TYPE: UPDATE_ROW
NDB$CFT_CAUSE: TRANS_IN_CONFLICT
NDB$ORIG_TRANSID: 193282978304
id: 1
value$OLD: 10
value$NEW: 20
1 row in set (0.00 sec)

PRIMARY&amp;gt; SELECT * FROM simple2$EX\G
*************************** 1. row ***************************
NDB$server_id: 9
NDB$master_server_id: 20
NDB$master_epoch: 31344671326216
NDB$count: 1
NDB$OP_TYPE: UPDATE_ROW
NDB$CFT_CAUSE: TRANS_IN_CONFLICT
NDB$ORIG_TRANSID: 193282978304
id: 1
value$OLD: 10
value$NEW: 20

PRIMARY&amp;gt; SELECT * FROM simple3$EX\G
Empty set (0.01 sec)

SECONDARY&amp;gt; START SLAVE IO_THREAD;

SECONDARY&amp;gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    13 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

These are the results we expect &amp;#8211; simple1 has the value set by the primary with the subsequent change on the secondary rolled back; simple2 was not updated by the primary but the change on the secondary was rolled back as it was made in the same transaction as the conflicting update to simple1. The change on the secondary to simple3 has survived as it was made outside of any conflicting transaction and the change was not dependent on any conflicting changes. Finally just confirm that the data is identical on the primary:
PRIMARY&amp;gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    13 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

Statistics are provided on the primary that record that 1 conflict has been detected, effecting 1 transaction and that it resulted in 2 row changes being rolled back:
PRIMARY&amp;gt; SHOW STATUS LIKE 'ndb_conflict%';
+------------------------------------------+----------------+
| Variable_name                            | Value          |
+------------------------------------------+----------------+
| Ndb_conflict_fn_max                      | 0              |
| Ndb_conflict_fn_old                      | 0              |
| Ndb_conflict_fn_max_del_win              | 0              |
| Ndb_conflict_fn_epoch                    | 0              |
| Ndb_conflict_fn_epoch_trans              | 0              |
| Ndb_conflict_fn_epoch2                   | 0              |
| Ndb_conflict_fn_epoch2_trans             | 1              |
| Ndb_conflict_trans_row_conflict_count    | 1              |
| Ndb_conflict_trans_row_reject_count      | 2              |
| Ndb_conflict_trans_reject_count          | 1              |
| Ndb_conflict_trans_detect_iter_count     | 1              |
| Ndb_conflict_trans_conflict_commit_count | 1              |
| Ndb_conflict_epoch_delete_delete_count   | 0              |
| Ndb_conflict_reflected_op_prepare_count  | 0              |
| Ndb_conflict_reflected_op_discard_count  | 0              |
| Ndb_conflict_refresh_op_count            | 0              |
| Ndb_conflict_last_conflict_epoch         | 37391985278995 |
| Ndb_conflict_last_stable_epoch           | 37417755082760 |
+------------------------------------------+----------------+

SECONDARY&amp;gt; SHOW STATUS LIKE 'ndb_conflict%';
+------------------------------------------+----------------+
| Variable_name                            | Value          |
+------------------------------------------+----------------+
| Ndb_conflict_fn_max                      | 0              |
| Ndb_conflict_fn_old                      | 0              |
| Ndb_conflict_fn_max_del_win              | 0              |
| Ndb_conflict_fn_epoch                    | 0              |
| Ndb_conflict_fn_epoch_trans              | 0              |
| Ndb_conflict_fn_epoch2                   | 0              |
| Ndb_conflict_fn_epoch2_trans             | 0              |
| Ndb_conflict_trans_row_conflict_count    | 0              |
| Ndb_conflict_trans_row_reject_count      | 0              |
| Ndb_conflict_trans_reject_count          | 0              |
| Ndb_conflict_trans_detect_iter_count     | 0              |
| Ndb_conflict_trans_conflict_commit_count | 0              |
| Ndb_conflict_epoch_delete_delete_count   | 0              |
| Ndb_conflict_reflected_op_prepare_count  | 1              |
| Ndb_conflict_reflected_op_discard_count  | 1              |
| Ndb_conflict_refresh_op_count            | 2              |
| Ndb_conflict_last_conflict_epoch         | 32280974196749 |
| Ndb_conflict_last_stable_epoch           | 18897856102416 |
+------------------------------------------+----------------+

These status variables are described in the MySQL Cluster documentation.
Faster Restarts
You can restart MySQL Cluster processes (nodes) without losing database service (for example if adding extra memory to a server) and so on the face of it, the speed of the restarts isn&amp;#8217;t that important. Having said that, while the node is restarting you&amp;#8217;ve lost some of your high-availability which for super-critical applications can make you nervous. Additionally, faster restarts mean that you can complete maintenance activities faster &amp;#8211; for example, a software upgrade requires a rolling restart of all of the nodes &amp;#8211; if you have 48 data nodes then you want each of the data nodes to restart as quickly as possible.
MySQL 7.4 includes a number of optimisations to the restart code and so if you&amp;#8217;re already using MySQL Cluster, it might be interesting to see how much faster it gets for your application.
Enhanced Reporting
MySQL Cluster presents a lot of monitoring information through the ndbinfo database and in 7.4 we&amp;#8217;ve added some extra information on how memory is used for individual tables and how operations are distributed.
Extra Memory Reporting
MySQL Cluster allocates all of the required memory when a data node starts and so any information on memory usage from the operating system is of limited use and provides no clues as to how memory is used with the data nodes &amp;#8211; for example, which tables are using the most memory. Also, as this is a distributed database, it is helpful to understand whether a particular table is using a similar amount of memory in each data node (if not then it could be that a better partitioning/sharding key could be used). Finally, when rows are deleted from a table, the memory for those rows would typically remain allocated against that table and so it is helpful to understand how many of these ‘empty slots’ are available for use by new rows in that table. MySQL Cluster 7.4 introduces a new table – ndbinfo.memory_per_fragment &amp;#8211; that provides that information.
For example; to see how much memory is being used by each data node for a particular table&amp;#8230;
mysql&amp;gt; CREATE DATABASE clusterdb;USE clusterdb;
mysql&amp;gt; CREATE TABLE simples (id INT NOT NULL AUTO_INCREMENT PRIMARY KEY) ENGINE=NDB;
mysql&amp;gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1280 |         40 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1280 |         40 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4256 |        133 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4256 |        133 |
+------+------+-------------+------------+------------+

When you delete rows from a MySQL Cluster table, the memory is not actually freed up and so if you check the existing memoryusage table you won&amp;#8217;t see a change. This memory will be reused when you add new rows to that same table. In MySQL Cluster 7.4, it&amp;#8217;s possible to see how much memory is in that state for a table&amp;#8230;
mysql&amp;gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1280 |         40 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1280 |         40 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4256 |        133 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4256 |        133 |
+------+------+-------------+------------+------------+
mysql&amp;gt; DELETE FROM clusterdb.simples LIMIT 1;
mysql&amp;gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1312 |         41 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1312 |         41 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4288 |        134 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4288 |        134 |
+------+------+-------------+------------+------------+

As a final example, we can check whether a table is being evenly sharded accross the data nodes (in this case a realy bad sharding key was chosen)&amp;#8230;
mysql&amp;gt; CREATE TABLE simples (id INT NOT NULL AUTO_INCREMENT, \
        species VARCHAR(20) DEFAULT &quot;Human&quot;, 
        PRIMARY KEY(id, species)) engine=ndb PARTITION BY KEY(species);

// Add some data

mysql&amp;gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |           0 |          0 |          0 |
|    1 |    2 |      196608 |      11732 |        419 |
|    2 |    0 |           0 |          0 |          0 |
|    2 |    2 |      196608 |      11732 |        419 |
|    3 |    1 |           0 |          0 |          0 |
|    3 |    3 |           0 |          0 |          0 |
|    4 |    1 |           0 |          0 |          0 |
|    4 |    3 |           0 |          0 |          0 |
+------+------+-------------+------------+------------+

Extra Operations Reporting
To ensure that resources are being used effectively, it is very helpful to understand the how each table is being access (how frequently and for what types of operations). To support this, the ndbinfo. operations_per_fragment table is provided. For example, the data in this table would let you identify that a large number of full table scans are performed on a particular table.
It is also important to identify if there are any hotspots where a disproportionate share of the queries for a table are hitting a particular fragment/data node. Again, ndbinfo. operations_per_fragment provides this information.
As an example of how to use some of the data from this table, a simple table is created and populated and then ndbinfo.operations_per_fragment is used to monitor how many Primary Key reads and table scans are performed[1]:
mysql&amp;gt; CREATE TABLE simples (id INT AUTO_INCREMENT PRIMARY KEY, time TIMESTAMP) ENGINE=NDB;

mysql&amp;gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     1 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     1 |
+-----------------------+-----------+-------+-------+

mysql&amp;gt; INSERT INTO simples VALUES ();  # Repeated several times
mysql&amp;gt; SELECT * FROM simples;
+----+---------------------+
| id | time                |
+----+---------------------+
|  7 | 2015-01-22 15:12:42 |
…
|  8 | 2015-01-22 15:12:58 |
+----+---------------------+
23 rows in set (0.00 sec)

mysql&amp;gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     2 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     2 |
+-----------------------+-----------+-------+-------+

mysql&amp;gt; SELECT * FROM simples WHERE id=11;
+----+---------------------+
| id | time                |
+----+---------------------+
| 11 | 2015-01-22 15:12:59 |
+----+---------------------+

mysql&amp;gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     2 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     1 |     2 |
+-----------------------+-----------+-------+-------+

Note that there are two rows listed for each data node but only one row for each has non-zero values; this is because each data node holds the primary fragment for one of the partitions and the secondary fragment for the other – all operations are performed only on the active fragments. This is made clearer if the fragment number is included in the query:
mysql&amp;gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', fragment_num AS 'Fragment', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+----------+-------+-------+
| Table                 | Data Node | Fragment | Reads | Scans |
+-----------------------+-----------+----------+-------+-------+
| clusterdb/def/simples |         3 |        0 |     0 |     2 |
| clusterdb/def/simples |         3 |        1 |     0 |     0 |
| clusterdb/def/simples |         4 |        0 |     0 |     0 |
| clusterdb/def/simples |         4 |        1 |     1 |     2 |
+-----------------------+-----------+----------+-------+-------+

Conclusion
We&amp;#8217;re really excited about the GA for this new MySQL Cluster release; if you get chance to try it out then please let us know how you get on &amp;#8211; either through a comment on this blog, a MySQL bug report or a post to the MySQL Cluster Forum.</description>
    <content:encoded><![CDATA[<p><a href="http://www.clusterdb.com/wp-content/uploads/2015/02/MySQL-Cluster-7.4-GA-banner.jpg"><img class="aligncenter size-full wp-image-4021" src="http://www.clusterdb.com/wp-content/uploads/2015/02/MySQL-Cluster-7.4-GA-banner.jpg" alt="MySQL Cluster 7.4 GA banner" width="600" /></a></p>
<p>The<b> General Availability of MySQL Cluster 7.4</b> has just been <a href="http://www.oracle.com/us/corporate/press/2431648" title="Oracle announces General Availability of MySQL Cluster 7.4 with 200 Million QPS" target="_blank">announced by Oracle</a>.</p>
<p>The MySQL team at Oracle are excited to announce the General Availability of MySQL Cluster 7.4, ready for production workloads.</p>
<p>MySQL Cluster 7.4.4 can be <a title="Download MySQL Cluster 7.4.4 - the GA release" href="http://dev.mysql.com/downloads/cluster/" target="_blank">downloaded from mysql.com</a> and the <a title="Changes in MySQL Cluster NDB 7.4.4 (5.6.22-ndb-7.4.4)" href="http://dev.mysql.com/doc/relnotes/mysql-cluster/7.4/en/mysql-cluster-news-5-6-22-ndb-7-4-4.html" target="_blank">release notes viewed in the MySQL docs</a>.</p>
<p>Figure 1 provides a summary of the enhancements delivered in this release:</p>
<ul>
<li>Performance
<ul>
<li>200M NoSQL Reads/Sec</li>
<li>2.5M SQL Ops/Sec</li>
<li>50% Faster Reads</li>
<li>40% Faster Mixed</li>
</ul>
</li>
<li>Active-Active
<ul>
<li>Active-Active Geographic Redundancy</li>
<li>Conflict Detection/Resolution</li>
</ul>
</li>
<li>Management
<ul>
<li>5X Faster Maintenance Ops</li>
<li>Detailed Reporting</li>
</ul>
</li>
</ul>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2015/02/MySQL-Cluster-7.4-summary.png"><img class="wp-image-4007 size-large" src="http://www.clusterdb.com/wp-content/uploads/2015/02/MySQL-Cluster-7.4-summary-1024x395.png" alt="MySQL Cluster 7.4 content summary" width="480" height="185" /></a><p>Figure 1: MySQL Cluster 7.4 content summary</p></div>
<p>The sections that follow delve into some more details.</p>
<h3>Performance</h3>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.35.23.png"><img class="wp-image-3952 size-medium" src="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.35.23-300x197.png" alt="MySQL CLuster 7.4.1 Read-Write Performance" width="300" height="197" /></a><p>Figure 2: Read/Writes 40% Faster than MySQL Cluster 7.3</p></div>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.35.15.png"><img class="wp-image-3951 size-medium" src="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.35.15-300x199.png" alt="MySQL CLuster 7.4 Read Performance" width="300" height="199" /></a><p>Figure 3: Reads 50% Faster than MySQL Cluster 7.3</p></div>
<h4>50% Faster Reads than MySQL Cluster 7.3</h4>
<p>Being a scaled-out, in-memory, real-time database, MySQL Cluster performance has always been great but we continue to work on making it faster each release. In particular, we want to keep pace with the trend of having more and more cores rather than faster ones. 7.4 continues along the path of better exploiting multiple cores &#8211; as can be seen from these benchmark results:</p>
<ul>
<li>Figure 3 shows that for reads, MySQL Cluster 7.4 is 50% faster than last release</li>
<li>Figure 2 shows that for mixed traffic, MySQL Cluster 7.4 is 50% faster than last release</li>
</ul>
<p>Just make sure that you&#8217;re using the multi-threaded data node (ndbmtd rather than ndbd) and have <a title="MySQL Cluster - configuring multi-threaded datanodes" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-programs-ndbmtd.html" target="_blank">configured how many threads it should use</a>.</p>
<p>The tests were performed using a 48 core/96 thread machine (also demonstrating how well MySQL Cluster can now scale up with large numbers of cores).</p>
<p>So, the sysbench benchmark results show how much faster MySQL Cluster 7.4 is compared to the previous 2 releases (a nice apples-to-apples test but on a very small cluster without any scale-out) but we now have some even more interesting <a title="MySQL Cluster Benchmark Results" href="http://www.mysql.com/why-mysql/benchmarks/mysql-cluster/" target="_blank">benchmark results</a> showing just how well MySQL Cluster can scale out to deliver incredible results&#8230;</p>
<h4>200 Million NoSQL NoSQL QPS!</h4>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2015/02/200-Million-NoSQL-QPS.png"><img class="size-medium wp-image-4015" src="http://www.clusterdb.com/wp-content/uploads/2015/02/200-Million-NoSQL-QPS-300x179.png" alt="200 Million NoSQL Queries Per Second with MySQL Cluster 7.4" width="300" height="179" /></a><p>Figure 4: 200 Million NoSQL Queries Per Second</p></div>
<p>Using the <a title="Download MySQL Bechmark tools" href="http://dev.mysql.com/downloads/benchmarks.html%20" target="_blank">flexAsynch benchmark tool</a> and a pool of 32 data nodes (with each data node running on a dedicated 56 thread Intel E5-2697 v3 (Haswell) machine) the cluster was able to process 200,000,000 reads per second. These results are shown in Figure 4, where the number of data nodes are scaled on the x-axis &#8211; as you&#8217;ll observe, performance scales virtually linearly with the number of data nodes. Note that MySQL Cluster supports up to 48 data nodes and so there&#8217;s still plenty of scope to scale out even further.</p>
<h4>2.5 Million SQL Operation per Second</h4>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2015/02/2.5-Million-SQL-operations-per-second.png"><img class="size-medium wp-image-4017" src="http://www.clusterdb.com/wp-content/uploads/2015/02/2.5-Million-SQL-operations-per-second-300x175.png" alt="2.5 Million SQL operations per second with MySQL Cluster" width="300" height="175" /></a><p>Figure 5: 2.5 Million SQL operations per second</p></div>
<p>Using a smaller cluster (just 16 data nodes running on the same 56 thread machines), the DBT2 Benchmark has been used to assess how well SQL performance scales as more data nodes are added. SQL performance was measured at 2,500,000 SQL read opeations per minute. This equates to around 5 Million Transactions Per Minutes or 2.2 Million NewOnly TPM.</p>
<p>As can be in Figure 5 the scaling of SQL reads is again almost linear.</p>
<h3>Active-Active (Multi-Master) Replication</h3>
<p>MySQL Cluster allows bi-directional replication between two (or more) clusters. Replication within each cluster is synchronous but between clusters it is asynchronous which means the following scenario is possible:</p>
<table border="1" summary="Active-Active asynchronous replication can lead to inconsistencies between databases" width="260" align="center">
<caption align="center">Conflict with asynchronous replication</caption>
<tbody>
<tr>
<th scope="col" align="center" width="80">Site A</th>
<th scope="col" align="center" width="100">Replication</th>
<th scope="col" align="center" width="80">Site B</th>
</tr>
<tr>
<td align="center">x == 10</td>
<td></td>
<td align="center">x == 10</td>
</tr>
<tr>
<td align="center">x = 11</td>
<td></td>
<td align="center">x = 20</td>
</tr>
<tr>
<td></td>
<td align="center">&#8211; x=11 &#8211;&gt;</td>
<td align="center">x == 11</td>
</tr>
<tr>
<td align="center">x==20</td>
<td align="center">&lt;&#8211; x=20 &#8211;</td>
<td></td>
</tr>
</tbody>
</table>
<p>&nbsp;</p>
<p>In this example a value (column for a row in a table) is set to 11 on site A and the change is queued for replication to site B. In the mean time, an application sets the value to 20 on site B and that change is queued for replication to site A. Once both sites have received and applied the replicated change from the other cluster site A contains the value 20 while site B contains 11 &#8211; in other words the databases are now inconsistent.</p>
<h4>How MySQL Cluster implements eventual consistency</h4>
<p>There are two phases to establishing consistency between both clusters after an inconsistency has been introduced:</p>
<ol>
<li>Detect that a conflict has happened</li>
<li>Resolve the inconsistency</li>
</ol>
<p>The following animation illustrates how MySQL Cluster 7.2 detects that an inconsistency has been introduced by the asynchronous, active-active replication:</p>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2011/11/ReflectingGCI1.gif"><img class="size-full wp-image-2149" title="ReflectingGCI" src="http://www.clusterdb.com/wp-content/uploads/2011/11/ReflectingGCI1.gif" alt="" width="600" height="463" /></a><p>Figure 6: Detecting conflicts</p></div>
<p>While we typically consider the 2 clusters in an active-active replication configuration to be peers, in this case we designate one to be the primary and the other the secondary. Reads and writes can still be sent to either cluster but it is the responsibility of the primary to identify that a conflict has arisen and then remove the inconsistency.</p>
<p>A logical clock is used to identify (in relative terms) when a change is made on the primary &#8211; for those who know something of the MySQL Cluster internals, we use the index of the Global Checkpoint that the update is contained in. For all tables that have this feature turned on, an extra, hidden column is automatically added on the primary &#8211; this represents the value of the logical clock when the change was made.</p>
<p>Once the change has been applied on the primary, there is a &#8220;window of conflict&#8221; for the effected row(s) during which if a different change is made to the same row(s) on the secondary then there will be an inconsistency. Once the slave on the secondary has applied the change from the primary, it will send a replication event back to the slave on the primary, containing the primary&#8217;s clock value associated with the changes that have just been applied on the secondary. (Remember that the clock is actually the Global Checkpoint Index and so this feature is sometimes referred to as Reflected GCI). Once the slave on the primary has received this event, it knows that all changes tagged with a clock value no later than the reflected GCI are now safe &#8211; the window of conflict has closed.</p>
<p>If an application modifies this same row on the secondary before the replication event from the primary was applied then it will send an associated replication event to the slave on the primary <strong>before</strong> it reflects the new GCI. The slave on the primary will process this replication event and compare the clock value recorded with the effected rows with the latest reflected GCI; as the clock value for the conflicting row is higher the primary recognises that a conflict has occured and will launch the algorithm to resolve the inconsistency.</p>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.34.15.png"><img class="wp-image-3950 size-large" src="http://www.clusterdb.com/wp-content/uploads/2014/09/Screen-Shot-2014-09-25-at-13.34.15-1024x372.png" alt="Options for MySQL Cluster replication conflict detection/resolution" width="480" height="174" /></a><p>Figure 7: Options for MySQL Cluster replication conflict detection/resolution</p></div>
<p>&nbsp;</p>
<p>After a conflict has been detected, you have the option of having the database simply report the conflict to the application or have it roll back just the conflicting row or the entire transaction and all subsequent transactions that were dependent on it.</p>
<p>So &#8211; what&#8217;s new in 7.4?</p>
<ul>
<li>Selective tracking of transaction reads for improved transaction dependency tracking</li>
<li>Improved NDB Exceptions table format : Non-PK columns, operation type, transaction id, before and after values</li>
<li>Improved manageability: Online conflict role change, improved visibility into system state, operations, current and historic consistency</li>
</ul>
<h4>Configuring Active-Active Replication with Transactional Conflict Resolution</h4>
<div><a href="http://www.clusterdb.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-11-at-17.36.17.png"><img class="size-medium wp-image-4026" src="http://www.clusterdb.com/wp-content/uploads/2015/02/Screen-Shot-2015-02-11-at-17.36.17-300x181.png" alt="MySQL Replication Configuration" width="300" height="181" /></a><p>Figure 8: MySQL Replication Configuration</p></div>
<p>This section assumes that replication has already been set up between two clusters as shown in Figure 8. For more details on how to set up that configuration then refer to the blog: <a title="Enhanced conflict resolution with MySQL Cluster active-active replication" href="http://www.clusterdb.com/mysql-cluster/enhanced-conflict-resolution-with-mysql-cluster-active-active-replication/%20" target="_blank">Enhanced conflict resolution with MySQL Cluster active-active replication</a>.</p>
<p>To keep things simple, just two hosts are used; 192.168.56.101 contains all nodes for the primary cluster and 192.168.56.102 will contain all nodes for the secondary. A single MySQL Server in each cluster acts as both the master and the slave.</p>
<p>The first step is to identify the tables that need conflict detection enabling. Each of those tables then has to have an entry in the mysql.ndb_replication table where they&#8217;re tagged as using the new NDB$EPOCH2_TRANS() function &#8211; you could also choose to use NDB$EPOCH(), in which case only the changes to conflicting rows will be rolled-back rather than the full transactions. A few things to note:</p>
<ul>
<li>This must be done before creating the application tables themselves</li>
<li>Should only be done on the primary</li>
<li>By default the table doesn&#8217;t exist and so the very first step is to create it</li>
</ul>
<pre>PRIMARY&gt; CREATE TABLE mysql.ndb_replication (
-&gt;     db VARBINARY(63),
-&gt;     table_name VARBINARY(63),
-&gt;     server_id INT UNSIGNED,
-&gt;     binlog_type INT UNSIGNED,
-&gt;     conflict_fn VARBINARY(128),
-&gt;     PRIMARY KEY USING HASH (db, table_name, server_id)
-&gt; )   ENGINE=NDB
-&gt; PARTITION BY KEY(db,table_name);

PRIMARY&gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple1', 0, 0,  'NDB$EPOCH2_TRANS()');
PRIMARY&gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple2', 0, 0,'NDB$EPOCH2_TRANS()');
PRIMARY&gt; INSERT INTO mysql.ndb_replication VALUES ('clusterdb', 'simple3', 0, 0,'NDB$EPOCH2_TRANS()');

SECONDARY&gt; SELECT * FROM mysql.ndb_replication;
+-----------+------------+-----------+-------------+--------------------+
| db        | table_name | server_id | binlog_type | conflict_fn        |
+-----------+------------+-----------+-------------+--------------------+
| clusterdb | simple2    |         0 |           0 | NDB$EPOCH2_TRANS() |
| clusterdb | simple1    |         0 |           0 | NDB$EPOCH2_TRANS() |
| clusterdb | simple3    |         0 |           0 | NDB$EPOCH2_TRANS() |
+-----------+------------+-----------+-------------+--------------------+
</pre>
<p>Refer to the <a title="Documentation for mysql.ndb_replication table" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-replication-conflict-resolution.html#mysql-cluster-ndb-replication-table" target="_blank">MySQL Cluster documentation</a> for details on the contents of the mysql.ndb_replication table.</p>
<p>The next step is to define which MySQL Cluster instance is to act as the PRIMARY and which as the SECONDARY (note that the slave SQL thread must be halted to make this change):</p>
<pre>PRIMARY&gt; STOP SLAVE SQL_THREAD;
SECONDARY&gt; STOP SLAVE SQL_THREAD;
PRIMARY&gt; SET GLOBAL ndb_slave_conflict_role='PRIMARY';
SECONDARY&gt; SET GLOBAL ndb_slave_conflict_role='SECONDARY';
SECONDARY&gt; START SLAVE SQL_THREAD;
PRIMARY&gt; START SLAVE SQL_THREAD;
</pre>
<p>For each of these tables you should also create an exceptions table which will record any conflicts that have resulted in changes being rolled back; the format of these tables is rigidly defined and so take care to copy the types exactly; again this only needs doing on the primary:</p>
<pre>PRIMARY&gt; CREATE DATABASE clusterdb;USE clusterdb;

PRIMARY&gt; CREATE TABLE simple1$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;

PRIMARY&gt; CREATE TABLE simple2$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;

PRIMARY&gt; CREATE TABLE simple3$EX (
NDB$server_id INT UNSIGNED,
NDB$master_server_id INT UNSIGNED,
NDB$master_epoch BIGINT UNSIGNED,
NDB$count INT UNSIGNED,
NDB$OP_TYPE ENUM('WRITE_ROW','UPDATE_ROW', 'DELETE_ROW', 'REFRESH_ROW', 'READ_ROW') NOT NULL,
NDB$CFT_CAUSE ENUM('ROW_DOES_NOT_EXIST', 'ROW_ALREADY_EXISTS', 'DATA_IN_CONFLICT', 'TRANS_IN_CONFLICT') NOT NULL,
NDB$ORIG_TRANSID BIGINT UNSIGNED NOT NULL,
id INT NOT NULL,
value$OLD INT,
value$NEW INT,
PRIMARY KEY(NDB$server_id, NDB$master_server_id, NDB$master_epoch, NDB$count)) ENGINE=NDB;
</pre>
<p>Refer to the <a title="Documentation for MySQL Cluster exception tables" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-replication-conflict-resolution.html#mysql-cluster-replication-exceptions-table" target="_blank">MySQL Cluster documentation</a> for details on the format of the exception tables.</p>
<p>Finally, the application tables themselves can be created (this only needs doing on the primary as they&#8217;ll be replicated to the secondary):</p>
<pre>PRIMARY&gt; CREATE TABLE simple1 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;
PRIMARY&gt; CREATE TABLE simple2 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;
PRIMARY&gt; CREATE TABLE simple3 (id INT NOT NULL PRIMARY KEY, value INT) ENGINE=ndb;
</pre>
<p>Everything is now set up and the new configuration can be tested to ensure that conflicts are detected and the correct updates are rolled back.</p>
<h4>Testing Active-Active Replication with Transactional Conflict Resolution</h4>
<p>The first step is to add some data to our new tables (note that at this point replication is running and so they only need to be created on the primary) and confirm that everything is replicated to the secondary:</p>
<pre>PRIMARY&gt; INSERT INTO simple1 VALUES (1,10);
PRIMARY&gt; INSERT INTO simple2 VALUES (1,10);
PRIMARY&gt; INSERT INTO simple3 VALUES (1,10);

SECONDARY&gt; SELECT * FROM simple1;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

SECONDARY&gt; SELECT * FROM simple2;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

SECONDARY&gt; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+
</pre>
<p>It is important that the NDB$EPOCH2_TRANS() function rolls back any transactions on the secondary that involve a conflict (as well as subsequent, dependent transactions that modify the same rows); to intentionally trigger this, the simplest approach is to stop the slave IO thread on the secondary Cluster in order to increase the size of the window of conflict (which is otherwise very short). Once the slave IO thread has been stopped a change is made to table simple1 on the primary and then the secondary makes a (conflicting) change to the same row as well as making a change to table simple2 in the same transaction. A second transaction on the primary will change a row in simple3 &#8211; as it doesn&#8217;t touch any rows that have been involved in a conflict then that change should stand.</p>
<pre>SECONDARY&gt; STOP SLAVE IO_THREAD;

PRIMARY&gt; UPDATE simple1 SET value=13 WHERE id=1;

SECONDARY&gt; BEGIN; # conflicting transaction
SECONDARY&gt; UPDATE simple1 SET value=20 WHERE id=1;
SECONDARY&gt; UPDATE simple2 SET value=20 WHERE id=1;
SECONDARY&gt; COMMIT;
SECONDARY&gt; UPDATE simple3 SET value=20 WHERE id=1; # non conflicting

SECONDARY&gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+
</pre>
<p>If you now check the exception tables then you can see that the primary has received the changes from the secondary and because the first transaction updated the same row in simple1 during its window of conflict it has recorded that the change needs to be rolled back &#8211; this will happen as soon as the replication thread is restarted on the secondary:</p>
<pre>PRIMARY&gt; SELECT * FROM simple1$EX\G
*************************** 1. row ***************************
NDB$server_id: 9
NDB$master_server_id: 20
NDB$master_epoch: 31344671326216
NDB$count: 3
NDB$OP_TYPE: UPDATE_ROW
NDB$CFT_CAUSE: TRANS_IN_CONFLICT
NDB$ORIG_TRANSID: 193282978304
id: 1
value$OLD: 10
value$NEW: 20
1 row in set (0.00 sec)

PRIMARY&gt; SELECT * FROM simple2$EX\G
*************************** 1. row ***************************
NDB$server_id: 9
NDB$master_server_id: 20
NDB$master_epoch: 31344671326216
NDB$count: 1
NDB$OP_TYPE: UPDATE_ROW
NDB$CFT_CAUSE: TRANS_IN_CONFLICT
NDB$ORIG_TRANSID: 193282978304
id: 1
value$OLD: 10
value$NEW: 20

PRIMARY&gt; SELECT * FROM simple3$EX\G
Empty set (0.01 sec)

SECONDARY&gt; START SLAVE IO_THREAD;

SECONDARY&gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    13 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+
</pre>
<p>These are the results we expect &#8211; simple1 has the value set by the primary with the subsequent change on the secondary rolled back; simple2 was not updated by the primary but the change on the secondary was rolled back as it was made in the same transaction as the conflicting update to simple1. The change on the secondary to simple3 has survived as it was made outside of any conflicting transaction and the change was not dependent on any conflicting changes. Finally just confirm that the data is identical on the primary:</p>
<pre>PRIMARY&gt; SELECT * FROM simple1; SELECT * FROM simple2; SELECT * FROM simple3;
+----+-------+
| id | value |
+----+-------+
|  1 |    13 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    10 |
+----+-------+

+----+-------+
| id | value |
+----+-------+
|  1 |    20 |
+----+-------+
</pre>
<p>Statistics are provided on the primary that record that 1 conflict has been detected, effecting 1 transaction and that it resulted in 2 row changes being rolled back:</p>
<pre>PRIMARY&gt; SHOW STATUS LIKE 'ndb_conflict%';
+------------------------------------------+----------------+
| Variable_name                            | Value          |
+------------------------------------------+----------------+
| Ndb_conflict_fn_max                      | 0              |
| Ndb_conflict_fn_old                      | 0              |
| Ndb_conflict_fn_max_del_win              | 0              |
| Ndb_conflict_fn_epoch                    | 0              |
| Ndb_conflict_fn_epoch_trans              | 0              |
| Ndb_conflict_fn_epoch2                   | 0              |
| Ndb_conflict_fn_epoch2_trans             | 1              |
| Ndb_conflict_trans_row_conflict_count    | 1              |
| Ndb_conflict_trans_row_reject_count      | 2              |
| Ndb_conflict_trans_reject_count          | 1              |
| Ndb_conflict_trans_detect_iter_count     | 1              |
| Ndb_conflict_trans_conflict_commit_count | 1              |
| Ndb_conflict_epoch_delete_delete_count   | 0              |
| Ndb_conflict_reflected_op_prepare_count  | 0              |
| Ndb_conflict_reflected_op_discard_count  | 0              |
| Ndb_conflict_refresh_op_count            | 0              |
| Ndb_conflict_last_conflict_epoch         | 37391985278995 |
| Ndb_conflict_last_stable_epoch           | 37417755082760 |
+------------------------------------------+----------------+

SECONDARY&gt; SHOW STATUS LIKE 'ndb_conflict%';
+------------------------------------------+----------------+
| Variable_name                            | Value          |
+------------------------------------------+----------------+
| Ndb_conflict_fn_max                      | 0              |
| Ndb_conflict_fn_old                      | 0              |
| Ndb_conflict_fn_max_del_win              | 0              |
| Ndb_conflict_fn_epoch                    | 0              |
| Ndb_conflict_fn_epoch_trans              | 0              |
| Ndb_conflict_fn_epoch2                   | 0              |
| Ndb_conflict_fn_epoch2_trans             | 0              |
| Ndb_conflict_trans_row_conflict_count    | 0              |
| Ndb_conflict_trans_row_reject_count      | 0              |
| Ndb_conflict_trans_reject_count          | 0              |
| Ndb_conflict_trans_detect_iter_count     | 0              |
| Ndb_conflict_trans_conflict_commit_count | 0              |
| Ndb_conflict_epoch_delete_delete_count   | 0              |
| Ndb_conflict_reflected_op_prepare_count  | 1              |
| Ndb_conflict_reflected_op_discard_count  | 1              |
| Ndb_conflict_refresh_op_count            | 2              |
| Ndb_conflict_last_conflict_epoch         | 32280974196749 |
| Ndb_conflict_last_stable_epoch           | 18897856102416 |
+------------------------------------------+----------------+
</pre>
<p>These status variables are described in the <a title="MySQL Cluster conflict detecion/resoultion status variables" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-status-variables.html" target="_blank">MySQL Cluster documentation</a>.</p>
<h3>Faster Restarts</h3>
<p>You can restart MySQL Cluster processes (nodes) without losing database service (for example if adding extra memory to a server) and so on the face of it, the speed of the restarts isn&#8217;t that important. Having said that, while the node is restarting you&#8217;ve lost some of your high-availability which for super-critical applications can make you nervous. Additionally, faster restarts mean that you can complete maintenance activities faster &#8211; for example, a software upgrade requires a rolling restart of all of the nodes &#8211; if you have 48 data nodes then you want each of the data nodes to restart as quickly as possible.</p>
<p>MySQL 7.4 includes a number of optimisations to the restart code and so if you&#8217;re already using MySQL Cluster, it might be interesting to see how much faster it gets for your application.</p>
<h3>Enhanced Reporting</h3>
<p>MySQL Cluster presents a lot of monitoring information through the ndbinfo database and in 7.4 we&#8217;ve added some extra information on how memory is used for individual tables and how operations are distributed.</p>
<h4>Extra Memory Reporting</h4>
<p>MySQL Cluster allocates all of the required memory when a data node starts and so any information on memory usage from the operating system is of limited use and provides no clues as to how memory is used with the data nodes &#8211; for example, which tables are using the most memory. Also, as this is a distributed database, it is helpful to understand whether a particular table is using a similar amount of memory in each data node (if not then it could be that a better partitioning/sharding key could be used). Finally, when rows are deleted from a table, the memory for those rows would typically remain allocated against that table and so it is helpful to understand how many of these ‘empty slots’ are available for use by new rows in that table. MySQL Cluster 7.4 introduces a new table – <a title="Understanding how memory is used in mmc" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-ndbinfo-memory-per-fragment.html" target="_blank">ndbinfo.memory_per_fragment</a> &#8211; that provides that information.</p>
<p>For example; to see how much memory is being used by each data node for a particular table&#8230;</p>
<pre>mysql&gt; CREATE DATABASE clusterdb;USE clusterdb;
mysql&gt; CREATE TABLE simples (id INT NOT NULL AUTO_INCREMENT PRIMARY KEY) ENGINE=NDB;
mysql&gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1280 |         40 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1280 |         40 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4256 |        133 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4256 |        133 |
+------+------+-------------+------------+------------+
</pre>
<p>When you delete rows from a MySQL Cluster table, the memory is not actually freed up and so if you check the existing <a title="memoryusage table" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-ndbinfo-memoryusage.html" target="_blank">memoryusage table</a> you won&#8217;t see a change. This memory will be reused when you add new rows to that same table. In MySQL Cluster 7.4, it&#8217;s possible to see how much memory is in that state for a table&#8230;</p>
<pre>mysql&gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1280 |         40 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1280 |         40 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4256 |        133 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4256 |        133 |
+------+------+-------------+------------+------------+
mysql&gt; DELETE FROM clusterdb.simples LIMIT 1;
mysql&gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |      131072 |       5504 |        172 |
|    1 |    2 |      131072 |       1312 |         41 |
|    2 |    0 |      131072 |       5504 |        172 |
|    2 |    2 |      131072 |       1312 |         41 |
|    3 |    1 |      131072 |       3104 |         97 |
|    3 |    3 |      131072 |       4288 |        134 |
|    4 |    1 |      131072 |       3104 |         97 |
|    4 |    3 |      131072 |       4288 |        134 |
+------+------+-------------+------------+------------+
</pre>
<p>As a final example, we can check whether a table is being evenly sharded accross the data nodes (in this case a realy bad sharding key was chosen)&#8230;</p>
<pre>mysql&gt; CREATE TABLE simples (id INT NOT NULL AUTO_INCREMENT, \
        species VARCHAR(20) DEFAULT "Human", 
        PRIMARY KEY(id, species)) engine=ndb PARTITION BY KEY(species);

// Add some data

mysql&gt; SELECT node_id AS node, fragment_num AS frag, \
        fixed_elem_alloc_bytes alloc_bytes, \
        fixed_elem_free_bytes AS free_bytes, \
        fixed_elem_free_rows AS spare_rows \
        FROM ndbinfo.memory_per_fragment \
        WHERE fq_name LIKE '%simples%';
+------+------+-------------+------------+------------+
| node | frag | alloc_bytes | free_bytes | spare_rows |
+------+------+-------------+------------+------------+
|    1 |    0 |           0 |          0 |          0 |
|    1 |    2 |      196608 |      11732 |        419 |
|    2 |    0 |           0 |          0 |          0 |
|    2 |    2 |      196608 |      11732 |        419 |
|    3 |    1 |           0 |          0 |          0 |
|    3 |    3 |           0 |          0 |          0 |
|    4 |    1 |           0 |          0 |          0 |
|    4 |    3 |           0 |          0 |          0 |
+------+------+-------------+------------+------------+
</pre>
<h4>Extra Operations Reporting</h4>
<p>To ensure that resources are being used effectively, it is very helpful to understand the how each table is being access (how frequently and for what types of operations). To support this, the ndbinfo. operations_per_fragment table is provided. For example, the data in this table would let you identify that a large number of full table scans are performed on a particular table.</p>
<p>It is also important to identify if there are any hotspots where a disproportionate share of the queries for a table are hitting a particular fragment/data node. Again, <a title="Monitoring the distributed nature of operations in MySQL Cluster" href="http://dev.mysql.com/doc/refman/5.6/en/mysql-cluster-ndbinfo-operations-per-fragment.html" target="_blank">ndbinfo. operations_per_fragment</a> provides this information.</p>
<p>As an example of how to use some of the data from this table, a simple table is created and populated and then ndbinfo.operations_per_fragment is used to monitor how many Primary Key reads and table scans are performed[1]:</p>
<pre>mysql&gt; CREATE TABLE simples (id INT AUTO_INCREMENT PRIMARY KEY, time TIMESTAMP) ENGINE=NDB;

mysql&gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     1 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     1 |
+-----------------------+-----------+-------+-------+

mysql&gt; INSERT INTO simples VALUES ();  # Repeated several times
mysql&gt; SELECT * FROM simples;
+----+---------------------+
| id | time                |
+----+---------------------+
|  7 | 2015-01-22 15:12:42 |
…
|  8 | 2015-01-22 15:12:58 |
+----+---------------------+
23 rows in set (0.00 sec)

mysql&gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     2 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     2 |
+-----------------------+-----------+-------+-------+

mysql&gt; SELECT * FROM simples WHERE id=11;
+----+---------------------+
| id | time                |
+----+---------------------+
| 11 | 2015-01-22 15:12:59 |
+----+---------------------+

mysql&gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+-------+-------+
| Table                 | Data Node | Reads | Scans |
+-----------------------+-----------+-------+-------+
| clusterdb/def/simples |         3 |     0 |     2 |
| clusterdb/def/simples |         3 |     0 |     0 |
| clusterdb/def/simples |         4 |     0 |     0 |
| clusterdb/def/simples |         4 |     1 |     2 |
+-----------------------+-----------+-------+-------+
</pre>
<p>Note that there are two rows listed for each data node but only one row for each has non-zero values; this is because each data node holds the primary fragment for one of the partitions and the secondary fragment for the other – all operations are performed only on the active fragments. This is made clearer if the fragment number is included in the query:</p>
<pre>mysql&gt; SELECT fq_name AS 'Table', node_id AS 'Data Node', fragment_num AS 'Fragment', tot_key_reads AS 'Reads', tot_frag_scans AS 'Scans' FROM ndbinfo.operations_per_fragment WHERE fq_name LIKE '%simples';
+-----------------------+-----------+----------+-------+-------+
| Table                 | Data Node | Fragment | Reads | Scans |
+-----------------------+-----------+----------+-------+-------+
| clusterdb/def/simples |         3 |        0 |     0 |     2 |
| clusterdb/def/simples |         3 |        1 |     0 |     0 |
| clusterdb/def/simples |         4 |        0 |     0 |     0 |
| clusterdb/def/simples |         4 |        1 |     1 |     2 |
+-----------------------+-----------+----------+-------+-------+
</pre>
<h3>Conclusion</h3>
<p>We&#8217;re really excited about the GA for this new MySQL Cluster release; if you get chance to try it out then please let us know how you get on &#8211; either through a comment on this blog, a <a title="MySQL bug report" href="http://bugs.mysql.com/" target="_blank">MySQL bug report</a> or a post to the <a title="MySQL Cluster Forum" href="http://forums.mysql.com/list.php?25" target="_blank">MySQL Cluster Forum</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989063&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989063&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 16:00:47 +0000</pubDate>
    <dc:creator>Andrew Morgan</dc:creator>
    <category>MySQL Cluster</category>
    <category>GA</category>
    <category>Geographic Replication</category>
    <category>Multi-Master Replication</category>
    <category>MySQL</category>
    <category>MySQL Cluster 7.4</category>
    <category>NoSQL</category>
    <category>Performance</category>
    <category>Replication</category>
    <category>sql</category>
  </item>

  <item>
    <title>Connector/Python 2.1.1 Alpha released with C Extension</title>
    <guid isPermaLink="false">http://geert.vanderkelen.org/?p=1666</guid>
    <link>http://geert.vanderkelen.org/mysql-connector-python-2-1-1/</link>
    <description>MySQL Connector/Python 2.1.1 took a while to release and that was because we had to add some more packages which contains the optional C Extension. Note that this is still Alpha and we want you guys to report any problems and requests.
The Connector/Python C Extension was added because in certain situations, for example reading a huge result set, can take a long time with pure Python. That&amp;#8217;s why we choose to interface with Connector/C (libmysqlclient).
Note: Pure Python is still default and it will be kept that way!
Installing Connector/Python 2.1 didn&amp;#8217;t change much:
shell&amp;gt; python setup.py install
If you&amp;#8217;d like the C Extension, you have to first install MySQL Connector/C or have the MySQL Server development packages available. Careful with mixing 32 and 64-bit: make sure Python matches your MySQL libraries. Connector/Python will try to detect the mismatch and notify you.
For example, on OS X with development tools installed, I would do the following:
shell&amp;gt; virtualenv CPYENV
shell&amp;gt; source CPYENV/bin/activate
shell&amp;gt; tar xzf ~/Downloads/mysql-connector-c-6.1.5-osx10.7-x86_64.tar.gz
shell&amp;gt; tar xzf ~/Downloads/mysql-connector-python-2.1.1.tar.gz
shell&amp;gt; cd mysql-connector-2.1.1
shell&amp;gt; python setup.py install --with-mysql-capi=../mysql-connector-c-6.1.5-osx10.7-x86_64
If all goes well, the above would have compiled and install the C Extension together with the pure Python code inside a virtual environment. Here is how you can check if the C Extension is available:
&amp;gt;&amp;gt;&amp;gt; import mysql.connector
&amp;gt;&amp;gt;&amp;gt; mysql.connector.HAVE_CEXT
True
If you want to see the speed improvements, you can load up the employees sample database and do the following in the Python interpreter:
shell&amp;gt; python
&amp;gt;&amp;gt;&amp;gt; import mysql.connector
&amp;gt;&amp;gt;&amp;gt; cnx = mysql.connector.connect(user='root', database='employees')
&amp;gt;&amp;gt;&amp;gt; cnxc = mysql.connector.connect(use_pure=False, user='root', database='employees')
&amp;gt;&amp;gt;&amp;gt; cur = cnx.cursor()
&amp;gt;&amp;gt;&amp;gt; q = &quot;SELECT * FROM salaries&quot;
&amp;gt;&amp;gt;&amp;gt; s=time(); cur.execute(q); r=cur.fetchall(); print(&quot;%.2f&quot; % (time()-s))
65.57
&amp;gt;&amp;gt;&amp;gt; cur = cnxc.cursor()
&amp;gt;&amp;gt;&amp;gt; s=time(); cur.execute(q); r=cur.fetchall(); print(&quot;%.2f&quot; % (time()-s))
13.09
That&amp;#8217;s 66 seconds vs. 13 seconds using the C Extension.
If that is not fast enough, and it is not, you can directly load the C Extension and use the wrapper around the MySQL C API (see manual). Here&amp;#8217;s an example:
&amp;gt;&amp;gt;&amp;gt; import _mysql_connector
&amp;gt;&amp;gt;&amp;gt; cnx = _mysql_connector.MySQL()
&amp;gt;&amp;gt;&amp;gt; cnx.connect(user='root', database='employees')
&amp;gt;&amp;gt;&amp;gt; cnx.query(&quot;SELECT emp_no, last_name, hire_date FROM employees&quot;)
True
&amp;gt;&amp;gt;&amp;gt; cnx.fetch_row()
(10001, 'Facello', datetime.date(1986, 6, 26))
&amp;gt;&amp;gt;&amp;gt; cnx.free_result()
&amp;gt;&amp;gt;&amp;gt; cnx.close()
It is a bit different than using mysql.connector, but notice that result coming from the C Extension is also converted to Python data types.
How fast is using _mysql_connector? Lets say we want the raw data, save the following to a Python script file and execute:
from time import time
import _mysql_connector
cnx = _mysql_connector.MySQL(raw=True)
cnx.connect(user='root', database='employees')
cnx.query(&quot;SELECT * FROM salaries&quot;)
s = time()
row = cnx.fetch_row()
while row:
  row = cnx.fetch_row()
cnx.free_result()

print(&quot;All fetched in %.2fs&quot; % (time() - s))

cnx.close()
The output would be something like this:
All fetched in 2.25s
If you put it all together, and this is not scientific, just on my OS X MacBook, SELECT * FORM salaries:

Pure Python, not raw: 66 seconds
Using C Extension with PEP-249, not raw 13 seconds
Using C Extension directly, not raw: 12 seconds
Using C Extension, raw: 3 seconds

If you want to dump big sets of data, and you want to do it the Python way, you can use the C Extension to get it faster.
Yes, the C Extension works and compiles on Windows!</description>
    <content:encoded><![CDATA[<p><a href="http://www.mysql.com" target="_blank">MySQL</a> <a href="http://forums.mysql.com/read.php?3,628575,628575#msg-628575" target="_blank">Connector/Python 2.1.1</a> took a while to release and that was because we had to add some more packages which contains the optional C Extension. Note that this is still Alpha and we want you guys to <a href="https://bugs.mysql.com" target="_blank">report any problems and requests</a>.</p>
<p>The Connector/Python C Extension was added because in certain situations, for example reading a huge result set, can take a long time with pure Python. That&#8217;s why we choose to interface with <a href="http://dev.mysql.com/doc/connector-c/en/index.html" target="_blank">Connector/C</a> (libmysqlclient).</p>
<p><strong>Note: Pure Python is still default and it will be kept that way!</strong></p>
<p>Installing Connector/Python 2.1 didn&#8217;t change much:</p>
<pre>shell&gt; python setup.py install</pre>
<p>If you&#8217;d like the C Extension, you have to first install MySQL Connector/C or have the MySQL Server development packages available. Careful with mixing 32 and 64-bit: make sure Python matches your MySQL libraries. Connector/Python will try to detect the mismatch and notify you.</p>
<p>For example, on OS X with development tools installed, I would do the following:</p>
<pre>shell&gt; virtualenv CPYENV
shell&gt; source CPYENV/bin/activate
shell&gt; tar xzf ~/Downloads/mysql-connector-c-6.1.5-osx10.7-x86_64.tar.gz
shell&gt; tar xzf ~/Downloads/mysql-connector-python-2.1.1.tar.gz
shell&gt; cd mysql-connector-2.1.1
shell&gt; python setup.py install --with-mysql-capi=../mysql-connector-c-6.1.5-osx10.7-x86_64</pre>
<p>If all goes well, the above would have compiled and install the C Extension together with the pure Python code inside a virtual environment. Here is how you can check if the C Extension is available:</p>
<pre>&gt;&gt;&gt; import mysql.connector
&gt;&gt;&gt; mysql.connector.HAVE_CEXT
True</pre>
<p>If you want to see the speed improvements, you can load up the <a href="https://dev.mysql.com/doc/employee/en/" target="_blank">employees sample database</a> and do the following in the Python interpreter:</p>
<pre>shell&gt; python
&gt;&gt;&gt; import mysql.connector
&gt;&gt;&gt; cnx = mysql.connector.connect(user='root', database='employees')
&gt;&gt;&gt; cnxc = mysql.connector.connect(<strong>use_pure=False</strong>, user='root', database='employees')
&gt;&gt;&gt; cur = cnx.cursor()
&gt;&gt;&gt; q = "SELECT * FROM salaries"
&gt;&gt;&gt; s=time(); cur.execute(q); r=cur.fetchall(); print("%.2f" % (time()-s))
<strong>65.57</strong>
&gt;&gt;&gt; cur = cnxc.cursor()
&gt;&gt;&gt; s=time(); cur.execute(q); r=cur.fetchall(); print("%.2f" % (time()-s))
<strong>13.09</strong></pre>
<p>That&#8217;s 66 seconds vs. 13 seconds using the C Extension.</p>
<p>If that is not fast enough, and it is not, you can directly load the C Extension and use the wrapper around the MySQL C API (see <a title="Connector/Python C Extension API Reference" href="http://dev.mysql.com/doc/connector-python/en/connector-python-cext-reference.html" target="_blank">manual</a>). Here&#8217;s an example:</p>
<pre>&gt;&gt;&gt; import _mysql_connector
&gt;&gt;&gt; cnx = _mysql_connector.MySQL()
&gt;&gt;&gt; cnx.connect(user='root', database='employees')
&gt;&gt;&gt; cnx.query("SELECT emp_no, last_name, hire_date FROM employees")
True
&gt;&gt;&gt; cnx.fetch_row()
(10001, 'Facello', datetime.date(1986, 6, 26))
&gt;&gt;&gt; cnx.free_result()
&gt;&gt;&gt; cnx.close()</pre>
<p>It is a bit different than using mysql.connector, but notice that result coming from the C Extension is also converted to Python data types.</p>
<p>How fast is using _mysql_connector? Lets say we want the raw data, save the following to a Python script file and execute:</p>
<pre>from time import time
import _mysql_connector</pre>
<pre>cnx = _mysql_connector.MySQL(<strong>raw=True</strong>)
cnx.connect(user='root', database='employees')
cnx.query("SELECT * FROM salaries")
s = time()
row = cnx.fetch_row()
while row:
  row = cnx.fetch_row()
cnx.free_result()

print("All fetched in %.2fs" % (time() - s))

cnx.close()</pre>
<p>The output would be something like this:</p>
<pre><strong>All fetched in 2.25s</strong></pre>
<p>If you put it all together, and this is not scientific, just on my OS X MacBook, SELECT * FORM salaries:</p>
<ul>
<li>Pure Python, not raw: 66 seconds</li>
<li>Using C Extension with PEP-249, not raw 13 seconds</li>
<li>Using C Extension directly, not raw: 12 seconds</li>
<li>Using C Extension, raw: 3 seconds</li>
</ul>
<p>If you want to dump big sets of data, and you want to do it the Python way, you can use the C Extension to get it faster.</p>
<p>Yes, the C Extension works and compiles on Windows!</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989057&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989057&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 11:31:49 +0000</pubDate>
    <dc:creator>Geert Vanderkelen</dc:creator>
    <category>MySQL</category>
    <category>Oracle</category>
    <category>Python</category>
    <category>Work</category>
    <category>myconnpy</category>
    <category>mysql</category>
    <category>python</category>
    <category>work</category>
  </item>

  <item>
    <title>Worrying about the ‘InnoDB: detected cycle in LRU for buffer pool (…)’ message?</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28663</guid>
    <link>http://www.percona.com/blog/2015/02/26/worrying-about-the-innodb-detected-cycle-in-lru-for-buffer-pool-message/</link>
    <description>If you use Percona Server 5.5 and you have configured it to use multiple buffer pool instances than sooner or later you&amp;#8217;ll see the following lines on the server&amp;#8217;s error log and chances are you&amp;#8217;ll be worried about them:InnoDB: detected cycle in LRU for buffer pool 5, skipping to next buffer pool.
InnoDB: detected cycle in LRU for buffer pool 3, skipping to next buffer pool.
InnoDB: detected cycle in LRU for buffer pool 7, skipping to next buffer pool.Worry not as this is mostly harmless. It&amp;#8217;s becoming a February tradition for me (Fernando) to face a question about this subject (ok, it&amp;#8217;s maybe a coincidence) and this time I&amp;#8217;ve teamed up with my dear colleague and software engineer George Lorch to provide you the most complete blog post ever published on this topic &amp;#8230; (with a belated thank you! to Ernie Souhrada, with whom I&amp;#8217;ve also discussed this same matter one year ago).InnoDB internals: what is &amp;#8220;LRU&amp;#8221; ?There&amp;#8217;s a short and to-the-point section of the MySQL manual that explains in a clear way what is the InnoDB buffer pool, how it operates and why it plays such an important role in MySQL performance. If you&amp;#8217;re interested in understanding InnoDB internals then that page is a good start. In this section we&amp;#8217;ll refrain ourselves to explain what the &amp;#8220;LRU&amp;#8221; that shows in our subject message is so we&amp;#8217;ll only slightly dig into InnoDB internals, enough to make for some context. Here&amp;#8217;s a quick introduction to the buffer pool, quoting from the above manual page:InnoDB maintains a storage area called the buffer pool for caching data and indexes in memory. (&amp;#8230;) Ideally, you set the size of the buffer pool to as large a value as practical, leaving enough memory for other processes on the server to run without excessive paging. The larger the buffer pool, the more InnoDB acts like an in-memory database, reading data from disk once and then accessing the data from memory during subsequent reads.In practice, however, we can rarely fit our whole dataset inside the InnoDB buffer pool so there must be a process to manage this limited pool of memory pages:InnoDB manages the pool as a list, using a variation of the least recently used (LRU) algorithm. When room is needed to add a new block to the pool, InnoDB evicts the least recently used block and adds the new block to the middle of the list.There you go, InnoDB employs a variation of the Least Recently Used algorithm called midpoint insertion strategy to manage the pages within the buffer pool. We should mention it does makes exceptions, such as during a full table scan, when it knows the loaded pages might end up being read only a single time.Dumping and reloading the buffer poolBefore we can get to the main point of this article lets first examine why would you want to dump the buffer pool to disk, which is at the core of the matter here: that&amp;#8217;s when those warning messages we&amp;#8217;re discussing may appear.When you start a MySQL server the buffer pool is empty by default. Performance is at it&amp;#8217;s worse at this point because no data can be found in memory so in practice each request for data results in an I/O operation to retrieve the data in the disk and bring it to memory. With time the buffer pool gets filled and performance improves &amp;#8211; more and more data can now be found in memory. With yet more time we reach a peek performance state: the buffer pool not only is full but it is filled with the most popular data. The time between the start of the server and reaching this optimum state in the buffer pool is called server warm up. How long it takes depends mostly on two things: the size of the buffer pool and the level of activity of the server &amp;#8211; the less busy it is the less requests it will get and thus more time is needed until the popular data is fully loaded.Now, there could be a shortcut: what if before we save the buffer pool on a disk file before we stop MySQL? We could later use it to reload the buffer pool to an optimum state when we restart the server, thus decreasing the warm up period dramatically.Percona was a pioneer in this field related to other MySQL distributions and implemented this functionality in Percona Server 5.5. Later on, MySQL 5.6 was released with a similar functionality which also allowed preloading the buffer pool for a faster warm up. Percona Server 5.6 incorporates this upstream feature, effectively replacing its own implementation. However, while in Percona Server 5.5 we could periodically dump the buffer pool in MySQL and Percona Server 5.6 it is only dumped at shutdown or at request.&amp;#8220;Detected cycle in LRU&amp;#8221;In the section above we introduced a functionality that allows to dump a fingerprint of the buffer pool to disk so we can later reload it at server restart (note that even though the buffer pool might be very large the fingerprint will be small enough to make this practical). What we didn&amp;#8217;t mention was that this is yet most useful outside of maintenance time and planned shutdows &amp;#8211; that is, when the server crashes. When a crash happens it&amp;#8217;s that more important to bring it back to a warm up state soon, so it can resume providing data fast enough. And giving we cannot predict a crash the only way we can arrange to have the latest buffer pool on disk is by flushing it often.While the buffer pool is divided into pages for efficiency of high-volume read operations it is implemented as a linked list of pages, for efficiency of cache management. During the process of dumping the buffer pool to disk a mutex is acquired on the LRU list. However, this mutex is not hold for the duration of the process &amp;#8211; it is periodically released to prevent stalling of the system. The problem is: in between the release of the mutex and the moment it is acquired again the list may get reshuffled. Since the dump keeps a pointer to its position across the mutex boundry, the dump can get put into some artificial cycling.Lets consider a linked list:A &amp;gt; B &amp;gt; C &amp;gt; D &amp;gt; Ewhere each letter corresponds to a memory page. Now lets say the initial dump was partially taken and covered the first three pages, &amp;#8220;A &amp;gt; B &amp;gt; C&amp;#8221;, placing a pointer on &amp;#8220;C&amp;#8221; before releasing the mutex. Once the mutex is reacquired the list has been reshuffled:  &amp;#8220;A &amp;gt; C &amp;gt; B &amp;gt; D &amp;gt; E&amp;#8221;. The resulting junction of the partial list we have already copied and the reshuffled list now includes a loop, which would incur in a cycle: &amp;#8220;(A &amp;gt; B &amp;gt; C) &amp;gt; B &amp;gt; D &amp;gt; E&amp;#8221;. When the dumping process detects a cycle on the LRU list it stops copying from the actual buffer pool, throws in a warning message, and moves on to the next buffer pool instance &amp;#8211; otherwise it would keep dumping in an infinite loop.How harmless are those messages ?It is fairly harmless except for the fact you will only have a partial LRU list dump for that buffer pool instance &amp;#8211; that is, until the next dump occurs. If the server crashes or is shutdown before the next dump takes place the existing one won&amp;#8217;t be totally up to date for the server warm up to complete &amp;#8211; it will still be used and will still provide a partially filled, somewhat &amp;#8220;warm&amp;#8221; buffer pool, just not as optimal as it could have been if the last dump had been taken fully.The post Worrying about the &amp;#8216;InnoDB: detected cycle in LRU for buffer pool (&amp;#8230;)&amp;#8217; message? appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p>If you use Percona Server 5.5 and you have configured it to use multiple buffer pool instances than sooner or later you&#8217;ll see the following lines on the server&#8217;s error log and chances are you&#8217;ll be worried about them:</p><pre>InnoDB: detected cycle in LRU for buffer pool 5, skipping to next buffer pool.
InnoDB: detected cycle in LRU for buffer pool 3, skipping to next buffer pool.
InnoDB: detected cycle in LRU for buffer pool 7, skipping to next buffer pool.</pre><p>Worry not as this is mostly harmless. It&#8217;s becoming a February tradition for me <em>(Fernando)</em> to face a question about this subject <em>(ok, it&#8217;s maybe a coincidence)</em> and this time I&#8217;ve teamed up with my dear colleague and software engineer <strong>George Lorch</strong> to provide you <em>the most complete blog post ever published on this topic</em> &#8230; <em>(with a belated <span>thank you</span>! to <strong>Ernie Souhrada</strong>, with whom I&#8217;ve also discussed this same matter one year ago)</em>.</p><h2>InnoDB internals: what is &#8220;LRU&#8221; ?</h2><p>There&#8217;s a short and to-the-point section of the MySQL <a title="The InnoDB Buffer Pool" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-buffer-pool.html">manual</a> that explains in a clear way what is the InnoDB buffer pool, how it operates and why it plays such an important role in MySQL performance. If you&#8217;re interested in understanding InnoDB internals then that page is a good start. In this section we&#8217;ll refrain ourselves to explain what the &#8220;LRU&#8221; that shows in our subject message is so we&#8217;ll only slightly dig into InnoDB internals, enough to make for some context. Here&#8217;s a quick introduction to the buffer pool, quoting from the above manual page:</p><blockquote><p>InnoDB maintains a storage area called the buffer pool for <strong>caching data and indexes in memory</strong>. (&#8230;) Ideally, you set the size of the buffer pool to as large a value as practical, leaving enough memory for other processes on the server to run without excessive paging. The larger the buffer pool, the more InnoDB acts like an in-memory database, reading data from disk once and then accessing the data from memory during subsequent reads.</p></blockquote><p>In practice, however, we can rarely fit our whole dataset inside the InnoDB buffer pool so there must be a process to manage this limited pool of memory pages:</p><blockquote><p>InnoDB manages the pool as a list, using a variation of the least recently used (LRU) algorithm. When room is needed to add a new block to the pool, InnoDB evicts the least recently used block and adds the new block to the middle of the list.</p></blockquote><p>There you go, InnoDB employs a variation of the <em><strong>L</strong>east <strong>R</strong>ecently <strong>U</strong>sed</em> algorithm called <em>midpoint insertion strategy </em>to manage the pages within the buffer pool. We should mention it does <a title="MySQL Glossary - LRU" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_lru">makes exceptions</a>, such as during a full table scan, when it knows the loaded pages might end up being read only a single time.</p><h2>Dumping and reloading the buffer pool</h2><p>Before we can get to the main point of this article lets first examine why would you want to dump the buffer pool to disk, which is at the core of the matter here: that&#8217;s when those warning messages we&#8217;re discussing may appear.</p><p>When you start a MySQL server the buffer pool is empty by default. Performance is at it&#8217;s worse at this point because no data can be found in memory so in practice each request for data results in an I/O operation to retrieve the data in the disk and bring it to memory. With time the buffer pool gets filled and performance improves &#8211; more and more data can now be found in memory. With yet more time we reach a peek performance state: the buffer pool not only is full but it is filled with the most popular data. The time between the start of the server and reaching this optimum state in the buffer pool is called server <a title="MySQL Glossary - warm up" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_warm_up"><strong>warm up</strong></a>. How long it takes depends mostly on two things: the size of the buffer pool and the level of activity of the server &#8211; the less busy it is the less requests it will get and thus more time is needed until the popular data is fully loaded.</p><p>Now, there could be a <strong>shortcut</strong>: what if before we save the buffer pool on a disk file before we stop MySQL? We could later use it to reload the buffer pool to an optimum state when we restart the server, thus decreasing the warm up period dramatically.</p><p>Percona was a pioneer in this field related to other MySQL distributions and <a title="DUMP/RESTORE OF THE BUFFER POOL" href="http://www.percona.com/doc/percona-server/5.5/management/innodb_lru_dump_restore.html">implemented this functionality</a> in Percona Server 5.5. Later on, MySQL 5.6 was released with a similar functionality which also allowed <a title="Preloading the InnoDB Buffer Pool for Faster Restart" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-preload-buffer-pool.html">preloading the buffer pool for a faster warm up</a>. Percona Server 5.6 incorporates this upstream feature, effectively replacing its own implementation. However, while in Percona Server 5.5 we could periodically dump the buffer pool in MySQL and Percona Server 5.6 it is only dumped at <a title="innodb_buffer_pool_dump_at_shutdown" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_buffer_pool_dump_at_shutdown">shutdown</a> or at <a title="innodb_buffer_pool_dump_now" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_buffer_pool_dump_now">request</a>.</p><h2>&#8220;Detected cycle in LRU&#8221;</h2><p>In the section above we introduced a functionality that allows to dump a <em>fingerprint</em> of the buffer pool to disk so we can later reload it at server restart (note that even though the buffer pool might be very large the fingerprint will be small enough to make this practical). What we didn&#8217;t mention was that this is yet most useful outside of maintenance time and planned shutdows &#8211; that is, when the server crashes. When a crash happens it&#8217;s that more important to bring it back to a warm up state soon, so it can resume providing data fast enough. And giving we cannot predict a crash the only way we can arrange to have the latest buffer pool on disk is by flushing it <span>often</span>.</p><p>While the buffer pool is divided into pages for efficiency of high-volume <span>read operations</span> it is <a title="MySQL Glossary - Buffer Pool" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/glossary.html#glos_buffer_pool">implemented as a <strong>linked list </strong>of pages</a>, for efficiency of <span>cache management</span>. During the process of dumping the buffer pool to disk a mutex is acquired on the LRU list. However, this mutex is not hold for the duration of the process &#8211; it is periodically released to prevent stalling of the system. The problem is: in between the release of the mutex and the moment it is acquired again the list may get reshuffled. Since the dump keeps a pointer to its position across the mutex boundry, the dump can get put into some <span>artificial cycling</span>.</p><p>Lets consider a linked list:</p><blockquote><p>A &gt; B &gt; C &gt; D &gt; E</p></blockquote><p>where each letter corresponds to a memory page. Now lets say the initial dump was partially taken and covered the first three pages, &#8220;A &gt; B &gt; C&#8221;, placing a pointer on &#8220;C&#8221; before releasing the mutex. Once the mutex is reacquired the list has been reshuffled:  &#8220;A &gt; C &gt; B &gt; D &gt; E&#8221;. The resulting junction of the partial list we have already copied and the reshuffled list now includes a loop, which would incur in a cycle: &#8220;(A &gt; <strong>B</strong> &gt; <span>C</span>) &gt; <strong>B</strong> &gt; D &gt; E&#8221;. When the dumping process detects a cycle on the LRU list it stops copying from the actual buffer pool, <strong>throws in a warning message</strong>, and moves on to the <a title="Using Multiple Buffer Pool Instances" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-multiple-buffer-pools.html">next buffer pool instance</a> &#8211; otherwise it would keep dumping in an infinite loop.</p><h2>How harmless are those messages ?</h2><p>It is fairly harmless except for the fact you will only have a partial LRU list dump for that buffer pool instance &#8211; that is, until the next dump occurs. If the server crashes or is shutdown before the next dump takes place the existing one won&#8217;t be totally up to date for the server warm up to complete &#8211; it will still be used and will still provide a partially filled, somewhat &#8220;warm&#8221; buffer pool, just not as optimal as it could have been if the last dump had been taken fully.</p><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/02/26/worrying-about-the-innodb-detected-cycle-in-lru-for-buffer-pool-message/">Worrying about the &#8216;InnoDB: detected cycle in LRU for buffer pool (&#8230;)&#8217; message?</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989054&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989054&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 08:00:50 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>InnoDB</category>
    <category>Insight for DBAs</category>
    <category>MySQL</category>
    <category>Ernie Souhrada</category>
    <category>Fernando Laudares</category>
    <category>George Lorch</category>
    <category>InnoDB buffer pool</category>
    <category>Least Recently Used</category>
    <category>LRU</category>
    <category>Primary</category>
  </item>

  <item>
    <title>See Connection Latency with VividCortex</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/02/26/see-connection-latency-with-vividcortex/</guid>
    <link>https://vividcortex.com/blog/2015/02/26/see-connection-latency-with-vividcortex/</link>
    <description>Recently, one of our customers reached out to us about a problem they were having trouble diagnosing - they noticed that every once in a while, it took their MySQL server a few seconds to initialize a client connection. They had figured out that the issue was a missing hosts entry for one of their clients, but VividCortex didn’t have any measurements to show this issue to them.

We always enjoy getting customer feedback, and in this case we saw a clear need for something that VividCortex was missing. To address this shortcoming, we’ve recently added the ability to see the connection latency for newly established connections against your MySQL server!

Connection latency can be a real problem for MySQL; there are plenty of documented cases of server configurations causing serious performance issues for users (see here for example). It’s also fairly easy for us to measure. Since we monitor metrics on database performance through packet-sniffing, measuring the latency is just a matter of identifying the client’s SYN packet and the server’s responding handshake packet. The resulting measurement covers the time it takes for the server to accept the incoming TCP connection and return a handle, then do a DNS lookup on the client address (unless you have skip_name_resolve set in your configuration), and respond with the MySQL handshake packet. 

Below is an example of the view:



Our servers have skip_name_resolve set, so it usually only takes a few hundred microseconds to accept an incoming connection and send out the initial handshake packet (the latency will be higher if your server has to do DNS resolution for a client connection). You can see that every once in a while this latency will jump to a few milliseconds - this is likely a result of the database being busy processing a query, so it takes a little longer to respond to the client connection. If the latency spikes higher to hundreds of milliseconds, then it could be an indication that the server is under heavy load or there is a problem with DNS resolution.

For our customers who are eager to check the connection latency of their MySQL servers, you can find the connection latency graph under the ‘MySQL’ category on our Charts page (note that this feature is not available if you are monitoring remote MySQL instances as it relies on TCP sniffing). As always, be sure to let us know if you have any feedback, and enjoy!

If you are not yet a customer, sign up for free trial to see connection latency and o, so much more.</description>
    <content:encoded><![CDATA[<p>Recently, one of our customers reached out to us about a problem they were having trouble diagnosing - they noticed that every once in a while, it took their MySQL server a few seconds to initialize a client connection. They had figured out that the issue was a missing hosts entry for one of their clients, but VividCortex didn’t have any measurements to show this issue to them.</p>

<p>We always enjoy getting customer feedback, and in this case we saw a clear need for something that VividCortex was missing. To address this shortcoming, we’ve recently added the ability to see the connection latency for newly established connections against your MySQL server!</p>

<p>Connection latency can be a real problem for MySQL; there are plenty of documented cases of server configurations causing serious performance issues for users (see <a href="http://www.percona.com/blog/2008/05/31/dns-achilles-heel-mysql-installation/">here</a> for example). It’s also fairly easy for us to measure. Since we monitor metrics on database performance through packet-sniffing, measuring the latency is just a matter of identifying the client’s SYN packet and the server’s responding handshake packet. The resulting measurement covers the time it takes for the server to accept the incoming TCP connection and return a handle, then do a DNS lookup on the client address (unless you have skip_name_resolve set in your configuration), and respond with the MySQL handshake packet. </p>

<p>Below is an example of the view:</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/connection-latency.png" alt="Connection Latency" /></p>

<p>Our servers have skip_name_resolve set, so it usually only takes a few hundred microseconds to accept an incoming connection and send out the initial handshake packet (the latency will be higher if your server has to do DNS resolution for a client connection). You can see that every once in a while this latency will jump to a few milliseconds - this is likely a result of the database being busy processing a query, so it takes a little longer to respond to the client connection. If the latency spikes higher to hundreds of milliseconds, then it could be an indication that the server is under heavy load or there is a problem with DNS resolution.</p>

<p>For our customers who are eager to check the connection latency of their MySQL servers, you can find the connection latency graph under the ‘MySQL’ category on our Charts page (note that this feature is not available if you are monitoring remote MySQL instances as it relies on TCP sniffing). As always, be sure to let us know if you have any feedback, and enjoy!</p>

<p>If you are not yet a customer, <a href="https://app.vividcortex.com/sign-up?utm_source=organic&amp;utm_medium=blog">sign up for free trial</a> to see connection latency and o, so much more.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989066&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989066&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>MySQL Cluster 7.4 GA: 200 Million QPS, Active-Active Geographic Replication and more</title>
    <guid isPermaLink="false">http://dev.mysql.com/tech-resources/articles/mysql-cluster-7.4.html</guid>
    <link>http://dev.mysql.com/tech-resources/articles/mysql-cluster-7.4.html</link>
    <description>The MySQL team at Oracle are excited to announce the General Availability of MySQL Cluster 7.4, in other words - it's now ready for production workloads.

This is a release which takes what was already great about MySQL Cluster (real-time performance through memory-optimized tables, linear scale-out with transparrent sharding and cross-shard joins, High Availability and SQL as well as NoSQL interfaces) and makes it even faster, easier to manage and simpler to run accross geographies.</description>
    <content:encoded><![CDATA[The MySQL team at Oracle are excited to announce the General Availability of MySQL Cluster 7.4, in other words - it's now ready for production workloads.

This is a release which takes what was already great about MySQL Cluster (real-time performance through memory-optimized tables, linear scale-out with transparrent sharding and cross-shard joins, High Availability and SQL as well as NoSQL interfaces) and makes it even faster, easier to manage and simpler to run accross geographies.<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989062&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989062&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Thu, 26 Feb 2015 00:59:59 +0000</pubDate>
    <dc:creator>MySQL</dc:creator>
  </item>

  <item>
    <title>Meet Devart ODBC Drivers for Oracle, MySQL, PostgreSQL and SQLite!</title>
    <guid isPermaLink="false">http://devart.com/news/2015/odbc-beta.html</guid>
    <link>http://devart.com/news/2015/odbc-beta.html</link>
    <description>Devart team is proud to introduce a new product line - ODBC Drivers. We believe we can offer the best features, quality, and technical support for database application developers.</description>
    <content:encoded><![CDATA[<p>Devart team is proud to introduce a new product line - <b>ODBC Drivers</b>. We believe we can offer the best features, quality, and technical support for database application developers.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989078&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989078&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 22:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Meet Devart ODBC Drivers for Oracle, MySQL, PostgreSQL and SQLite!</title>
    <guid isPermaLink="false">http://www.devart.com/news/2015/odbc-beta.html</guid>
    <link>http://www.devart.com/news/2015/odbc-beta.html</link>
    <description>Devart team is proud to introduce a new product line - ODBC Drivers. We believe we can offer the best features, quality, and technical support for database application developers.</description>
    <content:encoded><![CDATA[<p>Devart team is proud to introduce a new product line - <b>ODBC Drivers</b>. We believe we can offer the best features, quality, and technical support for database application developers.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989079&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989079&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 22:00:00 +0000</pubDate>
    <dc:creator>Julia Samarska</dc:creator>
  </item>

  <item>
    <title>Team MariaDB is back from SCALE13X - This show keeps getting bigger and better</title>
    <guid isPermaLink="false">1816 at http://mariadb.com</guid>
    <link>http://mariadb.com/blog/team-mariadb-back-scale13x-show-keeps-getting-bigger-and-better</link>
    <description>Wed, 2015-02-25 19:36Marc Sherwood Members of Team MariaDB have now all made it home from SCALE13X, held in LA. This year we had three talks. Max, talked about Advanced Query Routing and Proxying with MaxScale, and Sharding Your Data With Spider, while Colin Charles talks about MySQL in the Hosted Cloud. The talks will be available online shortly via the SCALE13X Website - we will tweet out links to our talks once they are posted.
This year the team who organizes SCALE13 added an extra day on Friday - and man was it busy (just check out the image above to see what the expo hall looked like). What is great about being in the expo hall at SCALE is that people are coming in to see the vendors - where at some shows they need to lure attendees in with the promise of free ice cream bars (not that I am against free ice cream), but this means that they are really interested in talking with those in hall. For me this speaks of highly of the culture that has been built, and maintained as this show grows. Though we must attribute our use of booth babes as part of our draw. That Rod Allen sure draws a crowd!
With every show that we attend we find more and more people that are not only familiar with MariaDB, but are also using MariaDB. For me this was the event where I felt that more people that I talked with are using MariaDB than not. This is due in part to our team taking part in doing talks at events like this, but also due to MariaDB now being the default RDBMS in most major Linux distributions, and then packaged with those where we are not the default. Those that we talked to who were not yet using MariaDB had lots of questions about how to migrate to MariaDB. Thankfully the process of migration is really so simple that even us Marketing folks could answer these questions. If you have any questions on how to migration you can have a look at our Knowledge Base, or you can contact us via our website. To see where we will be next have a look at the  Events Section on our website.
Tags:&amp;nbsp;Linux
About the Author
  
      


Marc Sherwood
Marc Sherwood is North American Marketing Manager.



</description>
    <content:encoded><![CDATA[<div><div><div>Wed, 2015-02-25 19:36</div></div></div><div></div><div><div><div>Marc Sherwood</div></div></div><div><div><div property="content:encoded"><p><a href="http://imgur.com/gMfdkca"><img src="http://i.imgur.com/gMfdkcam.jpg" style="float:left; margin:20px" title="source: imgur.com" /></a> Members of Team MariaDB have now all made it home from <a href="http://www.socallinuxexpo.org/scale/13x">SCALE13X</a>, held in LA. This year we had three talks. Max, talked about <a href="http://www.socallinuxexpo.org/scale/13x/presentations/advanced-query-routing-and-proxying-maxscale">Advanced Query Routing and Proxying with MaxScale</a>, and <a href="http://www.socallinuxexpo.org/scale/13x/presentations/sharding-your-data-spider">Sharding Your Data With Spider</a>, while Colin Charles talks about <a href="http://www.socallinuxexpo.org/scale/13x/presentations/mysql-hosted-cloud">MySQL in the Hosted Cloud.</a> The talks will be available online shortly via the <a href="http://www.socallinuxexpo.org/scale/13x">SCALE13X Website</a> - we will tweet out links to our talks once they are posted.</p>
<p>This year the team who organizes <a href="http://www.socallinuxexpo.org/scale/13x">SCALE13</a> added an extra day on Friday - and man was it busy (just check out the image above to see what the expo hall looked like). What is great about being in the expo hall at SCALE is that people are coming in to see the vendors - where at some shows they need to lure attendees in with the promise of <strong>free ice cream bars</strong> (not that I am against free ice cream), but this means that they are really interested in talking with those in hall. For me this speaks of highly of the culture that has been built, and maintained as this show grows. Though we must attribute our use of booth babes as part of our draw. <img src="http://i.imgur.com/X0phGLTm.jpg" style="float:right; margin:20px" title="source: imgur.com" />That <a href="http://www.linkedin.com/pub/rod-allen/6/559/aa/en">Rod Allen sure draws a crowd!</a></p>
<p>With every show that we attend we find more and more people that are not only familiar with MariaDB, but are also using MariaDB. For me this was the event where I felt that more people that I talked with are using MariaDB than not. This is due in part to our team taking part in doing talks at events like this, but also due to MariaDB now being the default RDBMS in most major Linux distributions, and then packaged with those where we are not the default. Those that we talked to who were not yet using MariaDB had lots of questions about how to migrate to MariaDB. Thankfully the process of migration is really so simple that even us Marketing folks could answer these questions. If you have any questions on how to migration you can have a look at our <a href="https://mariadb.com/kb/en/mariadb/upgrading-from-mysql-to-mariadb/">Knowledge Base</a>, or you can <a href="https://mariadb.com/about/contact">contact us</a> via our website. To see where we will be next have a look at the <a href="https://mariadb.com/news-events/events"> Events Section</a> on our website.</p>
</div></div></div><div><div>Tags:&nbsp;</div><div><div><a href="http://mariadb.com/blog-tags/linux" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Linux</a></div></div></div><div><div><div><div>
<h2>About the Author</h2>
<div>  <div>
    <img typeof="foaf:Image" src="http://mariadb.com/sites/default/files/styles/logo_thumb_b_w/public/pictures/picture-4525-1406599998.jpg?itok=Bwf550WF" width="72" height="72" alt="Marc Sherwood's picture" title="Marc Sherwood's picture" />  </div>
</div>
<div>
<div>Marc Sherwood</div>
<div><p>Marc Sherwood is North American Marketing Manager.</p>
</div>
</div>
</div>
</div></div></div><div><div><div><div><div><a tw:count="vertical" tw:via="mariadb"></a></div><div><a fb:like:layout="box_count"></a></div><div><a g:plusone:size="tall"></a></div><div><a></a></div><div></div></div></div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989049&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989049&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 19:36:20 +0000</pubDate>
    <dc:creator>Marc Sherwood</dc:creator>
  </item>

  <item>
    <title>MySQL Webinar: Analyze &amp;amp; Tune Queries for Better Performance</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-1508669603650457962.post-4057570537484875993</guid>
    <link>http://oysteing.blogspot.com/2015/02/mysql-webinar-analyze-tune-queries-for_25.html</link>
    <description>Thanks to everyone who attended my webinar today, and thanks for all the positive feedback in the Q&amp;amp;A session.&amp;nbsp; Unfortunately, I was not able to respond to everyone during the 15 minutes available for Q&amp;amp;A.&amp;nbsp; If your question did not get answered, feel free to use the comments section of this blog post to ask your question.&amp;nbsp; You can also ask questions on the MySQL Optimizer Forum.The slides from the presentation are available here.&amp;nbsp; In a few days, I expect it to be possible to access it as an MySQL On-Demand Webinar.</description>
    <content:encoded><![CDATA[<div dir="ltr" trbidi="on"><div>Thanks to everyone who attended my webinar today, and thanks for all the positive feedback in the Q&amp;A session.&nbsp; Unfortunately, I was not able to respond to everyone during the 15 minutes available for Q&amp;A.&nbsp; If your question did not get answered, feel free to use the comments section of this blog post to ask your question.&nbsp; You can also ask questions on the <a href="http://forums.mysql.com/list.php?115">MySQL Optimizer Forum</a>.</div><div><br /></div><div>The slides from the presentation are available <a href="http://www.slideshare.net/oysteing/how-to-analyze-and-tune-sql-queries-for-better-performance-webinar">here</a>.&nbsp; In a few days, I expect it to be possible to access it as an <a href="http://www.mysql.com/news-and-events/on-demand-webinars/">MySQL On-Demand Webinar</a>.</div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989046&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989046&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 18:49:00 +0000</pubDate>
    <dc:creator>Øystein Grøvlen</dc:creator>
  </item>

  <item>
    <title>Shinguz: MySQL Enterprise Incremental Backup simplified</title>
    <guid isPermaLink="false">http://www.fromdual.com/mysql-enterprise-incremental-backup-simplified</guid>
    <link>http://www.fromdual.com/mysql-enterprise-incremental-backup-simplified</link>
    <description>Taxonomy upgrade extras:&amp;nbsp;mebMySQL Enterprise BackupenterpriseBackupincremental backupMySQL Enterprise Backup (MEB) has the capability to make real incremental (differential and cumulative?) backups. The actual releases are quite cool and you should really look at it...

Unfortunately the original MySQL documentation is much too complicated for my simple mind. So I did some testing and simplified it a bit for our customers...

If you want to dive into the original documentation please look here: Making an Incremental Backup .

If you want to use MySQL Enterprise Backup please let us know and we send you a quote...


Prepare MySQL Backup infrastructure

mkdir /backup/full /backup/incremental1 /backup/incremental2Full MySQL Backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full apply-logFirst MySQL Incremental Backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --incremental --incremental-base=dir:/backup/full --incremental-backup-dir=/backup/incremental1 backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full --incremental-backup-dir=/backup/incremental1 apply-incremental-backupSecond MySQL Incremental Backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --incremental --incremental-base=dir:/backup/full --incremental-backup-dir=/backup/incremental2 backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full --incremental-backup-dir=/backup/incremental2 apply-incremental-backupand so on...


MySQL Restore

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full copy-backHave fun with MySQL Enterprise Backup. If you need any help with your MySQL Backup concept, please let us know.
</description>
    <content:encoded><![CDATA[<div><div>Taxonomy upgrade extras:&nbsp;</div><div><div><a href="http://www.fromdual.ch/forum/674" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">meb</a></div><div><a href="http://www.fromdual.ch/forum/675" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">MySQL Enterprise Backup</a></div><div><a href="http://www.fromdual.ch/forum/666" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">enterprise</a></div><div><a href="http://www.fromdual.ch/taxonomy/term/19" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">Backup</a></div><div><a href="http://www.fromdual.ch/forum/676" typeof="skos:Concept" property="rdfs:label skos:prefLabel" datatype="">incremental backup</a></div></div></div><div><div><div property="content:encoded"><p>MySQL Enterprise Backup (MEB) has the capability to make real incremental (differential and cumulative?) backups. The actual releases are quite cool and you should really look at it...</p>

<p>Unfortunately the original MySQL documentation is much too complicated for my simple mind. So I did some testing and simplified it a bit for our customers...</p>

<p>If you want to dive into the original documentation please look here: <a href="http://dev.mysql.com/doc/mysql-enterprise-backup/3.11/en/mysqlbackup.incremental.html" title="Making an Incremental Backup " target="_blank">Making an Incremental Backup </a>.</p>

<p>If you want to use MySQL Enterprise Backup please let us know and <a href="mailto:contact@fromdual.com?Subject=Quote%20for%20MySQL%20Enterprise%20Backup">we send you a quote</a>...</p>


<h2>Prepare MySQL Backup infrastructure</h2>

<pre>mkdir /backup/full /backup/incremental1 /backup/incremental2</pre><br /><h2>Full MySQL Backup</h2>

<pre>mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full apply-log</pre><br /><h2>First MySQL Incremental Backup</h2>

<pre>mysqlbackup --defaults-file=/etc/my.cnf --user=root --incremental --incremental-base=dir:/backup/full --incremental-backup-dir=/backup/incremental<strong>1</strong> backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full --incremental-backup-dir=/backup/incremental<strong>1</strong> apply-incremental-backup</pre><br /><h2>Second MySQL Incremental Backup</h2>

<pre>mysqlbackup --defaults-file=/etc/my.cnf --user=root --incremental --incremental-base=dir:/backup/full --incremental-backup-dir=/backup/incremental<strong>2</strong> backup

mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full --incremental-backup-dir=/backup/incremental<strong>2</strong> apply-incremental-backup</pre><br /><p>and so on...</p>


<h2>MySQL Restore</h2>

<pre>mysqlbackup --defaults-file=/etc/my.cnf --user=root --backup-dir=/backup/full copy-back</pre><br /><p>Have fun with MySQL Enterprise Backup. If you need any help with your MySQL Backup concept, please <a href="mailto:contact@fromdual.com?Subject=Help%20needed%20for%20MySQL%20Backup%20concept" title="Contact FromDual">let us know</a>.</p>
</div></div></div><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989048&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989048&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 18:41:17 +0000</pubDate>
  </item>

  <item>
    <title>Log Buffer #411, A Carnival of the Vanities for DBAs</title>
    <guid isPermaLink="false">http://www.pythian.com/blog/?p=72341</guid>
    <link>http://www.pythian.com/blog/log-buffer-411-a-carnival-of-the-vanities-for-dbas/</link>
    <description>This Log Buffer Edition brings you some blog posts from Oracle, SQL Server and MySQL.
Oracle:
Suppose you have a global zone with multiple zpools that you would like to convert into a native zone.
The digital revolution is creating abundance in almost every industry—turning spare bedrooms into hotel rooms, low-occupancy commuter vehicles into taxi services, and free time into freelance time
Every time I attend a conference, the Twitter traffic about said conference is obviously higher.  It starts a couple weeks or even months before, builds steadily as the conference approaches, and then hits a crescendo during the conference.
Calling All WebLogic Users: Please Help Us Improve WebLogic Documentation!
Top Two Cloud Security Concerns: Data Breaches and Data Loss
SQL Server:
This article describes a way to identify the user who truncated the table &amp;amp; how you can recover the data.
When SQL Server 2014 was released, it included Hekaton, Microsoft’s much talked about memory-optimized engine that brings In-Memory OLTP into play.
Learn how you can easily spread your backup across multiple files.
Daniel Calbimonte has written a code comparison for MariaDB vs. SQL Server as it pertains to how to comment, how to create functions and procedures with parameters, how to store query results in a text file, how to show the top n rows in a query, how to use loops, and more.
The article show a simple way we managed to schedule index rebuild and reorg for an SQL instance with 106 databases used by one application using a Scheduled job.
MySQL:
How to setup a PXC cluster with GTIDs (and have async slaves replicating from it!)
vCloud Air and business-critical MySQL
MySQL Dumping and Reloading the InnoDB Buffer Pool
How to benchmark MongoDB
MySQL Server on SUSE 12</description>
    <content:encoded><![CDATA[<p>This Log Buffer Edition brings you some blog posts from Oracle, SQL Server and MySQL.</p>
<p><span></span><strong>Oracle:</strong></p>
<p>Suppose you have a global zone with multiple <a href="https://blogs.oracle.com/zoneszone/entry/global_to_non_global_conversion">zpools</a> that you would like to convert into a native zone.</p>
<p>The <a href="https://blogs.oracle.com/OracleIDM/entry/look_puppies_and_other_stories">digital</a> revolution is creating abundance in almost every industry—turning spare bedrooms into hotel rooms, low-occupancy commuter vehicles into taxi services, and free time into freelance time</p>
<p>Every time I attend a conference, the Twitter <a href="http://spendolini.blogspot.com/2015/02/screaming-at-each-other.html">traffic</a> about said conference is obviously higher.  It starts a couple weeks or even months before, builds steadily as the conference approaches, and then hits a crescendo during the conference.</p>
<p>Calling All <a href="http://blogs.oracle.com/WebLogicServer/entry/calling_all_weblogic_users_please1">WebLogic</a> Users: Please Help Us Improve WebLogic Documentation!</p>
<p>Top Two Cloud Security Concerns: <a href="https://blogs.oracle.com/securityinsideout/entry/top_two_cloud_security_concerns">Data</a> Breaches and Data Loss</p>
<p><strong>SQL Server:</strong></p>
<p>This <a href="http://www.sqlservercentral.com/articles/Truncate/109387/">article</a> describes a way to identify the user who truncated the table &amp; how you can recover the data.</p>
<p>When <a href="http://www.sqlservercentral.com/redirect/articles/122321/">SQL</a> Server 2014 was released, it included Hekaton, Microsoft’s much talked about memory-optimized engine that brings In-Memory OLTP into play.</p>
<p>Learn how you can easily <a href="http://www.sqlservercentral.com/articles/Backups/121689/">spread</a> your backup across multiple files.</p>
<p>Daniel Calbimonte has written a code comparison for <a href="http://www.sqlservercentral.com/redirect/articles/122328/">MariaDB</a> vs. SQL Server as it pertains to how to comment, how to create functions and procedures with parameters, how to store query results in a text file, how to show the top n rows in a query, how to use loops, and more.</p>
<p>The article show a simple way we managed to schedule index rebuild and reorg for an <a href="http://www.sqlservercentral.com/articles/Indexing/121744/">SQL</a> instance with 106 databases used by one application using a Scheduled job.</p>
<p><strong>MySQL:</strong></p>
<p>How to setup a PXC cluster with <a href="http://www.percona.com/blog/2015/02/20/how-to-setup-a-pxc-cluster-with-gtids-and-have-async-slaves-replicating-from-it/">GTID</a>s (and have async slaves replicating from it!)</p>
<p><a href="http://continuent-tungsten.blogspot.com/2015/02/clustered-mysql-in-vcloud-air.html">vCloud</a> Air and business-critical MySQL</p>
<p><a href="http://mysqlserverteam.com/mysql-dumping-and-reloading-the-innodb-buffer-pool/">MySQL</a> Dumping and Reloading the InnoDB Buffer Pool</p>
<p>How to benchmark <a href="http://www.acmebenchmarking.com/2015/02/how-to-benchmark-mongodb.html">MongoDB</a></p>
<p>MySQL Server on <a href="http://mysqlrelease.com/2015/02/mysql-server-on-suse-12/">SUSE</a> 12</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989044&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989044&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 18:00:33 +0000</pubDate>
    <dc:creator>The Pythian Group</dc:creator>
    <category>Pythian</category>
    <category>Log Buffer</category>
  </item>

  <item>
    <title>Increasing Cloud Database Efficiency – Like Crows in a Closet</title>
    <guid isPermaLink="false">http://www.tokutek.com/?p=9003</guid>
    <link>http://www.tokutek.com/2015/02/increasing-cloud-database-efficiency-like-crows-closet/</link>
    <description>In Mo’ Data, Mo’ Problems, we explored the paradox that “Big Data” projects pose to organizations and how Tokutek is taking an innovative approach to solving those problems. In this post, we’re going to talk about another hot topic in IT, “The Cloud,” and how enterprises undertaking Cloud efforts often struggle with idea of “problem trading.” Also, for some reason, databases are just given a pass as traditionally “noisy neighbors” and that there is nothing that can be done about it. Lets take a look at why we disagree.
With the birth of the information age came a coupling of business and IT. Increasingly strategic business projects and objectives were reliant on information infrastructure to provide information storage and retrieval instead of paper and filing cabinets. This was the dawn of the database and what gave rise to companies like Oracle, Sybase and MySQL. With the appearance of true Enterprise Grade databases, companies now had the unprecedented ability to store huge volumes of information and parse it at speeds that were unthinkable even five years prior. This dramatically increased the quality and speed at which business decisions were made. However, reliance on IT came with a downside, the planning, design, procurement and implementation process often comprised a majority of the overall project timeline, often pushing out completion by, at least, 3 months and up to 24. Furthermore, the projectizing of IT resources was quickly leading to islands of compute, network and storage resources. These projectized islands were reducing the dollar per dollar effectiveness of IT, which is commonly a cost center, and making it the target of every CFO and CIO looking to boost their bottom line.
The “resource island” problem was the first one that innovative companies started solving. Technologies like Mainframe computing, Storage Area Networks (SAN) and, later, hardware abstracted virtualization (VMware ESX, Citrix Xen, et al) allowed resources to be grouped into logical pools and shared across many “machines.” Now, once stranded resources were being made available to other machines, increasing the effectiveness of every dollar of investment in the IT infrastructure. Furthermore, the speed at which infrastructure could be provisioned was dramatically increased. No longer did equipment need to be ordered &amp;amp; installed, it can be simply carved out of an existing pool of resources and the pool can be expanded at a later date. What once took well over three months to complete, could now be done in less than one week. This ability to pool resources and carve them up into containers is the first key enabler of “The Cloud.”
The second key enabler is profiling and orchestration. The idea here is that an organization would profile their workloads and determine the attributes that are common between the applications that they run. Once common attributes are identified, their attributes are distilled down into proifiles and used to orchestrate provisioning of resources from the logical pools based on the attributes identified during profiling. This is what people are describing when they use the term, “The Cloud.” If you want to actually see what this looks like, you can look at the machine profiles for AWS, Google or Microsoft. Once complete, users can order IT resources like they order lunch and they will be available in a matter of minutes.
Problem Trading
I know what you’re thinking, “sounds awesome&amp;#8230;sign me up” and that’s why you feel queezy every time you hear “The Cloud;” it’s a wildly popular concept! Organizations are clamoring to increase the efficiency and agility of not only their IT org, but also their business. However, this model does have its drawbacks. This is what I meant when I used the term “problem trading;” trading efficiency and provisioning problems for the problems that consolidation brings (resource availability). As the consolidation ratio (efficiency) increases, which it does in Cloud environments, going “down the stack” (see below graphic) you have an increasingly difficult time ensuring that there are enough performance resources available for unexpected spikes. Oftentimes, this rears it’s ugly head at the storage level because it has the highest consolidation (greatest number of servers per storage array). Suddenly, a spike in the I/O workload for even 5% of your applications leads to everything on that storage array slowing down. This is the ugly side of the cloud; a Catch-22 situation between driving efficiency and resource contention. If you look at the enterprise storage array space, you will notice that there is another solid state disk based startup coming out of stealth mode every day; this is the new problem to solve. From capacity problems to performance problems.

I did take the time to explain the inception of the database market and all the benefits that it brought to business’. The reason for that is that databases tend to be the biggest talkers on any storage network and, also, the most sensitive to performance (latency) problems. Furthermore, databases underlie almost any application that people find useful and when they don’t perform, customer satisfaction decreases (either external or internal) . Would you be happy if it took 2 minutes between the time you hit “buy” for your Luggable Loo on Amazon (oh yes, a five gallon bucket can hold more than just joint compund) and the time you got your confirmation? I wouldn’t, and furthermore, Amazon would have never turned into the retail giant that they are today if that were the case.
What’s the Solution
Unfortunately, there’s is no single solution to the problem. An advanced IT organization will use a combination of headroom in resource pools, monitoring of performance trends, quality of service (QoS) and voodoo to fight off service interruption/degradation. However, at Tokutek, we believe that databases have an undeniable obligation to be better neighbors and that’s why we invented the Fractal Tree. It’s only natural for the software to evolve with the infrastructure. Like Ritalin to ADHD (Attention Deficit Hyperactivity Disorder), the Fractal Tree is the answer for the “storage hyperactive” database. As I quickly pointed out above, consolidation breeds contention and having many databases, who are firing thousands of IOPS (I/O operations, the basic unit of storage performance) at storage, becomes overwhelming and expensive. Think of it this way, Hitchcock references aside, being locked in a closet with a talkative crow would be annoying, but being locked in a closet with 100 would be unbearable. Making each crow 1/12th as loud would make the situation, while still challenging, much more manageable. That’s the value of the Fractal Tree, making the database the most efficient that it can be, all while offering accelerated performance. I understand that proof is in the putting, so I present the graphs at the bottom for your perusal. Simply put, 200M rows, or documents, are inserted into a database with iiBench running on AWS. The Fractal Tree inserts faster (first graph) and transfers less data (second graph) than traditional database indexing, all while consuming fewer resources. Distilled down, the Fractal Tree can accelerate workloads while making them a more thoughtful neighbor in highly consolidated environments.
 What if a mechanic told you that they could easily increase your car’s mileage per gallon 90% while increasing its performance? Your first thought would probably be skeptical, and rightfully so. But, what if he offered to let you try it with no obligation…


Jon is the Lead Sales Engineer &amp;amp; Evangelist at Tokutek
Follow him: www.linkedin.com/in/jonathanetobin
@jontobs
The post Increasing Cloud Database Efficiency &amp;#8211; Like Crows in a Closet appeared first on Tokutek.</description>
    <content:encoded><![CDATA[<p>In Mo’ Data, Mo’ Problems, we explored the paradox that “Big Data” projects pose to organizations and how Tokutek is taking an innovative approach to solving those problems. In this post, we’re going to talk about another hot topic in IT, “The Cloud,” and how enterprises undertaking Cloud efforts often struggle with idea of “problem trading.” Also, for some reason, databases are just given a pass as traditionally “noisy neighbors” and that there is nothing that can be done about it. Lets take a look at why we disagree.</p>
<p>With the birth of the information age came a coupling of business and IT. Increasingly strategic business projects and objectives were reliant on information infrastructure to provide information storage and retrieval instead of paper and filing cabinets. This was the dawn of the database and what gave rise to companies like Oracle, Sybase and MySQL. With the appearance of true Enterprise Grade databases, companies now had the unprecedented ability to store huge volumes of information and parse it at speeds that were unthinkable even five years prior. This dramatically increased the quality and speed at which business decisions were made. However, reliance on IT came with a downside, the planning, design, procurement and implementation process often comprised a majority of the overall project timeline, often pushing out completion by, at least, 3 months and up to 24. Furthermore, the projectizing of IT resources was quickly leading to islands of compute, network and storage resources. These projectized islands were reducing the dollar per dollar effectiveness of IT, which is commonly a cost center, and making it the target of every CFO and CIO looking to boost their bottom line.</p>
<p>The “resource island” problem was the first one that innovative companies started solving. Technologies like Mainframe computing, Storage Area Networks (SAN) and, later, hardware abstracted virtualization (VMware ESX, Citrix Xen, et al) allowed resources to be grouped into logical pools and shared across many “machines.” Now, once stranded resources were being made available to other machines, increasing the effectiveness of every dollar of investment in the IT infrastructure. Furthermore, the speed at which infrastructure could be provisioned was dramatically increased. No longer did equipment need to be ordered &amp; installed, it can be simply carved out of an existing pool of resources and the pool can be expanded at a later date. What once took well over three months to complete, could now be done in less than one week. This ability to pool resources and carve them up into containers is the first key enabler of “The Cloud.”</p>
<p>The second key enabler is profiling and orchestration. The idea here is that an organization would profile their workloads and determine the attributes that are common between the applications that they run. Once common attributes are identified, their attributes are distilled down into proifiles and used to orchestrate provisioning of resources from the logical pools based on the attributes identified during profiling. This is what people are describing when they use the term, “The Cloud.” If you want to actually see what this looks like, you can look at the machine profiles for <a href="http://aws.amazon.com/ec2/instance-types/">AWS</a>, <a href="https://cloud.google.com/compute/pricing">Google</a> or <a href="http://azure.microsoft.com/en-us/pricing/details/virtual-machines/#Linux">Microsoft</a>. Once complete, users can order IT resources like they order lunch and they will be available in a matter of minutes.</p>
<p><b>Problem Trading</b></p>
<p>I know what you’re thinking, “sounds awesome&#8230;sign me up” and that’s why you feel queezy every time you hear “The Cloud;” it’s a wildly popular concept! Organizations are clamoring to increase the efficiency and agility of not only their IT org, but also their business. However, this model does have its drawbacks. This is what I meant when I used the term “problem trading;” trading efficiency and provisioning problems for the problems that consolidation brings (resource availability). As the consolidation ratio (efficiency) increases, which it does in Cloud environments, going “down the stack” (see below graphic) you have an increasingly difficult time ensuring that there are enough performance resources available for unexpected spikes. Oftentimes, this rears it’s ugly head at the storage level because it has the highest consolidation (greatest number of servers per storage array). Suddenly, a spike in the I/O workload for even 5% of your applications leads to everything on that storage array slowing down. This is the ugly side of the cloud; a Catch-22 situation between driving efficiency and resource contention. If you look at the enterprise storage array space, you will notice that there is another solid state disk based startup coming out of stealth mode every day; this is the new problem to solve. From capacity problems to performance problems.</p>
<p><strong><strong><a href="http://www.tokutek.com/wp-content/uploads/2015/02/Consolidation-Pyramid-1.png"><img class="alignnone size-full wp-image-9007 aligncenter" src="http://www.tokutek.com/wp-content/uploads/2015/02/Consolidation-Pyramid-1.png" alt="Consolidation Pyramid (1)" width="542" height="287" /></a></strong></strong></p>
<p>I did take the time to explain the inception of the database market and all the benefits that it brought to business’. The reason for that is that databases tend to be the biggest talkers on any storage network and, also, the most sensitive to performance (latency) problems. Furthermore, databases underlie almost any application that people find useful and when they don’t perform, customer satisfaction decreases (either external or internal) . Would you be happy if it took 2 minutes between the time you hit “buy” for your <a href="http://www.amazon.com/Reliance-Products-Luggable-Portable-Gallon/dp/B000FIAPXO/ref=cm_lmf_tit_33">Luggable Loo on Amazon</a> (oh yes, a five gallon bucket can hold more than just joint compund) and the time you got your confirmation? I wouldn’t, and furthermore, Amazon would have never turned into the retail giant that they are today if that were the case.</p>
<p><b>What’s the Solution</b></p>
<p>Unfortunately, there’s is no single solution to the problem. An advanced IT organization will use a combination of headroom in resource pools, monitoring of performance trends, quality of service (QoS) and voodoo to fight off service interruption/degradation. However, at Tokutek, we believe that databases have an undeniable obligation to be better neighbors and that’s why we invented the Fractal Tree. It’s only natural for the software to evolve with the infrastructure. Like Ritalin to ADHD (Attention Deficit Hyperactivity Disorder), the Fractal Tree is the answer for the “storage hyperactive” database. As I quickly pointed out above, consolidation breeds contention and having many databases, who are firing thousands of IOPS (I/O operations, the basic unit of storage performance) at storage, becomes overwhelming and expensive. Think of it this way, Hitchcock references aside, being locked in a closet with a talkative crow would be annoying, but being locked in a closet with 100 would be unbearable. Making each crow 1/12th as loud would make the situation, while still challenging, much more manageable. That’s the value of the Fractal Tree, making the database the most efficient that it can be, all while offering accelerated performance. I understand that proof is in the putting, so I present the graphs at the bottom for your perusal. Simply put, 200M rows, or documents, are inserted into a database with <a href="https://github.com/tmcallaghan?tab=repositories">iiBench</a> running on AWS. The Fractal Tree inserts faster (first graph) and transfers less data (second graph) than traditional database indexing, all while consuming fewer resources. Distilled down, the Fractal Tree can accelerate workloads while making them a more thoughtful neighbor in highly consolidated environments.</p>
<p><strong><strong> </strong></strong>What if a mechanic told you that they could easily increase your car’s mileage per gallon 90% while increasing its performance? Your first thought would probably be skeptical, and rightfully so. But, what if he offered to let you try it with no obligation…</p>
<p><a href="http://www.tokutek.com/wp-content/uploads/2015/02/iibench_insert_speed_AWS.png"><img class="alignnone size-full wp-image-9006" src="http://www.tokutek.com/wp-content/uploads/2015/02/iibench_insert_speed_AWS.png" alt="iibench_insert_speed_AWS" width="651" height="373" /></a></p>
<p><a href="http://www.tokutek.com/wp-content/uploads/2015/02/iibench_data_transferred_AWS.png"><img class="alignnone size-full wp-image-9008" src="http://www.tokutek.com/wp-content/uploads/2015/02/iibench_data_transferred_AWS.png" alt="iibench_data_transferred_AWS" width="651" height="374" /></a></p>
<p>Jon is the Lead Sales Engineer &amp; Evangelist at Tokutek<br />
Follow him: <a href="http://www.tokutek.com/www.linkedin.com/in/jonathanetobin"><span>www.linkedin.com/in/</span></a><span><a href="http://www.tokutek.com/www.linkedin.com/in/jonathanetobin">jonathanetobin<br />
</a></span>@jontobs</p>
<p>The post <a rel="nofollow" href="http://www.tokutek.com/2015/02/increasing-cloud-database-efficiency-like-crows-closet/">Increasing Cloud Database Efficiency &#8211; Like Crows in a Closet</a> appeared first on <a rel="nofollow" href="http://www.tokutek.com">Tokutek</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989051&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989051&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 17:24:15 +0000</pubDate>
    <dc:creator>Tokuview Blog</dc:creator>
    <category>TokuView</category>
    <category>aws</category>
    <category>big data</category>
    <category>cloud</category>
    <category>cloud performance</category>
    <category>fractal tree</category>
    <category>MongoDB</category>
    <category>mongodb performance</category>
    <category>mysql</category>
    <category>mysql performance</category>
    <category>Oracle</category>
    <category>performance</category>
    <category>TokuDB</category>
    <category>tokumx</category>
    <category>Tokutek</category>
  </item>

  <item>
    <title>Performance Impact of InnoDB Transaction Isolation Modes in MySQL 5.7</title>
    <guid isPermaLink="false">http://mysqlserverteam.com/?p=3036</guid>
    <link>http://mysqlserverteam.com/performance-impact-of-innodb-transaction-isolation-modes-in-mysql-5-7/</link>
    <description>During the process of reviewing our server defaults for MySQL 5.7, we thought that it might be better to change the default transaction isolation level from REPEATABLE-READ to READ-COMMITTED (the default for PostgreSQL, Oracle, and SQL Server). After some benchmarking, however, it seems that we should stick with REPEATABLE-READ as the default for now.
It&amp;#8217;s very easy to modify the default isolation level, however, and it can even be done at the SESSION level. For the most optimal performance you can change the transaction isolation level dynamically in your SESSION according the situation:

For short running queries and transactions, use the default level of REPEATABLE-READ.
For long running queries and transactions, use the level of READ-COMMITTED.

You can find the full details of our recent benchmarking efforts on this topic here: http://dimitrik.free.fr/blog/archives/2015/02/mysql-performance-impact-of-innodb-transaction-isolation-modes-in-mysql-57.html
As always, THANK YOU for using MySQL!</description>
    <content:encoded><![CDATA[<p>During the process of <a href="http://www.tocker.ca/2015/01/05/what-defaults-would-you-like-to-see-changed-in-mysql-5-7.html" target="_blank">reviewing our server defaults for MySQL 5.7</a>, we thought that it might be better to change the default <a href="http://dev.mysql.com/doc/refman/5.7/en/set-transaction.html" target="_blank">transaction isolation level</a> from <code>REPEATABLE-READ</code> to <code>READ-COMMITTED</code> (the default for PostgreSQL, Oracle, and SQL Server). After some benchmarking, however, it seems that we should stick with REPEATABLE-READ as the default for now.</p>
<p>It&#8217;s very easy to <a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_tx_isolation" target="_blank">modify the default isolation level</a>, however, and it can even be done at the <code>SESSION</code> level. For the most optimal performance you can change the transaction isolation level dynamically in your SESSION according the situation:</p>
<ul>
<li>For short running queries and transactions, use the default level of <code>REPEATABLE-READ</code>.</li>
<li>For long running queries and transactions, use the level of <code>READ-COMMITTED</code>.</li>
</ul>
<p>You can find the full details of our recent benchmarking efforts on this topic here: <a href="http://dimitrik.free.fr/blog/archives/2015/02/mysql-performance-impact-of-innodb-transaction-isolation-modes-in-mysql-57.html" target="_blank">http://dimitrik.free.fr/blog/archives/2015/02/mysql-performance-impact-of-innodb-transaction-isolation-modes-in-mysql-57.html</a></p>
<p>As always, <strong>THANK YOU</strong> for using MySQL!</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989041&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989041&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 13:38:57 +0000</pubDate>
    <dc:creator>Dimitri Kravtchuk</dc:creator>
    <category>Benchmarking</category>
    <category>InnoDB</category>
    <category>Internals</category>
    <category>MySQL</category>
    <category>Performance</category>
  </item>

  <item>
    <title>Using MySQL Event Scheduler and how to prevent contention</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28431</guid>
    <link>http://www.percona.com/blog/2015/02/25/using-mysql-event-scheduler-and-how-to-prevent-contention/</link>
    <description>MySQL introduced the Event Scheduler in version 5.1.6. The Event Scheduler is a MySQL-level &amp;#8220;cron job&amp;#8221;, which will run events inside MySQL. Up until now, this was not a very popular feature, however, it has gotten more popular since the adoption of Amazon RDS &amp;#8211; as well as similar MySQL database as a service offerings where there is no OS level.What is important to understand about the Event Scheduler is that it does not have any protection against multiple execution (neither does linux cron). Let&amp;#8217;s imagine you have created an event that executes every 10 seconds, but the logic inside the event (i.e. queries or stored procedure call) can take longer than 10 seconds (may be in case of the high load), so it can pile-up. In the worst case, when an event contains a set of &amp;#8220;insert&amp;#8221; + &amp;#8220;update&amp;#8221;/&amp;#8221;delete&amp;#8221; statement inside a transaction, it can cause a deadlock.Adding &amp;#8220;get_lock&amp;#8221; conditions inside of the event will help to prevent such situation:If a repeating event does not terminate within its scheduling interval, the result may be multiple instances of the event executing simultaneously. If this is undesirable, you should institute a mechanism to prevent simultaneous instances. For example, you could use the GET_LOCK() function, or row or table locking. Read more at event_scheduler documentation.Function GET_LOCK() can be used for communications between threads:The following example can illustrate using get_lock:DELIMITER //
CREATE EVENT testlock_event ON SCHEDULE EVERY 2 SECOND DO
BEGIN
 DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
 BEGIN
   DO RELEASE_LOCK('testlock_event');
 END;
 IF GET_LOCK('testlock_event', 0) THEN
   -- add some business logic here, for example:
   -- insert into test.testlock_event values(NULL, NOW());
  END IF;
  DO RELEASE_LOCK('testlock_event');
END;
//
DELIMITER ;DECLARE CONTINUE HANDLER FOR SQLEXCEPTION is needed here to release lock even if the event failed or was killed.The above GET_LOCK / RELEASE_LOCK combination will help to prevent contention inside the MySQL Event Scheduler.The post Using MySQL Event Scheduler and how to prevent contention appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p>MySQL introduced the <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.1/en/events.html">Event Scheduler</a> in version 5.1.6. The Event Scheduler is a MySQL-level <a rel="nofollow" href="http://en.wikipedia.org/wiki/Cron">&#8220;cron job&#8221;</a>, which will run events inside MySQL. Up until now, this was not a very popular feature, however, it has gotten more popular since the adoption of <a href="http://www.percona.com/blog/2014/07/28/what-i-learned-while-migrating-a-customer-mysql-installation-to-amazon-rds/">Amazon RDS</a> &#8211; as well as similar MySQL database as a service offerings where there is no OS level.</p><p>What is important to understand about the Event Scheduler is that it does not have any protection against multiple execution (neither does linux cron). Let&#8217;s imagine you have created an event that executes every 10 seconds, but the logic inside the event (i.e. queries or stored procedure call) can take longer than 10 seconds (may be in case of the high load), so it can pile-up. In the worst case, when an event contains a set of &#8220;insert&#8221; + &#8220;update&#8221;/&#8221;delete&#8221; statement inside a transaction, it can cause a deadlock.</p><p>Adding &#8220;get_lock&#8221; conditions inside of the event will help to prevent such situation:</p><blockquote><p>If a repeating event does not terminate within its scheduling interval, the result may be multiple instances of the event executing simultaneously. If this is undesirable, you should institute a mechanism to prevent simultaneous instances. For example, you could use the GET_LOCK() function, or row or table locking. Read more at <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/create-event.html">event_scheduler documentation</a>.</p></blockquote><p>Function <a rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/miscellaneous-functions.html#function_get-lock">GET_LOCK()</a> can be used for communications between threads:</p><p>The following example can illustrate using get_lock:</p><pre>DELIMITER //
CREATE EVENT testlock_event ON SCHEDULE EVERY 2 SECOND DO
BEGIN
 DECLARE CONTINUE HANDLER FOR SQLEXCEPTION
 BEGIN
   DO RELEASE_LOCK('testlock_event');
 END;
 IF GET_LOCK('testlock_event', 0) THEN
   -- add some business logic here, for example:
   -- insert into test.testlock_event values(NULL, NOW());
  END IF;
  DO RELEASE_LOCK('testlock_event');
END;
//
DELIMITER ;</pre><p>DECLARE CONTINUE HANDLER FOR SQLEXCEPTION is needed here to release lock even if the event failed or was killed.</p><p>The above GET_LOCK / RELEASE_LOCK combination will help to prevent contention inside the MySQL Event Scheduler.</p><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/02/25/using-mysql-event-scheduler-and-how-to-prevent-contention/">Using MySQL Event Scheduler and how to prevent contention</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989040&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989040&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 11:00:54 +0000</pubDate>
    <dc:creator>Alexander Rubin</dc:creator>
    <category>Insight for DBAs</category>
    <category>MySQL</category>
    <category>Alexander Rubin</category>
    <category>Amazon RDS</category>
    <category>DBaaS</category>
    <category>Event Scheduler</category>
    <category>Primary</category>
  </item>

  <item>
    <title>Introducing Query and Metric Pages</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/02/25/introducing-query-and-metric-pages/</guid>
    <link>https://vividcortex.com/blog/2015/02/25/introducing-query-and-metric-pages/</link>
    <description>We recently deployed multiple new pages at VividCortex providing detailed query and metric information. To access this level of information previously, you had to utilize Top Queries, one of our most popular tools. While the power of Top Queries lies in the ability to rank queries by specific criteria, these new pages allow users to easily filter and see detailed information about any query or metric.

VividCortex provides industry-leading, one-second granularity to provide unparallelled  visibility into the work being done on your database systems. We track thousands of metrics from the database and operating system, providing you with actionable analysis through our innovative tools. When you deal with this much information, simplicity is a priority. Both of these new tools are simple to use but provide a tremendous amount of data.

Selecting Queries or Metrics in the left-hand navigation menu takes you to a table of, you guessed it, queries or metrics. From this page, you can filter results by text, age, or recent activity. Selecting a row takes you to that row’s detail page where relevant information such as notifications, samples, and per-host breakdown can be easily seen and shared. 



Two other features are worth noting. First, these pages are (or will be shortly) integrated to easily allow access from other pages within the app. While this is a small implementation detail, it’s extremely powerful. For example, if an event occurs related to a specific query or metric, we automatically link to the relevant detail page, providing users quick access to more information. 

Second is “The Explorer,” which is found within the Metrics detail page. As stated previously, VividCortex retrieves thousands of metrics. The Explorer uses algorithms to cluster metrics that are similarly shaped, so that when you find a metric that is of interest, you can easily find out more information through the shape of other metrics. 

We’re excited about these new pages, and as always, thank our customers for their feedback through the development. If you’re not using VividCortex yet, sign up for free trial (no credit card required). We believe you’ll learn something about your system that you didn’t know.</description>
    <content:encoded><![CDATA[<p>We recently deployed multiple new pages at VividCortex providing detailed query and metric information. To access this level of information previously, you had to utilize Top Queries, one of our most popular tools. While the power of Top Queries lies in the ability to rank queries by specific criteria, these new pages allow users to easily filter and see detailed information about <strong>any</strong> query or metric.</p>

<p>VividCortex provides industry-leading, one-second granularity to provide unparallelled  visibility into the work being done on your database systems. We track thousands of metrics from the database and operating system, providing you with actionable analysis through our innovative tools. When you deal with this much information, simplicity is a priority. Both of these new tools are simple to use but provide a tremendous amount of data.</p>

<p>Selecting <strong>Queries</strong> or <strong>Metrics</strong> in the left-hand navigation menu takes you to a table of, you guessed it, queries or metrics. From this page, you can filter results by text, age, or recent activity. Selecting a row takes you to that row’s detail page where relevant information such as notifications, samples, and per-host breakdown can be easily seen and shared. </p>

<p><img src="https://vividcortex.com/img/articles/2015/02/metrics.png" alt="Queries and Metrics" /></p>

<p>Two other features are worth noting. First, these pages are (or will be shortly) integrated to easily allow access from other pages within the app. While this is a small implementation detail, it’s extremely powerful. For example, if an event occurs related to a specific query or metric, we automatically link to the relevant detail page, providing users quick access to more information. </p>

<p>Second is “The Explorer,” which is found within the Metrics detail page. As stated previously, VividCortex retrieves thousands of metrics. The Explorer uses algorithms to cluster metrics that are similarly shaped, so that when you find a metric that is of interest, you can easily find out more information through the shape of other metrics. </p>

<p>We’re excited about these new pages, and as always, thank our customers for their feedback through the development. If you’re not using VividCortex yet, <a href="https://app.vividcortex.com/sign-up?utm_source=organic&amp;utm_medium=blog">sign up for free trial</a> (no credit card required). We believe you’ll learn something about your system that you didn’t know.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989050&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989050&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Wed, 25 Feb 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Proposal to change additional defaults in MySQL 5.7 (February Edition)</title>
    <guid isPermaLink="false">http://www.tocker.ca/?p=581</guid>
    <link>http://www.tocker.ca/2015/02/24/proposal-to-change-additional-defaults-in-mysql-5-7-february-edition.html</link>
    <description>Following on from my two earlier posts, in the MySQL team we are proposing a new set of changes to defaults for MySQL 5.7:


Setting
Old Default
New Default


log_slow_admin_statements
OFF
ON


log_slow_slave_statements
OFF
ON


long-query-time
10
2


log-queries-not-using-indexes
OFF
ON


min-examined-row-limit
0
1000


MySQL Command Line Client
-
+show_warnings


group_concat_max_len
1024
1M


max_allowed_packet
4M
64M


symbolic-links
-
OFF


temp-pool
1
0


table_open_cache_instances
1
16


To explain these changes in more details:

The slow query log remains disabled by default, but when enabled our hope is that we can make it immediately useful.  A query exceeding 2 seconds will now be considered slow, and since queries that do not use indexes have the highest chance of causing future problems, they will also be logged unless they examine fewer than 1000 rows.  The number of 1000 was chosen because it should eliminate the majority of false positives from queries retrieving a static set of records such as navigation menu items, which are well suited to tablescans.
In proposing this change, we considered the defaults that other products use for considering a query slow.  A popular web performance tool uses a default of 0.5 seconds for an acceptable time to render a full page server side, with 2 seconds (4x) slow enough to warrant logging. Our own MySQL Enterprise Monitor considers 100ms acceptable, and 400ms (4x) as slow.  We also considered that setting the min-examined-row-limit to a non zero value will now require an additional step for those that set their long-query-time to zero seconds. We would like to thank Daniel Black for suggesting we change our slow query log options.

The MySQL command line client has the ability to automatically print warnings out to screen as they occur.  We feel like ON is the more useful default, as a user will typically not intend to execute statements that cause warnings.  This behavior applies to both interactive and batch modes, and can be disabled with show_warnings=0.  We would like to thank Ryuta Kamizono for this suggestion.
We find the GROUP_CONCAT() function in MySQL to be incredibly useful for summarizing results in an aggregate query.  We also believe that the default maximum length for grouping values has not kept pace with the larger amounts of memory available on modern hardware.  We would like to thank Shlomi Noach for this suggestion.
In MySQL 5.6 we increased the max_allowed_packet from 1M to a conservative 4M.  We have received a lot of feedback on this change, and are proposing to increase the default to 64M for MySQL 5.7.  We would like to thank Shlomi Noach for this suggestion.
Many of our configuration files for MySQL packages (as well as those that ship with Linux distributions) default to disabling symbolic links for security reasons.  We feel that changing a de facto default to a compiled default will improve user experience (as well as security).  We would like to thank Honza Horak for this suggestion.
The temp-pool option was originally created as a means to work around potential filesystem issues on Linux when using internal MyISAM based temp tables. The underlying OS issues have since been resolved, and disabling this option removes mutex contention for all disk based temp table engines (MyISAM and InnoDB).
We are proposing to increase the default table_open_cache_instances to 16 in order to reduce contention on internal mutexes when opening tables.

For those of you who would like to test out these changes (along with previous changes proposed), I have sample configuration files available:

For MySQL 5.7
For MySQL 5.6 (enables all 5.7 options where applicable)

Please let us know what you think of these changes!
You can leave a comment here, or get in touch via email.</description>
    <content:encoded><![CDATA[<p>Following on from my two <a href="http://www.tocker.ca/2015/01/23/proposal-to-change-additional-defaults-in-mysql-5-7.html">earlier</a> <a href="http://www.tocker.ca/2015/01/14/proposal-to-change-replication-and-innodb-settings-in-mysql-5-7.html">posts</a>, in the MySQL team we are proposing a new set of changes to defaults for MySQL 5.7:</p>
<table>
<tr>
<td><strong>Setting</strong></td>
<td><strong>Old Default</strong></td>
<td><strong>New Default</strong></td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_log_slow_admin_statements">log_slow_admin_statements</a></td>
<td>OFF</td>
<td>ON</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/replication-options-slave.html#sysvar_log_slow_slave_statements">log_slow_slave_statements</a></td>
<td>OFF</td>
<td>ON</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_long_query_time">long-query-time</a></td>
<td>10</td>
<td>2</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_log_queries_not_using_indexes">log-queries-not-using-indexes</a></td>
<td>OFF</td>
<td>ON</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_min-examined-row-limit">min-examined-row-limit</a></td>
<td>0</td>
<td>1000</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/mysql-command-options.html#option_mysql_show-warnings">MySQL Command Line Client</a></td>
<td>-</td>
<td>+show_warnings</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_group_concat_max_len">group_concat_max_len</a></td>
<td>1024</td>
<td>1M</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_max_allowed_packet">max_allowed_packet</a></td>
<td>4M</td>
<td>64M</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_symbolic-links">symbolic-links</a></td>
<td>-</td>
<td>OFF</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-options.html#option_mysqld_temp-pool">temp-pool</a></td>
<td>1</td>
<td>0</td>
</tr>
<tr>
<td><a href="http://dev.mysql.com/doc/refman/5.7/en/server-system-variables.html#sysvar_table_open_cache_instances">table_open_cache_instances</a></td>
<td>1</td>
<td>16</td>
</tr>
</table>
<p>To explain these changes in more details:</p>
<ul>
<li>The slow query log remains disabled by default, but when enabled our hope is that we can make it immediately useful.  A query exceeding 2 seconds will now be considered slow, and since queries that do not use indexes have the highest chance of causing future problems, they will also be logged unless they examine fewer than 1000 rows.  The number of 1000 was chosen because it should eliminate the majority of false positives from queries retrieving a static set of records such as navigation menu items, which are well suited to tablescans.
<p>In proposing this change, we considered the defaults that other products use for considering a query slow.  A popular web performance tool uses a default of 0.5 seconds for an acceptable time to render a full page server side, with 2 seconds (4x) slow enough to warrant logging. Our own MySQL Enterprise Monitor considers 100ms acceptable, and 400ms (4x) as slow.  We also considered that setting the <tt>min-examined-row-limit</tt> to a non zero value will now require an additional step for those that set their <a href="https://www.box.com/blog/the-slow-query-log-is-not/">long-query-time to zero seconds</a>. We would like to thank Daniel Black for suggesting we change our slow query log options.
</li>
<li>The MySQL command line client has the ability to automatically print warnings out to screen as they occur.  We feel like ON is the more useful default, as a user will typically not intend to execute statements that cause warnings.  This behavior applies to both interactive and batch modes, and can be disabled with <tt>show_warnings=0</tt>.  We would like to thank Ryuta Kamizono for this suggestion.</li>
<li>We find the <tt>GROUP_CONCAT()</tt> function in MySQL to be incredibly useful for summarizing results in an aggregate query.  We also believe that the default maximum length for grouping values has not kept pace with the larger amounts of memory available on modern hardware.  We would like to thank Shlomi Noach for this suggestion.</li>
<li>In MySQL 5.6 we increased the <tt>max_allowed_packet</tt> from 1M to a conservative 4M.  We have received a lot of feedback on this change, and are proposing to increase the default to 64M for MySQL 5.7.  We would like to thank Shlomi Noach for this suggestion.</li>
<li>Many of our configuration files for MySQL packages (as well as those that ship with Linux distributions) default to disabling symbolic links for security reasons.  We feel that changing a de facto default to a compiled default will improve user experience (as well as security).  We would like to thank Honza Horak for this suggestion.</li>
<li>The temp-pool option was originally created as a means to work around potential filesystem issues on Linux when using internal MyISAM based temp tables. The underlying OS issues have since been resolved, and disabling this option removes mutex contention for all disk based temp table engines (MyISAM and InnoDB).</li>
<li>We are proposing to increase the default <tt>table_open_cache_instances</tt> to 16 in order to reduce contention on internal mutexes when opening tables.</li>
</ul>
<p>For those of you who would like to test out these changes (along with previous changes proposed), I have sample configuration files available:</p>
<ul>
<li><a href="https://github.com/morgo/mysql-compatibility-config/blob/master/mysql-57/mysql-57-proposed.cnf">For MySQL 5.7</a></li>
<li><a href="https://github.com/morgo/mysql-compatibility-config/blob/master/mysql-56/mysql-57-proposed.cnf">For MySQL 5.6</a> (enables all 5.7 options where applicable)</li>
</ul>
<p>Please let us know what you think of these changes!<br />
You can leave a comment here, or <a href="http://www.tocker.ca/contact/">get in touch</a> via email.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989035&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989035&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 24 Feb 2015 19:35:07 +0000</pubDate>
    <dc:creator>Morgan Tocker</dc:creator>
    <category>Community</category>
    <category>Configuration</category>
  </item>

  <item>
    <title>Regarding MySQL 5.6 temporary tables format</title>
    <guid isPermaLink="false">http://dbahire.com/?p=644</guid>
    <link>http://dbahire.com/regarding-mysql-5-6-temporary-tables-format/</link>
    <description>default_tmp_storage_engine variable was introduced in 5.6.3, allowing the configuration of the default engine for temporary tables. This seems to be in the direction, as I commented before, of making MyISAM an optional engine. In 5.7, a separate tablespace is being created to hold those tables in order to reduce its performance penalty (those tables do not need to be redone if the server crashes, so extra writes are avoided).
However, I have seen many people assuming that because default_tmp_storage_engine has the value &amp;#8220;InnoDB&amp;#8221;, all temporary tables are created in InnoDB format in 5.6. This is not true: first, because implicit temporary tables are still being created in memory using the MEMORY engine (sometimes called the HEAP engine), while MyISAM is being used for on-disk tables. If you do not trust the reference manual on this, here it is a quick test to check it:
mysql&gt; SELECT version();
+------------+
| version()  |
+------------+
| 5.6.23-log |
+------------+
1 row in set (0.00 sec)
mysql&gt; SHOW GLOBAL VARIABLES like 'default%';
+----------------------------+--------+
| Variable_name              | Value  |
+----------------------------+--------+
| default_storage_engine     | InnoDB |
| default_tmp_storage_engine | InnoDB |
| default_week_format        | 0      |
+----------------------------+--------+
3 rows in set (0.00 sec)
mysql&gt; SHOW GLOBAL VARIABLES like 'tmpdir';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| tmpdir        | /tmp  |
+---------------+-------+
1 row in set (0.00 sec)
mysql&gt; CREATE TABLE test (id serial, a text);
Query OK, 0 rows affected (0.10 sec)
mysql&gt; insert into test (a) values ('a');
Query OK, 1 row affected (0.06 sec)
mysql&gt; insert into test (a) values ('aa');
Query OK, 1 row affected (0.00 sec)
mysql&gt; insert into test (a) values ('aaa');
Query OK, 1 row affected (0.00 sec)
mysql&gt; SELECT *, sleep(10) FROM test ORDER BY rand();
...
[ec2-user@jynus_com tmp]$ ls -la
total 24
drwxrwxrwt  5 root     root     4096 Feb 24 11:55 .
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix
-rw-rw----  1 mysql    mysql       0 Feb 24 11:55 #sql_7bbd_0.MYD
-rw-rw----  1 mysql    mysql    1024 Feb 24 11:55 #sql_7bbd_0.MYI
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo
...
+----+------+-----------+
| id | a    | sleep(10) |
+----+------+-----------+
|  1 | a    |         0 |
|  2 | aa   |         0 |
|  3 | aaa  |         0 |
+----+------+-----------+
3 rows in set (30.00 sec)

The only thing I have done above is forcing the creation of the temporary table on disk by adding a TEXT field (incompatible with the MEMORY engine, so it has to be created on disk) and using sleep so that we have enough time to check the filesystem. You can see on the output of ls the .MYD and .MYI particular to the MyISAM engine. That last step would be unnecessary if we just used PERFORMANCE_SCHEMA to check the waits/io.
A second, and more obvious reason why thinking that all temporary tables are created in InnoDB format, is because explicit temporary tables can still be created in a different engine with the ENGINE keyword:
mysql&gt; CREATE TEMPORARY TABLE test (i serial) ENGINE=MyISAM;
Query OK, 0 rows affected (0.00 sec)
[ec2-user@jynus_com tmp]$ ls -la
total 36
drwxrwxrwt  5 root     root     4096 Feb 24 12:16 .
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix
-rw-rw----  1 mysql    mysql    8554 Feb 24 12:12 #sql7bbd_36a3_0.frm
-rw-rw----  1 mysql    mysql       0 Feb 24 12:12 #sql7bbd_36a3_0.MYD
-rw-rw----  1 mysql    mysql    1024 Feb 24 12:12 #sql7bbd_36a3_0.MYI
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo
mysql&gt; DROP TEMPORARY TABLE test;
Query OK, 0 rows affected (0.00 sec)
[ec2-user@jynus_com tmp]$ ls -la
total 20
drwxrwxrwt  5 root     root     4096 Feb 24 12:17 .
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo

Will this change in the future? 5.7.5 continues to have the same behavior as 5.6. However, as Stewart pointed some time ago, the performance optimizations in 5.7 make some uses of MEMORY and MyISAM obsolete so I will not be surprised if that dependency, together with MyISAM grant tables, will be removed in the future.
Update: I&amp;#8217;ve been told by email by Morgan that the yet-to-be-released (at the time of this writing) 5.7.6 will finally change the default behavior to be full InnoDB for implicit temporary tables, too, as seen on the release notes:
InnoDB: The default setting for the internal_tmp_disk_storage_engine option, which defines the storage engine the server uses for on-disk internal temporary tables (see How MySQL Uses Internal Temporary Tables), is now INNODB. With this change, the Optimizer uses the InnoDB storage engine instead of MyISAM for internal temporary tables.
internal_tmp_disk_storage_engine was introduced in 5.7.5, but its default value then was MYISAM.
This is in order to get advantage of the in-memory performance of InnoDB for variable-lengh fields, which I am personally 100% for. Thank you Morgan for the extra information!</description>
    <content:encoded><![CDATA[<p><img src="http://dbahire.com/wp-content/uploads/2015/02/temporary-table.png" alt="A temporary table" title="This is clearly a metaphor for a MySQL temporary table and not a SEO trick" width="280" height="300" class="alignright size-full wp-image-645" /><code>default_tmp_storage_engine</code> variable was introduced in 5.6.3, allowing the configuration of the default engine for temporary tables. This seems to be in the direction, as I commented before, of <a href="http://dbahire.com/today-is-the-day-in-which-myisam-is-no-longer-needed/" title="Today is the day in which MyISAM is no longer needed">making MyISAM an optional engine</a>. In 5.7, <a href="https://blogs.oracle.com/mysqlinnodb/entry/https_blogs_oracle_com_mysqlinnodb">a separate tablespace is being created to hold those tables in order to reduce its performance penalty</a> (those tables do not need to be redone if the server crashes, so extra writes are avoided).</p>
<p>However, I have seen many people assuming that because <code>default_tmp_storage_engine</code> has the value &#8220;InnoDB&#8221;, all temporary tables are created in InnoDB format <em>in 5.6</em>. This is not true: first, because <strong>implicit temporary tables are still being created in memory using the <code>MEMORY</code> engine (sometimes called the <code>HEAP</code> engine), while MyISAM is being used for on-disk tables</strong>. If you do not trust the reference manual on this, here it is a quick test to check it:</p>
<p><code>mysql> SELECT version();<br />
+------------+<br />
| version()  |<br />
+------------+<br />
| 5.6.23-log |<br />
+------------+<br />
1 row in set (0.00 sec)</p>
<p>mysql> SHOW GLOBAL VARIABLES like 'default%';<br />
+----------------------------+--------+<br />
| Variable_name              | Value  |<br />
+----------------------------+--------+<br />
| default_storage_engine     | InnoDB |<br />
| default_tmp_storage_engine | InnoDB |<br />
| default_week_format        | 0      |<br />
+----------------------------+--------+<br />
3 rows in set (0.00 sec)</p>
<p>mysql> SHOW GLOBAL VARIABLES like 'tmpdir';<br />
+---------------+-------+<br />
| Variable_name | Value |<br />
+---------------+-------+<br />
| tmpdir        | /tmp  |<br />
+---------------+-------+<br />
1 row in set (0.00 sec)</p>
<p>mysql> CREATE TABLE test (id serial, a text);<br />
Query OK, 0 rows affected (0.10 sec)</p>
<p>mysql> insert into test (a) values ('a');<br />
Query OK, 1 row affected (0.06 sec)</p>
<p>mysql> insert into test (a) values ('aa');<br />
Query OK, 1 row affected (0.00 sec)</p>
<p>mysql> insert into test (a) values ('aaa');<br />
Query OK, 1 row affected (0.00 sec)</p>
<p>mysql> SELECT *, sleep(10) FROM test ORDER BY rand();<br />
...</p>
<p>[ec2-user@jynus_com tmp]$ ls -la<br />
total 24<br />
drwxrwxrwt  5 root     root     4096 Feb 24 11:55 .<br />
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..<br />
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix<br />
-rw-rw----  1 mysql    mysql       0 Feb 24 11:55 #sql_7bbd_0.MYD<br />
-rw-rw----  1 mysql    mysql    1024 Feb 24 11:55 #sql_7bbd_0.MYI<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo</p>
<p>...<br />
+----+------+-----------+<br />
| id | a    | sleep(10) |<br />
+----+------+-----------+<br />
|  1 | a    |         0 |<br />
|  2 | aa   |         0 |<br />
|  3 | aaa  |         0 |<br />
+----+------+-----------+<br />
3 rows in set (30.00 sec)<br />
</code></p>
<p>The only thing I have done above is forcing the creation of the temporary table on disk by adding a TEXT field (incompatible with the MEMORY engine, so it has to be created on disk) and using sleep so that we have enough time to check the filesystem. You can see on the output of <code>ls</code> the .MYD and .MYI particular to the MyISAM engine. That last step would be unnecessary if we just used <code>PERFORMANCE_SCHEMA</code> to check the <code>waits/io</code>.</p>
<p>A second, and more obvious reason why thinking that all temporary tables are created in InnoDB format, is because <strong>explicit temporary tables <a href="http://dev.mysql.com/doc/refman/5.6/en/create-table.html">can still be created in a different engine</a> with the ENGINE keyword</strong>:</p>
<p><code>mysql> CREATE TEMPORARY TABLE test (i serial) ENGINE=MyISAM;<br />
Query OK, 0 rows affected (0.00 sec)</p>
<p>[ec2-user@jynus_com tmp]$ ls -la<br />
total 36<br />
drwxrwxrwt  5 root     root     4096 Feb 24 12:16 .<br />
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..<br />
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix<br />
-rw-rw----  1 mysql    mysql    8554 Feb 24 12:12 #sql7bbd_36a3_0.frm<br />
-rw-rw----  1 mysql    mysql       0 Feb 24 12:12 #sql7bbd_36a3_0.MYD<br />
-rw-rw----  1 mysql    mysql    1024 Feb 24 12:12 #sql7bbd_36a3_0.MYI<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo</p>
<p>mysql> DROP TEMPORARY TABLE test;<br />
Query OK, 0 rows affected (0.00 sec)</p>
<p>[ec2-user@jynus_com tmp]$ ls -la<br />
total 20<br />
drwxrwxrwt  5 root     root     4096 Feb 24 12:17 .<br />
dr-xr-xr-x 23 root     root     4096 Jan 28 14:09 ..<br />
drwxrwxrwt  2 root     root     4096 Jan 28 14:09 .ICE-unix<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:41 ssh-5ZGoXWFwtQ<br />
drwx------  2 ec2-user ec2-user 4096 Feb 24 11:43 ssh-w9IkW0SvYo<br />
</code></p>
<p>Will this change in the future? 5.7.5 continues to have the same behavior as 5.6. However, as <a href="https://www.flamingspork.com/blog/2013/04/26/a-few-notes-on-innodb-in-mysql-5-7-1/">Stewart pointed some time ago</a>, the performance optimizations in 5.7 make some uses of MEMORY and MyISAM obsolete so I will not be surprised if that dependency, together with MyISAM grant tables, will be removed in the future.</p>
<p><strong>Update:</strong> I&#8217;ve been told by email by <a href="http://www.tocker.ca/">Morgan</a> that the yet-to-be-released (at the time of this writing) 5.7.6 will finally change the default behavior to be full InnoDB for implicit temporary tables, too, as seen on the <a href="http://dev.mysql.com/doc/relnotes/mysql/5.7/en/news-5-7-6.html">release notes</a>:</p>
<blockquote><p>InnoDB: The default setting for the internal_tmp_disk_storage_engine option, which defines the storage engine the server uses for on-disk internal temporary tables (see How MySQL Uses Internal Temporary Tables), is now INNODB. With this change, the Optimizer uses the InnoDB storage engine instead of MyISAM for internal temporary tables.</p></blockquote>
<p><code>internal_tmp_disk_storage_engine</code> was introduced in 5.7.5, but its default value then was MYISAM.</p>
<p>This is in order to get advantage of the in-memory performance of InnoDB for variable-lengh fields, which I am personally 100% for. Thank you Morgan for the extra information!</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989031&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989031&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 24 Feb 2015 12:47:27 +0000</pubDate>
    <dc:creator>Jaime Crespo</dc:creator>
    <category>mysql</category>
    <category>5.6</category>
    <category>5.7</category>
    <category>engine</category>
    <category>format</category>
    <category>heap</category>
    <category>innodb</category>
    <category>memory</category>
    <category>myisam</category>
    <category>primary</category>
    <category>tables</category>
    <category>temporary</category>
    <category>tmp</category>
  </item>

  <item>
    <title>Is MySQL’s innodb_file_per_table slowing you down?</title>
    <guid isPermaLink="false">http://www.percona.com/blog/?p=28192</guid>
    <link>http://www.percona.com/blog/2015/02/24/mysqls-innodb_file_per_table-slowing/</link>
    <description>MySQL&amp;#8217;s innodb_file_per_table is a wonderful thing &amp;#8211; most of the time. Having every table use its own .ibd file allows you to easily reclaim space when dropping or truncating tables. But in some use cases, it may cause significant performance issues.Many of you in the audience are responsible for running automated tests on your codebase before deploying to production. If you are, then one of your goals is having tests run as quickly as possible so you can run them as frequently as possible. Often times you can change specific settings in your test environment that don&amp;#8217;t affect the outcome of the test, but do improve throughput. This post discusses how innodb_file_per_table is one of those settings.I recently spoke with a customer whose use case involved creating hundreds of tables on up to 16 schemas concurrently as part of a Jenkins testing environment. This was not in production, so performance was far more important than durability. They&amp;#8217;d run their tests, and then drop the schemas. This process took close to 20 minutes. They asked &amp;#8220;How can we make this faster?&amp;#8221;Due to the number of tables involved innodb_file_per_table seemed a likely culprit.It&amp;#8217;s been noted here on the MySQL Performance Blog that innodb_file_per_table can cause table creation and drops to slow down. But what exactly is the performance hit? We wanted to find out.The innodb_file_per_table Test:On a test server running CentOS release 6.5, xfs filesystem, and 5.6.22-71.0-log Percona Server, I ran the following homemade benchmark bash script:[root@host ~]# time $(for db in {1..16};
do mysql -e &quot;create database bench$db&quot;;
$(for tb in {1..500}; do $(mysql bench$db -e &quot;create table tab${tb} (i int) engine=innodb&quot;); done) &amp;amp; done)If you open the mysql client in another screen or terminal, you should see something like this:...
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
| Id    | User | Host      | db      | Command | Time | State          | Info                                     | Rows_sent | Rows_examined |         
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
| 80013 | root | localhost | NULL    | Query   |    0 | init           | show processlist                         |         0 |             0 |         
| 89462 | root | localhost | bench5  | Query   |    0 | creating table | create table tab95 (i int) engine=innodb |         0 |             0 |         
| 89466 | root | localhost | bench8  | Query   |    0 | creating table | create table tab81 (i int) engine=innodb |         0 |             0 |         
| 89467 | root | localhost | bench1  | Query   |    0 | creating table | create table tab91 (i int) engine=innodb |         0 |             0 |         
| 89468 | root | localhost | bench13 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89469 | root | localhost | bench15 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89472 | root | localhost | bench9  | Query   |    0 | creating table | create table tab86 (i int) engine=innodb |         0 |             0 |         
| 89473 | root | localhost | bench10 | Query   |    0 | creating table | create table tab94 (i int) engine=innodb |         0 |             0 |         
| 89474 | root | localhost | bench11 | Query   |    0 | creating table | create table tab80 (i int) engine=innodb |         0 |             0 |         
| 89475 | root | localhost | bench3  | Query   |    0 | creating table | create table tab80 (i int) engine=innodb |         0 |             0 |         
| 89476 | root | localhost | bench2  | Query   |    0 | creating table | create table tab82 (i int) engine=innodb |         0 |             0 |         
| 89478 | root | localhost | bench4  | Query   |    0 | creating table | create table tab91 (i int) engine=innodb |         0 |             0 |         
| 89479 | root | localhost | bench16 | Query   |    0 | creating table | create table tab88 (i int) engine=innodb |         0 |             0 |         
| 89481 | root | localhost | bench12 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89483 | root | localhost | bench6  | Query   |    0 | creating table | create table tab96 (i int) engine=innodb |         0 |             0 |         
| 89484 | root | localhost | bench14 | Query   |    0 | creating table | create table tab95 (i int) engine=innodb |         0 |             0 |         
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
...        After creating the tables, I dropped all schemas concurrently:[root@host ~]# time $(for db in {1..16};
do mysql -e &quot;drop database bench${db}&quot; &amp;amp; done)So what was the difference with innodb_file_per_table ON vs OFF?With innodb_file_per_table=ONSchema and table creation = 1m54.852sSchema drops = 1m21.682sWith innodb_file_per_table=OFFSchema and table creation = 0m59.968sSchema drops = 0m54.870sSo creation time decreased by 48%, drop time decreased by 33%.I think its worth noting that this benchmark creates and drops empty tables. Dropping InnoDB tables created with innodb_file_per_table=ON can take much longer if they have large amounts of data.Please also be aware that there are always trade-offs when modifying your InnoDB settings. That is outside the scope of this post, so please research and test before making changes. The MySQL documentation discusses that here.  In 5.6.6 and up, innodb_file_per_table is ON by default. MySQL 5.6 will also create temp tables as InnoDB, as noted here.So there you have it. If your primary goal is to improve create and drop table time, turning OFF innodb_file_per_table will save significant amounts of time.The post Is MySQL&amp;#8217;s innodb_file_per_table slowing you down? appeared first on MySQL Performance Blog.</description>
    <content:encoded><![CDATA[<p>MySQL&#8217;s <a title="innodb_file_per_table" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-parameters.html#sysvar_innodb_file_per_table" target="_blank">innodb_file_per_table</a> is a wonderful thing &#8211; most of the time. Having every table use its own .ibd file allows you to easily reclaim space when dropping or truncating tables. But in some use cases, it may cause significant performance issues.</p><p>Many of you in the audience are responsible for running automated tests on your codebase before deploying to production. If you are, then one of your goals is having tests run as quickly as possible so you can run them as frequently as possible. Often times you can change specific settings in your test environment that don&#8217;t affect the outcome of the test, but do improve throughput. This post discusses how innodb_file_per_table is one of those settings.</p><p>I recently spoke with a customer whose use case involved creating hundreds of tables on up to 16 schemas concurrently as part of a Jenkins testing environment. This was not in production, so performance was far more important than durability. They&#8217;d run their tests, and then drop the schemas. This process took close to 20 minutes. They asked &#8220;How can we make this faster?&#8221;</p><p>Due to the number of tables involved innodb_file_per_table seemed a likely culprit.</p><p>It&#8217;s been noted <a title="here" href="http://www.percona.com/blog/2011/02/03/performance-problem-with-innodb-and-drop-table" target="_blank">here</a> on the MySQL Performance Blog that innodb_file_per_table can cause table creation and drops to slow down. But what exactly is the performance hit? We wanted to find out.</p><h2>The innodb_file_per_table Test:</h2><p>On a test server running CentOS release 6.5, xfs filesystem, and 5.6.22-71.0-log <a href="http://www.percona.com/software/percona-server" target="_blank">Percona Server</a>, I ran the following homemade benchmark bash script:</p><pre>[root@host ~]# time $(for db in {1..16};
do mysql -e "create database bench$db";
$(for tb in {1..500}; do $(mysql bench$db -e "create table tab${tb} (i int) engine=innodb"); done) &amp; done)</pre><p>If you open the mysql client in another screen or terminal, you should see something like this:</p><pre>...
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
| Id    | User | Host      | db      | Command | Time | State          | Info                                     | Rows_sent | Rows_examined |         
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
| 80013 | root | localhost | NULL    | Query   |    0 | init           | show processlist                         |         0 |             0 |         
| 89462 | root | localhost | bench5  | Query   |    0 | creating table | create table tab95 (i int) engine=innodb |         0 |             0 |         
| 89466 | root | localhost | bench8  | Query   |    0 | creating table | create table tab81 (i int) engine=innodb |         0 |             0 |         
| 89467 | root | localhost | bench1  | Query   |    0 | creating table | create table tab91 (i int) engine=innodb |         0 |             0 |         
| 89468 | root | localhost | bench13 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89469 | root | localhost | bench15 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89472 | root | localhost | bench9  | Query   |    0 | creating table | create table tab86 (i int) engine=innodb |         0 |             0 |         
| 89473 | root | localhost | bench10 | Query   |    0 | creating table | create table tab94 (i int) engine=innodb |         0 |             0 |         
| 89474 | root | localhost | bench11 | Query   |    0 | creating table | create table tab80 (i int) engine=innodb |         0 |             0 |         
| 89475 | root | localhost | bench3  | Query   |    0 | creating table | create table tab80 (i int) engine=innodb |         0 |             0 |         
| 89476 | root | localhost | bench2  | Query   |    0 | creating table | create table tab82 (i int) engine=innodb |         0 |             0 |         
| 89478 | root | localhost | bench4  | Query   |    0 | creating table | create table tab91 (i int) engine=innodb |         0 |             0 |         
| 89479 | root | localhost | bench16 | Query   |    0 | creating table | create table tab88 (i int) engine=innodb |         0 |             0 |         
| 89481 | root | localhost | bench12 | Query   |    0 | creating table | create table tab90 (i int) engine=innodb |         0 |             0 |         
| 89483 | root | localhost | bench6  | Query   |    0 | creating table | create table tab96 (i int) engine=innodb |         0 |             0 |         
| 89484 | root | localhost | bench14 | Query   |    0 | creating table | create table tab95 (i int) engine=innodb |         0 |             0 |         
+-------+------+-----------+---------+---------+------+----------------+------------------------------------------+-----------+---------------+         
...        </pre><p>After creating the tables, I dropped all schemas concurrently:</p><pre>[root@host ~]# time $(for db in {1..16};
do mysql -e "drop database bench${db}" &amp; done)</pre><p>So what was the difference with innodb_file_per_table ON vs OFF?</p><ul><li>With innodb_file_per_table=ON<ul><li>Schema and table creation = 1m54.852s</li><li>Schema drops = 1m21.682s</li></ul></li><li>With innodb_file_per_table=OFF<ul><li>Schema and table creation = 0m59.968s</li><li>Schema drops = 0m54.870s</li></ul></li></ul><p>So creation time decreased by 48%, drop time decreased by 33%.</p><p>I think its worth noting that this benchmark creates and drops empty tables. Dropping InnoDB tables created with innodb_file_per_table=ON can take much longer if they have large amounts of data.</p><p>Please also be aware that there are always trade-offs when modifying your InnoDB settings. That is outside the scope of this post, so please research and test before making changes. The MySQL documentation discusses that <a title="here" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/innodb-multiple-tablespaces.html" target="_blank">here</a>.  In 5.6.6 and up, innodb_file_per_table is ON by default. MySQL 5.6 will also create temp tables as InnoDB, as noted <a title="here" rel="nofollow" href="http://dev.mysql.com/doc/refman/5.6/en/server-system-variables.html#sysvar_default_tmp_storage_engine" target="_blank">here</a>.</p><p>So there you have it. If your primary goal is to improve create and drop table time, turning OFF innodb_file_per_table will save significant amounts of time.</p><p>The post <a rel="nofollow" href="http://www.percona.com/blog/2015/02/24/mysqls-innodb_file_per_table-slowing/">Is MySQL&#8217;s innodb_file_per_table slowing you down?</a> appeared first on <a rel="nofollow" href="http://www.percona.com/blog">MySQL Performance Blog</a>.</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989029&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989029&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 24 Feb 2015 11:00:48 +0000</pubDate>
    <dc:creator>MySQL Performance Blog</dc:creator>
    <category>Benchmarks</category>
    <category>InnoDB</category>
    <category>MySQL</category>
    <category>Percona MySQL Consulting</category>
    <category>Percona Server</category>
    <category>Benchmarking</category>
    <category>innodb_file_per_table</category>
    <category>Percona Consulting Jobs</category>
    <category>Primary</category>
  </item>

  <item>
    <title>Schemaless Databases Don't Exist</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/02/24/schemaless-databases-dont-exist/</guid>
    <link>https://vividcortex.com/blog/2015/02/24/schemaless-databases-dont-exist/</link>
    <description>There’s no such thing as a schemaless database. I know, lots of people want a schemaless database, and lots of companies are promoting their products as schemaless DBMSs. And schemaless DBMSs exist. But schemaless databases are mythical beasts because there is always a schema somewhere. Usually in multiple places, which I will later claim is what causes grief.

There Is Always A Schema

We should define “schema” first. It comes from Greek roots, meaning “form, figure” according to my dictionary. Wikipedia says, roughly,


  A database schema is its structure; a set of integrity constraints imposed on a database. These integrity constraints ensure compatibility between parts of the schema.


In other words, a schema expresses expectations about what fields exist in a database, and what their types will be. It also enforces those expectations, at least to some extent (there’s usually some flexibility).

My claim is that there’s always a schema, because somewhere, something has expectations about what’s in a database. At least, any useful, practical, real database. The DBMS itself may not have such expectations, but something else does.

Schema In The Database

When the DBMS enforces the schema, then we say the schema is in the database. If you’re using MySQL and you try to insert a value into a column that doesn’t exist, you’ll get an error like this:

ERROR 1054 (42S22): Unknown column 'flavor' in 'field list'


Whoops. I’ll have to run an ALTER TABLE if I want to do that.

Schema In The Code

If I used MongoDB, I wouldn’t have this problem. I could write my code to insert flavor fields in documents, and read back those documents and do something with the flavor field. I don’t have to have the schema in the database (the DBMS doesn’t have to enforce it).

Now my schema is in my code, isn’t it? I can’t do anything useful with something’s flavor attribute unless the code knows it’s there. You could argue that maybe my code doesn’t have to know about it; perhaps it just mindlessly accesses whatever it finds and lets something else do what it pleases with it. In that case, though, the schema is in the client application or user. The buck has to stop somewhere.

It reminds me of the semantic web, microformats, and the like. All very nice, but somewhere, something or someone has to know what a person is, what an address is, what a song is, what an album and artist is. It can’t be infinite turtles all the way down, can it?



Schema In Both Places

I’ve just claimed that the schema is in the code if it’s not in the DBMS. If I use MySQL and add a flavor column to the table, then my DBMS knows that this attribute is valid. But even when the DBMS has the schema, the code does too. If my code doesn’t know, respect, and agree with the schema in the DBMS, then we’re going to have problems like the Unknown column error above.

This is where the fallacy enters, in my opinion. People say their database has no schema, is unstructured, etc. It would be more accurate to say “there is no single centralized schema definition. It is scattered throughout my code.”

Is that a bad thing?

In my opinion, no. A strongly enforced central definition is a dependency that doesn’t scale well, in human terms. Large codebases end up with dependencies on centralized schema definitions that are brittle and require lots of things to be updated at a single time, instead of allowing the code to cope with a fluid and evolving schema definition and gradually be updated.

I remember working at an ecommerce website that had many hundreds of databases, thousands of tables, and if I recall correctly, millions of stored procedures. We used a vendor tool to scan all our source code and databases and show us graphs of the relationships between all these things. After months of waiting for the indexing to complete, we opened up the application and the moment of truth arrived. “Let’s look at the order inventory table,” someone suggested. A glorious hairball emerged, slowly painting line after line until the screen was just a big black blob. It was useless and just told us what we already knew: the schema of the order inventory table was expressed in so many places, a change to it was probably impossible. I don’t know, but I’d bet a donut it hasn’t changed since then.

The other point of view on this is that the database’s job is to define the data and ensure only valid data is entered. I know this is a common point of pride among people who like PostgreSQL better than MySQL. And it’s surely valid, as well. It’s true that if the DBMS is permissive, you can end up with garbage in it. But my experience with large applications has been that this feels good at first and then becomes a problem later on. Just my two cents.

Conclusion

Since this is more or less a rant, I should not go on too much longer. Main points:


  A database isn’t just a DBMS and the schema and data in it. The apps that interact with the data are usually part of the database per se, too.
  There’s no such thing as schemaless. The schema is always in the code; the question is whether it’s also centrally enforced in the DBMS.
  My experience has been that centralized schema definitions are harder to scale on large applications and codebases.


Pic credit: Dilbert</description>
    <content:encoded><![CDATA[<p>There’s no such thing as a schemaless database. I know, lots of people want a schemaless database, and lots of companies are promoting their products as schemaless DBMSs. And schemaless DBMSs exist. But schemaless <em>databases</em> are mythical beasts because there is always a schema somewhere. Usually in multiple places, which I will later claim is what causes grief.</p>

<h3>There Is Always A Schema</h3>

<p>We should define “schema” first. It comes from Greek roots, meaning “form, figure” according to my dictionary. Wikipedia says, roughly,</p>

<blockquote>
  <p>A database schema is its structure; a set of integrity constraints imposed on a database. These integrity constraints ensure compatibility between parts of the schema.</p>
</blockquote>

<p>In other words, a schema expresses expectations about what fields exist in a database, and what their types will be. It also enforces those expectations, at least to some extent (there’s usually some flexibility).</p>

<p>My claim is that there’s always a schema, because somewhere, <em>something</em> has expectations about what’s in a database. At least, any useful, practical, real database. The DBMS itself may not have such expectations, but something else does.</p>

<h3>Schema In The Database</h3>

<p>When the DBMS enforces the schema, then we say the schema is in the database. If you’re using MySQL and you try to insert a value into a column that doesn’t exist, you’ll get an error like this:</p>

<pre><code>ERROR 1054 (42S22): Unknown column 'flavor' in 'field list'
</code></pre>

<p>Whoops. I’ll have to run an <code>ALTER TABLE</code> if I want to do that.</p>

<h3>Schema In The Code</h3>

<p>If I used MongoDB, I wouldn’t have this problem. I could write my code to insert <code>flavor</code> fields in documents, and read back those documents and do something with the <code>flavor</code> field. I don’t have to have the schema in the database (the DBMS doesn’t have to enforce it).</p>

<p>Now my schema is in my code, isn’t it? I can’t do anything useful with something’s <code>flavor</code> attribute unless the code knows it’s there. You could argue that maybe my code doesn’t have to know about it; perhaps it just mindlessly accesses whatever it finds and lets something else do what it pleases with it. In that case, though, the schema is in the client application or user. The buck has to stop somewhere.</p>

<p>It reminds me of the semantic web, microformats, and the like. All very nice, but somewhere, something or someone has to know what a person is, what an address is, what a song is, what an album and artist is. It can’t be infinite turtles all the way down, can it?</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/dilbert-turtles.png" alt="infinite turtles" /></p>

<h3>Schema In Both Places</h3>

<p>I’ve just claimed that the schema is in the code if it’s not in the DBMS. If I use MySQL and add a <code>flavor</code> column to the table, then my DBMS knows that this attribute is valid. But even when the DBMS has the schema, <em>the code does too</em>. If my code doesn’t know, respect, and agree with the schema in the DBMS, then we’re going to have problems like the <code>Unknown column</code> error above.</p>

<p>This is where the fallacy enters, in my opinion. People say their database has no schema, is unstructured, etc. It would be more accurate to say “there is no single centralized schema definition. It is scattered throughout my code.”</p>

<p>Is that a bad thing?</p>

<p>In my opinion, no. A strongly enforced central definition is a dependency that doesn’t scale well, in human terms. Large codebases end up with dependencies on centralized schema definitions that are brittle and require lots of things to be updated at a single time, instead of allowing the code to cope with a fluid and evolving schema definition and gradually be updated.</p>

<p>I remember working at an ecommerce website that had many hundreds of databases, thousands of tables, and if I recall correctly, millions of stored procedures. We used a vendor tool to scan all our source code and databases and show us graphs of the relationships between all these things. After months of waiting for the indexing to complete, we opened up the application and the moment of truth arrived. “Let’s look at the order inventory table,” someone suggested. A glorious hairball emerged, slowly painting line after line until the screen was just a big black blob. It was useless and just told us what we already knew: the schema of the order inventory table was expressed in so many places, a change to it was probably impossible. I don’t know, but I’d bet a donut it hasn’t changed since then.</p>

<p>The other point of view on this is that the database’s job is to define the data and ensure only valid data is entered. I know this is a common point of pride among people who like PostgreSQL better than MySQL. And it’s surely valid, as well. It’s true that if the DBMS is permissive, you can end up with garbage in it. But my experience with large applications has been that this feels good at first and then becomes a problem later on. Just my two cents.</p>

<h3>Conclusion</h3>

<p>Since this is more or less a rant, I should not go on too much longer. Main points:</p>

<ul>
  <li>A database isn’t just a DBMS and the schema and data in it. The apps that interact with the data are usually part of the database per se, too.</li>
  <li>There’s no such thing as schemaless. The schema is always in the code; the question is whether it’s also centrally enforced in the DBMS.</li>
  <li>My experience has been that centralized schema definitions are harder to scale on large applications and codebases.</li>
</ul>

<p>Pic credit: <a href="http://dilbert.com/strip/2011-02-07">Dilbert</a></p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989033&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989033&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Tue, 24 Feb 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Real-time data loading from Oracle and MySQL to data warehouses, analytics</title>
    <guid isPermaLink="false">tag:blogger.com,1999:blog-5088740386330348582.post-850961184348114407</guid>
    <link>http://continuent-tungsten.blogspot.com/2015/02/real-time-data-loading-from-oracle-and.html</link>
    <description>Analyzing transactional data is becoming increasingly common, especially as the data sizes and complexity increase and transactional stores are no longer to keep pace with the ever-increasing storage. Although there are many techniques available for loading data, getting effective data in real-time into your data warehouse store is a more difficult problem.In this webinar-on-demand we showcase</description>
    <content:encoded><![CDATA[Analyzing transactional data is becoming increasingly common, especially as the data sizes and complexity increase and transactional stores are no longer to keep pace with the ever-increasing storage. Although there are many techniques available for loading data, getting effective data in real-time into your data warehouse store is a more difficult problem.In this webinar-on-demand we showcase<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989025&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989025&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 23 Feb 2015 19:45:00 +0000</pubDate>
    <dc:creator>Petri Virsunen</dc:creator>
    <category>Amazon Redshift</category>
    <category>Apache Hadoop</category>
    <category>Continuent Tungsten</category>
    <category>data analytics</category>
    <category>data warehouse</category>
    <category>database replication</category>
    <category>HP Vertica</category>
    <category>MongoDB</category>
    <category>Oracle</category>
    <category>replication</category>
  </item>

  <item>
    <title>Restore dropped MySQL database from binary logs</title>
    <guid isPermaLink="false">http://kedar.nitty-witty.com/blog/?p=2021</guid>
    <link>http://kedar.nitty-witty.com/blog/restore-dropped-mysql-database-from-binary-logs</link>
    <description>In this post I will share a scenario of a MySQL database restore from the binary logs. Recently someone accidentally dropped an important MySQL database and the backup was not present! As we know the the binary log contains DMLs to table data and that&amp;#8217;s where our hope lies. Luckily the binary log retention period [&amp;#8230;]</description>
    <content:encoded><![CDATA[In this post I will share a scenario of a MySQL database restore from the binary logs. Recently someone accidentally dropped an important MySQL database and the backup was not present! As we know the the binary log contains DMLs to table data and that&#8217;s where our hope lies. Luckily the binary log retention period [&#8230;]<br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989018&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989018&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 23 Feb 2015 12:42:12 +0000</pubDate>
    <dc:creator>Kedar Vaijanapurkar</dc:creator>
    <category>MySQL</category>
    <category>MySQL-Articles</category>
  </item>

  <item>
    <title>What Is Com admin commands In MySQL?</title>
    <guid isPermaLink="false">https://vividcortex.com/blog/2015/02/23/what-is-com-admin-commands-in-mysql/</guid>
    <link>https://vividcortex.com/blog/2015/02/23/what-is-com-admin-commands-in-mysql/</link>
    <description>If you’ve ever looked at the COM_XYZ status counters in MySQL’s SHOW STATUS output, you’ve probably seen Com_admin_commands. It’s not clear what this means, but it can be a major contributor to overall COM_ counters, and it’s actually quite important for server and application performance, as well as being a marker of code quality. In this blog post I’ll explain what the counter really means, and then as a bonus I’ll demonstrate that VividCortex will show you exactly what’s going on in the murkiness of “admin commands.”



What Does The Com_admin_commands Counter Mean?

The first question is “what is this counter?” The manual isn’t really helpful. It just says the following:


  The Com_xxx statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement.


That doesn’t really clarify at all, because if you know the MySQL protocol you know there is no “admin command” statement. Fortunately, MySQL is open-source, so we can find out exactly what it means! And now that MySQL’s source code is on GitHub it’s even easier. Here’s a snippet of code comment that explains what’s really inside that counter, which comes from a variable (in the source code) called com_other:


#ifndef DBUG_OFF
  /*
    We have few debug-only commands in com_status_vars, only visible in debug
    builds. for simplicity we enable the assert only in debug builds

    There are 8 Com_ variables which don't have corresponding SQLCOM_ values:
    (TODO strictly speaking they shouldn't be here, should not have Com_ prefix
    that is. Perhaps Stmt_ ? Comstmt_ ? Prepstmt_ ?)

      Com_admin_commands       =&amp;gt; com_other
      Com_stmt_close           =&amp;gt; com_stmt_close
      Com_stmt_execute         =&amp;gt; com_stmt_execute
      Com_stmt_fetch           =&amp;gt; com_stmt_fetch
      Com_stmt_prepare         =&amp;gt; com_stmt_prepare
      Com_stmt_reprepare       =&amp;gt; com_stmt_reprepare
      Com_stmt_reset           =&amp;gt; com_stmt_reset
      Com_stmt_send_long_data  =&amp;gt; com_stmt_send_long_data

    With this correction the number of Com_ variables (number of elements in
    the array, excluding the last element - terminator) must match the number
    of SQLCOM_ constants.
  */
  compile_time_assert(sizeof(com_status_vars)/sizeof(com_status_vars[0]) - 1 ==
                     SQLCOM_END + 8);
#endif


In a nutshell, this counter is an aggregate of some others (why they didn’t just keep them separate I don’t know). By grepping through the source code I found the following things that increment com_other:


COM_CHANGE_USER
COM_SHUTDOWN
COM_PING
COM_DEBUG
COM_BINLOG_DUMP_GTID
COM_BINLOG_DUMP


So those six commands increment what ends up being Com_admin_commands. In practice, all but COM_PING are fairly rare. The shutdown and debug commands don’t happen often, most applications don’t switch the connection’s user, and asking the server to send you its binary logs tends to be a single command followed by a very long stream of data, not a rapidly repeated command.

But still, saying “It’s probably just COM_PING” is not good enough. I’m sure you have your own war stories to tell about how the most likely explanation sometimes wastes days of effort because the least likely thing turns out to be true after all.

Luckily, you have VividCortex! But before we get to that, let’s take a look at what COM_PING really does.

COM_PING Considered Harmful

Ping commands are generally a sign that something quite unwise is being done. I’ve written about this before in other places, but to recap, a lot of ORMs and connection libraries do race-prone pings to check whether a database connection is alive before they send a query on it.

I know that this is true in Ruby On Rails:


def active?
  return false unless @connection
  @connection.ping
end


I also know it’s true in Perl’s Ima::DBI module, which is the foundation for a lot of other things in Perl:


       # reopen if this is a new process or if the connection
       # is bad
       if ($process_id != $$ or 
                    not ($dbh &amp;amp;&amp;amp; $dbh-&amp;gt;FETCH('Active') &amp;amp;&amp;amp; $dbh-&amp;gt;ping)) {
                    $dbh = DBI-&amp;gt;connect_cached($dsn, $user, $pass, $attr);
                    $process_id = $$;
       }
       return $dbh;


So what’s so bad about it? Quite a bit:


  It is the unmistakable code smell of a race condition. Pinging doesn’t guarantee the connection is still alive by the time you execute the command the caller wanted to run.
  It is completely wasted work. Far, far better to do it the way Go’s database/sql package does, without a Ping at all. If you don’t know how that works, you should read our comprehensive ebook on database/sql.
  It adds latency to the application. You might think it doesn’t add much, because come on, a ping is a no-op, right? But it does. It’s a bunch of work for the application, followed by a network round-trip plus typically about 50 microseconds server-side, and then a bunch more work for the application.


The net effects of all these libraries doing race-prone, latency-adding pings all the time are a bad user experience and more brittle applications. If you’re using an ORM or database library that does this, you should use something else instead. (When I was a consultant, a bunch of my Rails clients used to monkey-patch the active method after I pointed out all of the above. I have a feeling a nontrivial percentage of Rails apps are running in production with this silliness monkey-patched out… double-silliness!)

And now I will prove to you with measurements what I’ve claimed with words above. Because, naturally, you can measure this with VividCortex!

Measuring Pings With VividCortex

At this point we know more about COM_PING, but we don’t yet know the impact. The first thing you might notice with VividCortex is the size of the Com_admin_commands stripe in your Commands graph. Here it’s the red stripe on the bottom (in the app you can see that if you mouse over it, but I’m not showing that here because it takes up too much vertical space).



Now, if we didn’t know what was inside that counter, we’d have to stop there. But in VividCortex, every command to the server is represented as a query. If we go to Top Queries, and rank by Count, we’ll see the following:



Notice that the number 1 query by count, more than twice as big as the second one, is protocol com_ping. That’s the pseudo-query we use to identify this protocol command. It is about half of the total queries this server runs (which makes total sense, because with an ORM like the ones above, every time you run a query, you run this command first). And look, during the 20-minute period shown in the screenshot, it accumulated more than a minute of total latency!

Naturally, we can switch to view queries by time instead of count. Do you think pings are far down the list? No, they rank #4 by total time, accounting for more than 5% of the total work this server does.



The story from the app’s point of view is even worse, because that 5% of server-side latency is magnified by the network round trip and the processing the app has to do.

If I told you that there was “this one weird trick to get 5% more capacity from your servers and reduce end-user latency and improve quality of service,” would you do it? If you had 20 servers and wanted to get rid of one of them, you certainly would.

In conclusion, with VividCortex, you’ve got a powerful set of tools to find out what your servers are really doing, in production, with practically no overhead, and better yet practically no effort. Just open up the app and it’s two clicks to insight.

Recap

In this post I explained that…


  Com_admin_commands is a combination of commands not accounted separately inside the server; usually COM_PING but could be other things
  If it is pings, it’s the sign of a badly written database connection library, and in my experience it is a frequent cause of performance problems for the server and the app
  VividCortex measures everything the server does, even if they’re not queries, and lets you very easily see what’s really going on in your production systems.


Are you curious what VividCortex could reveal about your own servers? If you haven’t yet experienced VividCortex’s unmatched level of insight into your databases (MySQL and PostgreSQL currently, more on the way), you should sign up for a free trial right away!</description>
    <content:encoded><![CDATA[<p>If you’ve ever looked at the <code>COM_XYZ</code> status counters in MySQL’s <code>SHOW STATUS</code> output, you’ve probably seen <code>Com_admin_commands</code>. It’s not clear what this means, but it can be a major contributor to overall <code>COM_</code> counters, and it’s actually quite important for server and application performance, as well as being a marker of code quality. In this blog post I’ll explain what the counter really means, and then as a bonus I’ll demonstrate that VividCortex will show you exactly what’s going on in the murkiness of “admin commands.”</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/com-admin.png" alt="COM_Admin_commands" /></p>

<h3>What Does The <code>Com_admin_commands</code> Counter Mean?</h3>

<p>The first question is “what is this counter?” The manual isn’t really helpful. It just <a href="http://dev.mysql.com/doc/refman/5.7/en/server-status-variables.html#statvar_Com_xxx">says</a> the following:</p>

<blockquote>
  <p>The <code>Com_xxx</code> statement counter variables indicate the number of times each xxx statement has been executed. There is one status variable for each type of statement.</p>
</blockquote>

<p>That doesn’t really clarify at all, because if you know the MySQL protocol you know there <em>is</em> no “admin command” statement. Fortunately, MySQL is open-source, so we can find out exactly what it means! And now that MySQL’s source code is on GitHub it’s even easier. Here’s a <a href="https://github.com/mysql/mysql-server/blob/f964f6b37337b98d467c1e9fb7914c1655a08e84/sql/mysqld.cc#L2803-L2827">snippet of code comment</a> that explains what’s really inside that counter, which comes from a variable (in the source code) called <code>com_other</code>:</p>

<pre>
#ifndef DBUG_OFF
  /*
    We have few debug-only commands in com_status_vars, only visible in debug
    builds. for simplicity we enable the assert only in debug builds

    There are 8 Com_ variables which don't have corresponding SQLCOM_ values:
    (TODO strictly speaking they shouldn't be here, should not have Com_ prefix
    that is. Perhaps Stmt_ ? Comstmt_ ? Prepstmt_ ?)

      Com_admin_commands       =&gt; com_other
      Com_stmt_close           =&gt; com_stmt_close
      Com_stmt_execute         =&gt; com_stmt_execute
      Com_stmt_fetch           =&gt; com_stmt_fetch
      Com_stmt_prepare         =&gt; com_stmt_prepare
      Com_stmt_reprepare       =&gt; com_stmt_reprepare
      Com_stmt_reset           =&gt; com_stmt_reset
      Com_stmt_send_long_data  =&gt; com_stmt_send_long_data

    With this correction the number of Com_ variables (number of elements in
    the array, excluding the last element - terminator) must match the number
    of SQLCOM_ constants.
  */
  compile_time_assert(sizeof(com_status_vars)/sizeof(com_status_vars[0]) - 1 ==
                     SQLCOM_END + 8);
#endif
</pre>

<p>In a nutshell, this counter is an aggregate of some others (why they didn’t just keep them separate I don’t know). By grepping through the source code I found the following things that increment <code>com_other</code>:</p>

<pre>
COM_CHANGE_USER
COM_SHUTDOWN
COM_PING
COM_DEBUG
COM_BINLOG_DUMP_GTID
COM_BINLOG_DUMP
</pre>

<p>So those six commands increment what ends up being <code>Com_admin_commands</code>. In practice, all but <code>COM_PING</code> are fairly rare. The shutdown and debug commands don’t happen often, most applications don’t switch the connection’s user, and asking the server to send you its binary logs tends to be a single command followed by a very long stream of data, not a rapidly repeated command.</p>

<p>But still, saying “It’s probably just <code>COM_PING</code>” is not good enough. I’m sure you have your own war stories to tell about how the most likely explanation sometimes wastes days of effort because the least likely thing turns out to be true after all.</p>

<p>Luckily, you have VividCortex! But before we get to that, let’s take a look at what <code>COM_PING</code> really does.</p>

<h3><code>COM_PING</code> Considered Harmful</h3>

<p>Ping commands are generally a sign that something quite unwise is being done. I’ve written about this before in other places, but to recap, a lot of ORMs and connection libraries do race-prone pings to check whether a database connection is alive before they send a query on it.</p>

<p>I know that this is true in <a href="https://github.com/rails/rails/blob/c0c6dd6aa27dd8e5ff334605fb133625b434f4bd/activerecord/lib/active_record/connection_adapters/mysql2_adapter.rb#L81-L84">Ruby On Rails</a>:</p>

<pre>
def active?
  return false unless @connection
  @connection.ping
end
</pre>

<p>I also know it’s true in <a href="http://cpansearch.perl.org/src/PERRIN/Ima-DBI-0.35/lib/Ima/DBI.pm">Perl’s <code>Ima::DBI</code> module</a>, which is the foundation for a lot of other things in Perl:</p>

<pre>
       # reopen if this is a new process or if the connection
       # is bad
       if ($process_id != $$ or 
                    not ($dbh &amp;&amp; $dbh-&gt;FETCH('Active') &amp;&amp; $dbh-&gt;ping)) {
                    $dbh = DBI-&gt;connect_cached($dsn, $user, $pass, $attr);
                    $process_id = $$;
       }
       return $dbh;
</pre>

<p>So what’s so bad about it? Quite a bit:</p>

<ol>
  <li>It is the unmistakable code smell of a race condition. Pinging doesn’t guarantee the connection is still alive by the time you execute the command the caller wanted to run.</li>
  <li>It is completely wasted work. Far, far better to do it the way Go’s <code>database/sql</code> package does, without a Ping at all. If you don’t know how that works, you should read <a href="https://vividcortex.com/resources/building-database-driven-apps-with-go/">our comprehensive ebook on database/sql</a>.</li>
  <li>It adds latency to the application. You might think it doesn’t add much, because come on, a ping is a no-op, right? But it does. It’s a bunch of work for the application, followed by a network round-trip plus typically about 50 microseconds server-side, and then a bunch more work for the application.</li>
</ol>

<p>The net effects of all these libraries doing race-prone, latency-adding pings all the time are a bad user experience and more brittle applications. If you’re using an ORM or database library that does this, you should use something else instead. (When I was a consultant, a bunch of my Rails clients used to monkey-patch the <code>active</code> method after I pointed out all of the above. I have a feeling a nontrivial percentage of Rails apps are running in production with this silliness monkey-patched out… double-silliness!)</p>

<p>And now I will prove to you with measurements what I’ve claimed with words above. Because, naturally, you can measure this with VividCortex!</p>

<h3>Measuring Pings With VividCortex</h3>

<p>At this point we know more about <code>COM_PING</code>, but we don’t yet know the impact. The first thing you might notice with VividCortex is the size of the <code>Com_admin_commands</code> stripe in your Commands graph. Here it’s the red stripe on the bottom (in the app you can see that if you mouse over it, but I’m not showing that here because it takes up too much vertical space).</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/com-admin.png" alt="COM_Admin_commands" /></p>

<p>Now, if we didn’t know what was inside that counter, we’d have to stop there. But in VividCortex, <em>every command to the server is represented as a query</em>. If we go to Top Queries, and rank by Count, we’ll see the following:</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/com-ping-count.png" alt="COM_PING count" /></p>

<p>Notice that the number 1 query by count, more than twice as big as the second one, is <code>protocol com_ping</code>. That’s the pseudo-query we use to identify this protocol command. It is about half of the total queries this server runs (which makes total sense, because with an ORM like the ones above, every time you run a query, you run this command first). And look, during the 20-minute period shown in the screenshot, it accumulated more than a minute of total latency!</p>

<p>Naturally, we can switch to view queries by time instead of count. Do you think pings are far down the list? No, they rank #4 by total time, accounting for more than 5% of the total work this server does.</p>

<p><img src="https://vividcortex.com/img/articles/2015/02/com-ping-time.png" alt="COM_PING time" /></p>

<p>The story from the app’s point of view is even worse, because that 5% of server-side latency is magnified by the network round trip and the processing the app has to do.</p>

<p>If I told you that there was “this one weird trick to get 5% more capacity from your servers and reduce end-user latency and improve quality of service,” would you do it? If you had 20 servers and wanted to get rid of one of them, you certainly would.</p>

<p>In conclusion, with VividCortex, you’ve got a powerful set of tools to find out what your servers are really doing, in production, with practically no overhead, and better yet practically no effort. Just open up the app and it’s two clicks to insight.</p>

<h3>Recap</h3>

<p>In this post I explained that…</p>

<ol>
  <li><code>Com_admin_commands</code> is a combination of commands not accounted separately inside the server; usually <code>COM_PING</code> but could be other things</li>
  <li>If it <em>is</em> pings, it’s the sign of a badly written database connection library, and in my experience it is a frequent cause of performance problems for the server and the app</li>
  <li>VividCortex measures everything the server does, even if they’re not queries, and lets you very easily see what’s really going on in your production systems.</li>
</ol>

<p>Are you curious what VividCortex could reveal about your own servers? If you haven’t yet experienced VividCortex’s unmatched level of insight into your databases (MySQL and PostgreSQL currently, more on the way), you should <a href="https://app.vividcortex.com/sign-up?utm_source=site&amp;utm_medium=blog">sign up for a free trial</a> right away!</p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989024&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989024&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Mon, 23 Feb 2015 05:00:00 +0000</pubDate>
  </item>

  <item>
    <title>Causal Consistency</title>
    <guid isPermaLink="false">https://blog.mariadb.org/?p=2726</guid>
    <link>https://blog.mariadb.org/causal-consistency/</link>
    <description>Introduction
Causal consistency [1] is one of the consistency criteria that can be used on distributed databases as consistency criteria.
Distributed database provides causal consistency if read and write operations that are causally related are seen by every node of the distributed system in the same order. Concurrent writes may be seen in different order in diffrent nodes.  Causal consistency is waker than sequential consistency [2] but stronger than eventual consistency [3]. See earlier blog for more detailed description on eventual consistency https://blog.mariadb.org/eventually-consistent-databases-state-of-the-art/.
When a transaction performs a read operation followed later by a write operation, even on different object, the first read is said to be causally ordered before the write. This is because the value created by the write may have been dependent upon the result of the read operation. Similarly, a read operation is causally ordered after the earlier write on the same object that stored the data retrieved by the read. Also, even two write operations performed by the same node are defined to be causally ordered, in the order they were performed. Intuitively, after writing value v into object x, a node knows that a read of x would give v, so a later write could be said to be (potentially) causally related to the earlier one. Finally, causal order is transitive: that is, if operation A is (causally) ordered before B, and B is ordered before C, A is ordered before C.
Operations that are not causally related, even through other operations, are said to be concurrent. Consider following example:Direction of time ----------------&amp;amp;gt;
P1 : w1[x] = 1
P2 : w2[x] = 2
P3 :             r3[x] = 2
P4 :                         r4[x] = 1This execution is causally consistent because read operations r3[x] is causally dependent on write w2[x] and read operation r4[x] is causally dependent on write w1[x]. Note that concurrent write operations may be seen on different order on different transactions or nodes.
Implementing Causal Consistency
Causal consistency can be reached by using Lamport clocks [4] or version vectors [5]. The causal consistency model is implemented by using multi-part timestamps, which are assigned to each object. These timestamps are stored on a vector that contains the version number of the object at each replica. This vector must be included (in the form of dependencies) in all update and query requests so that operations respect the causal ordering: an operation A can only be processed at a given node if all operations, on which the operation A causally depends, have already been applied at that node.
Each element in the vector clock corresponds to one host and the value of element indicates the number of messages sent from that host. The vector clock information is used to order or delay the delivery of messages if necessary, thus maintaining the required consistency. However, for maintaining the consistency of data items, we need information about writes on each data item, and maintaining a clock per data item can help. Therefore, instead of a vector clock of size N (number of hosts), we maintain a vector of size M (number of objects). The value of v[i] in the vector v contains the number of writes on data item i.
Each host maintains a vector of size M. Whenever it updates the value of an item, it increments the corresponding element and sends the vector along with the message of data item and new value to every site, which has a copy of the replica. When a host receives an update message, it delays the delivery of a message till each element in its vector is greater than or equal to the one that is piggybacked. After that, the updates to the data items are applied. In this case, the message overhead is O(M) and thus is independent of the number of hosts in the system.
If each message is an update message, it carries the new value of the data item rather than instructions. Then the delivery of an update on a data-item does not need not wait for the previous updates on the same item. This would not have been possible if vector clocks had been used. In that case, the delivery of a massage would have been delayed even for previous messages that are causally overwritten.
Applications and Databases using Causal Consistency
COPS and Eiger
COPS system [6] (Clusters of Order-Preserving Servers) introduces the causal+ consistency and is designed to support complex online applications that are hosted in a small number of large-scale data-centers, each of which is composed of front-end servers (clients of COPS) and back-end key-value data stores. Eiger [7] has a similar design but a different implementation.
COPS and Eiger support causality through a client library. Both systems replicate writes to geographically distributed data centers and enforce observed ordering. The observed ordering is enforced by delaying the write operations until all causally previous operations have been already applied at that data center.
COPS executes all read and write operations in the local data center in a linearizable [8] fashion, and then replicates data across data centers in a causal+ consistent order in the background. Figure 1 describes the high level architecture of COPS.
&amp;nbsp;

Figure 1: Architecture of COPS and Eiger [6].
Similarly, the Eiger system provides the linearizability inside each data center and the causally-consistent data store based on a column-family data model to achieve better performance in a geo-distributed setting. All operations from clients are served from the local data center using a client library. The library mediates access to nodes in the local data center, executes the read and write transaction algorithms, and tracks causality and attaches dependencies to write operations. Each replica stores full replica of the database, and operations are handled locally. After an operation is executed locally, the operation is asynchronously pushed to remote data centers, but committed only after all causally dependent operations have been previously committed.
Bolts-on
Bailis et. al. in [9] propose a client-side middle-ware software called Bolt-on. This middle-ware guarantees only application-defined dependencies as an alternative to the causal consistency. Figure 2 describes the architecture of the Bolt-on middle-ware.
&amp;nbsp;

Figure 2: Architecture of Bolts-on [9].
The Bolt-on architecture assumes that the underlying data store handles most aspects of data management, including replication, availability, and convergence. In the architecture, the underlying data store locally uses the eventual consistency and allows a large space of read and write histories; the middle-ware handles causal dependencies, and consists of a set of rules, which limit the possible histories to the histories that obey the desired consistency model.
Application: MMORPG
In Massively Multi-player Online Role-Playing Games (MMORPG), players can cooperate with others in a virtual game world, and both players and different game words are naturally distributed. These systems manage large amounts of data, and the biggest problem is how to support data consistency. According to the CAP theorem, we have to sacrifice one of two properties: consistency or availability [10]. If an online game does not guarantee the availability, players&amp;#8217; requests may fail. If data is inconsistent, players may get data not conforming to the game logic, and this data can affect their operations. Therefore, it is important for the MMORPG environment to find a balance between the data consistency and the system availability. For this reason, we must analyze the data consistency requirements of MMORPG so as to find the balance [10].
Diao [10] has studied different consistency models for MMORPG and found that there indeed are part of data, where the causal consistency is an appealing choice: Game data. The game data contains e.g. the world appearance, the meta-data of non-player characters (the characters are created by game developers and controlled only by the game logic), the system configuration and game rules. This data is used by players and the game engine in the entire game, but can be only modified by the game developers. Consistency requirements for the game data are not so strict compared e.g. to the account data. Because e.g. a change of non-player character name or of the duration of bird animation may not be noticed by players.
Furthermore, some change of the game data needs to be delivered to all online players synchronously, e.g. a change of the word appearance, the weapon power, non-player characters, game rules and scripts. If there is inconsistency on these areas, it will cause errors on game display and logic errors on players. Therefore, some data needs to be stored on the server side and some on the client side. The game data on the client side could only synchronize with servers when a player logs in to or starts a game. For this reason, the causal consistency is required [10].
This could mean that when a player A uses the browser to connect with the game server, the game server will check the current local data and update the game data of the client side in the form of data packets. After updating, all future local accesses will return the updated value. Player B, who has not communicated with the game server, will still retain the outdated game data.
Game servers maintain the primary version of game data, and transfer it to client sides. Additionally, players on different game words cannot discuss to each other. Thus, the only need is to make sure that the game data is consistent in one game word in a time so that all players on that game word are handled equally. This requires using the strong consistency locally in the game word and the causal consistency among different game words. When the game data is modified by developers, the update value should be delivered synchronously to all replicates on that game word, and asynchronously to other game words.
While the possibility of using the causal consistency on MMORPG has been identified on research [10] to the authors&amp;#8217; knowledge there is no actual publications or other information that the causal consistency is actually used on MMORPG games (e.g. Lord of the Rings Online).
Application: Facebook
Facebook is an online Social networking service. After registering to use the site, users can create a user profile, add other users as friends, exchange messages, post status updates and photos, share videos and receive notifications when others update their profiles.
When you log into your account on Facebook, the server will show your own status messages and your friends&amp;#8217; status messages at that point in time. Status messages on Facebook may contain pictures, shared links and stories or your own messages. Naturally, your account data requires a strong consistency, but for status data the weaker consistency models are acceptable. During the time the user is online, the status updates of a user&amp;#8217;s friends and of the user do not need to be strictly ordered, and the causal ordering is enough.
Thus when a user A sends a status update and a user B replies to that update, there is a causal order on the two updates. However, when users C and D do a totally unrelated update, the order these updates appear to users A and B is not relevant. This is because users A and B do not know in which order updates are performed.
The reason why the eventual consistency is not enough for Facebook status updates is that the eventual consistency does not require any ordering between writes. Consider a case, where the user A first sends a status update, and after few seconds A updates the first status update. With the eventual consistency, all friends of A could see only the first update, because the eventual consistency does not guarantee that first update is performed before the second one. In the causal consistency, as there is a read (by user A) of first update and then write (updated status from user A), these are causally related and all user A&amp;#8217;s friends will naturally see second update.
Although the causal consistency is the possible consistency model for Facebook status updates and several similar distributed services containing status updates like LinkedIn, Twitter and Yahoo, to author&amp;#8217;s knowledge there is not scientific or other literature that would show the causal consistency being really used.
Conclusions
The causal consistency model can be enforced with Lamport clocks. Transactions using the causal consistency are executed in an order that reflects their causally-related read/write operations&amp;#8217; order. Concurrent operations may be committed in different orders and their results can be read also in different orders.
Actually, the causal consistency can solve many problems, which cannot be solved in the eventual consistency, such as ordering operations. The causal consistency ensures that every sees operations in the same causal order, and this makes the causal consistency stronger than the eventual consistency. However, the causal consistency cannot support e.g. distributed integrity constraints.
Although there are a few promising systems developed on research to the authors&amp;#8217; knowledge there is no commercial or mature systems using the causal consistency model. For more thorough discussion see [11].
References
[1] Mustaque Ahamad , Gil Neiger , James E. Burns , Prince Kohli , P.W. Hutto: Causal Memory: Definitions, Implementation and Programming, http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3356
[2] Leslie Lamport: How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, IEEE Trans. Comput. C-28,9 (Sept. 1979), 690-691.
[3] Werner Vogels: Eventually consistent. Communications of the ACM 52: 40. doi:10.1145/1435417.1435432
[4] Leslie Lamport: Time, clocks, and the ordering of events in a distributed system, Communications of the ACM, 21: 7, pp. 558–565, 1978.
[5] C. J. Fidge: Timestamps in message passing systems that preserve the partial ordering, in Theoretical Computer Science, 1988.
[6] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen: Don’t settle for eventual: scalable causal consistency for wide-area storage with COPS, in Proceedings of the 23rd ACM Symposium on Operating Systems Principles, pp. 401–416, Portugal, October 23-26, 2011.
[7] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen: Stronger semantics for low-latency geo-replicated storage,” in Proceedings of the 10th USENIX Symposium on Networked Systems Design and Implementation, pp. 313–328, Lombard, IL, USA, April 2-5, 2013.
[8] M. Herlihy and J. M. Wing, Linearizability: A correctness condition for concurrent objects, ACM
 Trans. Program. Lang. Syst., 12:3, pp. 463–492, 1990.
[9] . Bailis, A. Ghodsi, J. M. Hellerstein, and I. Stoica,: Bolt-on causal consistency,” in Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 761–772, New York, NY, USA, June 22-27, 2013
[10] Z. Diao, “Consistency models for cloud-based on-line games: the storage system’s perspective,” in
25th Workshop on Grundlagen von Datenbanken, Computing, Portland, Oregon, USA, July 16-19,
pp. 16–21, Ilmenau, Germany, May 28 &amp;#8211; 31, 2013.
[11] Mawahib Musa Elbushra, Jan Lindström: Causal Consistent Databases, Open Journal of Databases (OJDB), 2:1, http://www.ronpub.com/publications/OJDB_2015v2i1n02_Elbushra.pdf</description>
    <content:encoded><![CDATA[<h1>Introduction</h1>
<p>Causal consistency [1] is one of the consistency criteria that can be used on distributed databases as consistency criteria.</p>
<p>Distributed database provides causal consistency if read and write operations that are causally related are seen by every node of the distributed system in the same order. Concurrent writes may be seen in different order in diffrent nodes.  Causal consistency is waker than sequential consistency [2] but stronger than eventual consistency [3]. See earlier blog for more detailed description on eventual consistency <a href="https://blog.mariadb.org/eventually-consistent-databases-state-of-the-art/">https://blog.mariadb.org/eventually-consistent-databases-state-of-the-art/</a>.</p>
<p>When a transaction performs a read operation followed later by a write operation, even on different object, the first read is said to be causally ordered before the write. This is because the value created by the write may have been dependent upon the result of the read operation. Similarly, a read operation is causally ordered after the earlier write on the same object that stored the data retrieved by the read. Also, even two write operations performed by the same node are defined to be causally ordered, in the order they were performed. Intuitively, after writing value v into object x, a node knows that a read of x would give v, so a later write could be said to be (potentially) causally related to the earlier one. Finally, causal order is transitive: that is, if operation A is (causally) ordered before B, and B is ordered before C, A is ordered before C.</p>
<p>Operations that are not causally related, even through other operations, are said to be concurrent. Consider following example:</p><pre>Direction of time ----------------&amp;gt;
P1 : w1[x] = 1
P2 : w2[x] = 2
P3 :             r3[x] = 2
P4 :                         r4[x] = 1</pre><p>This execution is causally consistent because read operations r3[x] is causally dependent on write w2[x] and read operation r4[x] is causally dependent on write w1[x]. Note that concurrent write operations may be seen on different order on different transactions or nodes.</p>
<h2>Implementing Causal Consistency</h2>
<p>Causal consistency can be reached by using Lamport clocks [4] or version vectors [5]. The causal consistency model is implemented by using multi-part timestamps, which are assigned to each object. These timestamps are stored on a vector that contains the version number of the object at each replica. This vector must be included (in the form of dependencies) in all update and query requests so that operations respect the causal ordering: an operation A can only be processed at a given node if all operations, on which the operation A causally depends, have already been applied at that node.</p>
<p>Each element in the vector clock corresponds to one host and the value of element indicates the number of messages sent from that host. The vector clock information is used to order or delay the delivery of messages if necessary, thus maintaining the required consistency. However, for maintaining the consistency of data items, we need information about writes on each data item, and maintaining a clock per data item can help. Therefore, instead of a vector clock of size N (number of hosts), we maintain a vector of size M (number of objects). The value of v[i] in the vector v contains the number of writes on data item i.</p>
<p>Each host maintains a vector of size M. Whenever it updates the value of an item, it increments the corresponding element and sends the vector along with the message of data item and new value to every site, which has a copy of the replica. When a host receives an update message, it delays the delivery of a message till each element in its vector is greater than or equal to the one that is piggybacked. After that, the updates to the data items are applied. In this case, the message overhead is O(M) and thus is independent of the number of hosts in the system.</p>
<p>If each message is an update message, it carries the new value of the data item rather than instructions. Then the delivery of an update on a data-item does not need not wait for the previous updates on the same item. This would not have been possible if vector clocks had been used. In that case, the delivery of a massage would have been delayed even for previous messages that are causally overwritten.</p>
<h1>Applications and Databases using Causal Consistency</h1>
<h2>COPS and Eiger</h2>
<p>COPS system [6] (Clusters of Order-Preserving Servers) introduces the causal+ consistency and is designed to support complex online applications that are hosted in a small number of large-scale data-centers, each of which is composed of front-end servers (clients of COPS) and back-end key-value data stores. Eiger [7] has a similar design but a different implementation.</p>
<p>COPS and Eiger support causality through a client library. Both systems replicate writes to geographically distributed data centers and enforce observed ordering. The observed ordering is enforced by delaying the write operations until all causally previous operations have been already applied at that data center.</p>
<p>COPS executes all read and write operations in the local data center in a linearizable [8] fashion, and then replicates data across data centers in a causal+ consistent order in the background. Figure 1 describes the high level architecture of COPS.</p>
<p>&nbsp;</p>
<p><a href="https://blog.mariadb.org/wp-content/uploads/2015/02/cops.jpeg"><img class="alignnone size-full wp-image-2731" src="https://blog.mariadb.org/wp-content/uploads/2015/02/cops.jpeg" alt="cops" width="1474" height="427" /></a></p>
<p><strong>Figure 1: Architecture of COPS and Eiger [6].</strong></p>
<p>Similarly, the Eiger system provides the linearizability inside each data center and the causally-consistent data store based on a column-family data model to achieve better performance in a geo-distributed setting. All operations from clients are served from the local data center using a client library. The library mediates access to nodes in the local data center, executes the read and write transaction algorithms, and tracks causality and attaches dependencies to write operations. Each replica stores full replica of the database, and operations are handled locally. After an operation is executed locally, the operation is asynchronously pushed to remote data centers, but committed only after all causally dependent operations have been previously committed.</p>
<h2>Bolts-on</h2>
<p>Bailis et. al. in [9] propose a client-side middle-ware software called Bolt-on. This middle-ware guarantees only application-defined dependencies as an alternative to the causal consistency. Figure 2 describes the architecture of the Bolt-on middle-ware.</p>
<p>&nbsp;</p>
<p><a href="https://blog.mariadb.org/wp-content/uploads/2015/02/bolt.jpeg"><img class="alignnone size-full wp-image-2730" src="https://blog.mariadb.org/wp-content/uploads/2015/02/bolt.jpeg" alt="bolt" width="699" height="423" /></a></p>
<p><strong>Figure 2: Architecture of Bolts-on [9].</strong></p>
<p>The Bolt-on architecture assumes that the underlying data store handles most aspects of data management, including replication, availability, and convergence. In the architecture, the underlying data store locally uses the eventual consistency and allows a large space of read and write histories; the middle-ware handles causal dependencies, and consists of a set of rules, which limit the possible histories to the histories that obey the desired consistency model.</p>
<h2>Application: MMORPG</h2>
<p>In Massively Multi-player Online Role-Playing Games (MMORPG), players can cooperate with others in a virtual game world, and both players and different game words are naturally distributed. These systems manage large amounts of data, and the biggest problem is how to support data consistency. According to the CAP theorem, we have to sacrifice one of two properties: consistency or availability [10]. If an online game does not guarantee the availability, players&#8217; requests may fail. If data is inconsistent, players may get data not conforming to the game logic, and this data can affect their operations. Therefore, it is important for the MMORPG environment to find a balance between the data consistency and the system availability. For this reason, we must analyze the data consistency requirements of MMORPG so as to find the balance [10].</p>
<p>Diao [10] has studied different consistency models for MMORPG and found that there indeed are part of data, where the causal consistency is an appealing choice: Game data. The game data contains e.g. the world appearance, the meta-data of non-player characters (the characters are created by game developers and controlled only by the game logic), the system configuration and game rules. This data is used by players and the game engine in the entire game, but can be only modified by the game developers. Consistency requirements for the game data are not so strict compared e.g. to the account data. Because e.g. a change of non-player character name or of the duration of bird animation may not be noticed by players.</p>
<p>Furthermore, some change of the game data needs to be delivered to all online players synchronously, e.g. a change of the word appearance, the weapon power, non-player characters, game rules and scripts. If there is inconsistency on these areas, it will cause errors on game display and logic errors on players. Therefore, some data needs to be stored on the server side and some on the client side. The game data on the client side could only synchronize with servers when a player logs in to or starts a game. For this reason, the causal consistency is required [10].</p>
<p>This could mean that when a player A uses the browser to connect with the game server, the game server will check the current local data and update the game data of the client side in the form of data packets. After updating, all future local accesses will return the updated value. Player B, who has not communicated with the game server, will still retain the outdated game data.</p>
<p>Game servers maintain the primary version of game data, and transfer it to client sides. Additionally, players on different game words cannot discuss to each other. Thus, the only need is to make sure that the game data is consistent in one game word in a time so that all players on that game word are handled equally. This requires using the strong consistency locally in the game word and the causal consistency among different game words. When the game data is modified by developers, the update value should be delivered synchronously to all replicates on that game word, and asynchronously to other game words.</p>
<p>While the possibility of using the causal consistency on MMORPG has been identified on research [10] to the authors&#8217; knowledge there is no actual publications or other information that the causal consistency is actually used on MMORPG games (e.g. <a href="http://www.lotro.com/en">Lord of the Rings Online</a>).</p>
<h2>Application: Facebook</h2>
<p><b>Facebook</b> is an online Social networking service. After registering to use the site, users can create a user profile, add other users as friends, exchange messages, post status updates and photos, share videos and receive notifications when others update their profiles.</p>
<p>When you log into your account on Facebook, the server will show your own status messages and your friends&#8217; status messages at that point in time. Status messages on Facebook may contain pictures, shared links and stories or your own messages. Naturally, your account data requires a strong consistency, but for status data the weaker consistency models are acceptable. During the time the user is online, the status updates of a user&#8217;s friends and of the user do not need to be strictly ordered, and the causal ordering is enough.</p>
<p>Thus when a user A sends a status update and a user B replies to that update, there is a causal order on the two updates. However, when users C and D do a totally unrelated update, the order these updates appear to users A and B is not relevant. This is because users A and B do not know in which order updates are performed.</p>
<p>The reason why the eventual consistency is not enough for Facebook status updates is that the eventual consistency does not require any ordering between writes. Consider a case, where the user A first sends a status update, and after few seconds A updates the first status update. With the eventual consistency, all friends of A could see only the first update, because the eventual consistency does not guarantee that first update is performed before the second one. In the causal consistency, as there is a read (by user A) of first update and then write (updated status from user A), these are causally related and all user A&#8217;s friends will naturally see second update.</p>
<p>Although the causal consistency is the possible consistency model for Facebook status updates and several similar distributed services containing status updates like LinkedIn, Twitter and Yahoo, to author&#8217;s knowledge there is not scientific or other literature that would show the causal consistency being really used.</p>
<h1>Conclusions</h1>
<p>The causal consistency model can be enforced with Lamport clocks. Transactions using the causal consistency are executed in an order that reflects their causally-related read/write operations&#8217; order. Concurrent operations may be committed in different orders and their results can be read also in different orders.</p>
<p>Actually, the causal consistency can solve many problems, which cannot be solved in the eventual consistency, such as ordering operations. The causal consistency ensures that every sees operations in the same causal order, and this makes the causal consistency stronger than the eventual consistency. However, the causal consistency cannot support e.g. distributed integrity constraints.</p>
<p>Although there are a few promising systems developed on research to the authors&#8217; knowledge there is no commercial or mature systems using the causal consistency model. For more thorough discussion see [11].</p>
<h1>References</h1>
<p>[1] Mustaque Ahamad , Gil Neiger , James E. Burns , Prince Kohli , P.W. Hutto: Causal Memory: Definitions, Implementation and Programming, <a href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3356">http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.50.3356</a></p>
<p>[2] Leslie Lamport: How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs, <em>IEEE Trans. Comput.</em> C-28,9 (Sept. 1979), 690-691.</p>
<p>[3] Werner Vogels: Eventually consistent. <i>Communications of the ACM</i> <b>52</b>: 40. <a href="https://blog.mariadb.org/10.1145/1435417.1435432">doi:10.1145/1435417.1435432</a></p>
<p>[4] Leslie Lamport: Time, clocks, and the ordering of events in a distributed system, <em>Communications of the ACM</em>, <strong>21</strong>: 7, pp. 558–565, 1978.</p>
<p>[5] C. J. Fidge: Timestamps in message passing systems that preserve the partial ordering, in Theoretical Computer Science, 1988.</p>
<p>[6] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen: Don’t settle for eventual: scalable causal consistency for wide-area storage with COPS, in Proceedings of the 23rd ACM Symposium on Operating Systems Principles, pp. 401–416, Portugal, October 23-26, 2011.</p>
<p>[7] W. Lloyd, M. J. Freedman, M. Kaminsky, and D. G. Andersen: Stronger semantics for low-latency geo-replicated storage,” in Proceedings of the 10th USENIX Symposium on Networked Systems Design and Implementation, pp. 313–328, Lombard, IL, USA, April 2-5, 2013.</p>
<p>[8] M. Herlihy and J. M. Wing, Linearizability: A correctness condition for concurrent objects, <em>ACM</em><br />
<em> Trans. Program. Lang.</em> Syst., <strong>12</strong>:3, pp. 463–492, 1990.</p>
<p>[9] . Bailis, A. Ghodsi, J. M. Hellerstein, and I. Stoica,: Bolt-on causal consistency,” in Proceedings of the ACM SIGMOD International Conference on Management of Data, pp. 761–772, New York, NY, USA, June 22-27, 2013</p>
<p>[10] Z. Diao, “Consistency models for cloud-based on-line games: the storage system’s perspective,” in<br />
25th Workshop on Grundlagen von Datenbanken, Computing, Portland, Oregon, USA, July 16-19,<br />
pp. 16–21, Ilmenau, Germany, May 28 &#8211; 31, 2013.</p>
<p>[11] Mawahib Musa Elbushra, Jan Lindström: Causal Consistent Databases, <i>Open Journal of Databases (OJDB)</i>, <b>2</b>:1, h<a href="http://ttp:0//www.ronpub.com/publications/OJDB_2015v2i1n02_Elbushra.pdf">ttp://www.ronpub.com/publications/OJDB_2015v2i1n02_Elbushra.pdf</a></p><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989012&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989012&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Sun, 22 Feb 2015 10:14:15 +0000</pubDate>
    <dc:creator>MariaDB</dc:creator>
    <category>General</category>
  </item>

  <item>
    <title>Choosing the right MySQL High Availability Solution – webinar replay</title>
    <guid isPermaLink="false">http://www.clusterdb.com/?p=4038</guid>
    <link>http://www.clusterdb.com/mysql/choosing-the-right-mysql-high-availability-solution-webinar-replay</link>
    <description>Earlier this week, I presented a webinar on MySQL High Availability options for MySQL &amp;#8211; what they are and how to choose the most appropriate one for your application.
The replay of this webinar can now be viewed here or if you just want to look at the charts then scroll down. At the end of this post, I include a summary of the Q&amp;amp;A from the webinar.
How important is your data? Can you afford to lose it? What about just some of it? What would be the impact if you couldn&amp;#8217;t access it for a minute, an hour, a day or a week?
Different applications can have very different requirements for High Availability. Some need 100% data reliability with 24x7x365 read &amp;amp; write access while many others are better served by a simpler approach with more modest HA ambitions.
MySQL has an array of High Availability solutions ranging from simple backups, through replication and shared storage clustering &amp;#8211; all the way up to 99.999% available shared nothing, geographically replicated clusters. These solutions also have different &amp;#8216;bonus&amp;#8217; features such as full InnoDB compatibility, in-memory real-time performance, linear scalability and SQL &amp;amp; NoSQL APIs.
The purpose of this webinar is to help you decide where your application sits in terms of HA requirements and discover which of the MySQL solutions best fit the bill. It will also cover what you need outside of the database to ensure High Availability &amp;#8211; state of the art monitoring being a prime example.
The charts
&amp;nbsp;


Questions and Answers

What does &amp;#8220;HA&amp;#8221; stands for? High Availability
What is meant by scale-out? Scale-out is where you increase the capacity or the throughput of the system by adding extra (usually commodity) machines &amp;#8211; this is the opposite of scale-up where you buy the biggest single server that you can find. It tends to be much more economical this way and you can add extra capacity if and when you need it.
Most of my applications are for small businesses were the data load is not that big, is this session worth listening too? Yes &amp;#8211; the focus of this presentation is on keeping your data safe and accessible, not on scaling to massive volumes and throughput. Scale-out is touched on as if you need that as well then it can influence your choice of High Availability solution
How does all this compare with Amazon Aurora? Aurora is something that&amp;#8217;s offered by Amazon and is supported by them. Aurora can only be run on AWS &amp;#8211; the solutions covered here can be taken to lots of different environments &amp;#8211; whether cloud or &amp;#8216;bare metal&amp;#8217;
what amount of work would be involved in migrating actual InnoDB based DB&amp;#8217;s into NDB? As you&amp;#8217;d expect, it depends on the application. Changes should be minor to get it to run but you might need to make more adjustments to get the best performance. This white paper is a good place to start.
Is it possible to migrate InnoDB data to MySQL Cluster? Yes. The first thing to note is that to use MySQL Cluster, you have to use the mysqld (MySQL Server) process that comes with the MySQL Cluster package; once you&amp;#8217;ve switched to that binary then you can run ALTER TABLE my_tab ENGINE=NDB; provided that your schema is compatible with MySQL Cluster. Alternatively you can perform a mysqldump and then load the data in.
Does MySQL Fabric solutions support &amp;#8220;rolling upgrade&amp;#8221;? Yes &amp;#8211; you can perform a rolling upgrade of the managed MySQL Servers but you have to do it yourself, Fabric doesn&amp;#8217;t automate it at present
What about master-master replication? You can use active-active replication but the application is responsibe for avoiding conflicts between the two sites unless you use MySQL Cluster/NDB (where the functionality is built into the database)
What&amp;#8217;s the maximum distance that the master and slave can be apart? For MySQL Replication, there is no maximum distance; the latency of the master isn&amp;#8217;t impacted if you stick with the default asynchronous replication. If you use semi-synchronous replication then the latency of your transactions will be impacted by the WAN latency between the master and slave site(s).
Is there a monetary investment to implement this or is it free? The database technology presented in this session can be used under the GPL open source license; if you buy a commercal edition then you get access to some additional tools such as MySQL Enterprise Monitor and MySQL Cluster Manager.
Can we check the transaction sequence in slave side? Yes, with MySQL replication, you can check which transactions have been applied on the slave.
Can the slave switch over to master, in case the master needs to be shut down? Yes &amp;#8211; that&amp;#8217;s a very common use case
Do we have seperate binlog for different databases (schemas)? All of the databases (schemas) within the MySQL Server use the same binary log. When that log file fills up, it&amp;#8217;s rotated out and a new one used.
How can I implement &amp;#8220;auto-failover&amp;#8221; in mysql 5.6 replication? Are you talking abaout &amp;#8220;mysqlfailover&amp;#8221; script? You can use the mysqlfailover utility or MySQL Fabric
What are the similarties and differences between InnoDB and the NDB/MySQL Cluster engines? We&amp;#8217;ll cover some of this in this presentation and you can find more details in this white paper
With master-master replication, there were some parameters to control the autoincrement IDs so that we could avoid conflicts with active-active MySQL replication? If you ensure that the same row isn&amp;#8217;t written to on both masters then you can avoid conflicts. One option could be to store odd primary keys on one and even on the other. You can then set the auto_increment_increment and auto_increment_offset parameters and then use auto-increment primary keys on each MySQL Server
Is there an online backup tool available in mysql &amp;#8211; without locking my database? Yes &amp;#8211; if you&amp;#8217;re using InnoDB then you can use MySQL Enterprise Backup (part of MySQL Enterprise Edition and it&amp;#8217;s also much faster than mysqldump); MySQL Cluster it has a built-in online backup tool (which can be simpler to use in conjunction with MySQL Cluter Manager which is part of MySQL Cluster Carrier Grade Edition)
In multi-source replication, how are data conflicts handled? If more than one master modifies the same row, which one is applied? That&amp;#8217;s left as an exercise for the application. The application is responsible for making sure that there are no conflicting updates (if you care about the conflicts). You could also use MySQL Fabric to shard the data.
With MySQL Group Replication, if one master is down, will transactions still be applied? With MySQL Group Replication, updates can be sent to any of the servers and so if one is unavailable just switch to any of the others
what is the maximum data size MySQL can handle in it&amp;#8217;s latest release? The maximum size of an InnoDB table[space] is four billion pages (64TB with the default 16k page size). Beyond that you start partitioning or use MySQL Fabric
Can you give a short definition for sharding? Sharding is where you take the data for a table and split it accross multiple MySQL Server instances. Typically you&amp;#8217;ll choose one or more columns from the table to act as the sharding key to decide which shard a specific row should be stored in. You can also have functional sharding where you decide that the whole of table A will be in one shard and all of table B in another
Is there a storage engine to handle JSON documents? Here&amp;#8217;s an interesting blog on that topic
What algorithms are available for sharding in MySQL? With MySQL Fabric you can use a hash or define ranges for the shard key. With MySQL Cluster, the sharding is completely transparrent to the application but it uses a MD5 hashing under the covers
In MySQL cluster is all the data replicated to all of the data nodes? Data is synchrously replicated between the 2 data nodes forming a node group. Different node groups are responsible for different shards for any given table
What&amp;#8217;s the maximum number of data nodes in MySQL Cluster? 48
Which it best inter-connect method between MySQL Cluster data nodes for highest performance? Infiniband and GB Ethernet have both shown great results. If you&amp;#8217;re able to configure the behaviour then low latency will help get the best performance
Can I colocate MySQL Cluster data nodes with MySQL Servers in order use all posible resources? Yes, you can co-locate data nodes and MySQL Servers. Note that the management node (ndb_mgmd) should not run on the same machine as any of the data nodes &amp;#8211; read this post for details on how to deploy MySQL Cluster nodes for High Availability
</description>
    <content:encoded><![CDATA[<p><a href="http://www.clusterdb.com/wp-content/uploads/2013/10/MySQL-HA-Logo1.png"><img class="aligncenter size-full wp-image-2970" src="http://www.clusterdb.com/wp-content/uploads/2013/10/MySQL-HA-Logo1.png" alt="MySQL-HA-Logo1.png" width="950" height="100" /></a>Earlier this week, I presented a webinar on MySQL High Availability options for MySQL &#8211; what they are and how to choose the most appropriate one for your application.</p>
<p>The <a title="Achieving MySQL High Availability, Learn to Select Your Best Option - webinar replay" href="https://event.on24.com/eventRegistration/EventLobbyServlet?target=reg20.jsp&amp;eventid=930247&amp;sessionid=1&amp;key=FF21049401BAD50579B6733B15BC9D36&amp;sourcepage=register" target="_blank">replay of this webinar can now be viewed here</a> or if you just want to look at the charts then scroll down. At the end of this post, I include a summary of the Q&amp;A from the webinar.</p>
<p><span>How important is your data? Can you afford to lose it? What about just some of it? What would be the impact if you couldn&#8217;t access it for a minute, an hour, a day or a week?</span></p>
<p><span>Different applications can have very different requirements for High Availability. Some need 100% data reliability with 24x7x365 read &amp; write access while many others are better served by a simpler approach with more modest HA ambitions.</span></p>
<p><span>MySQL has an array of High Availability solutions ranging from simple backups, through replication and shared storage clustering &#8211; all the way up to 99.999% available shared nothing, geographically replicated clusters. These solutions also have different &#8216;bonus&#8217; features such as full InnoDB compatibility, in-memory real-time performance, linear scalability and SQL &amp; NoSQL APIs.</span></p>
<p><span>The purpose of this webinar is to help you decide where your application sits in terms of HA requirements and discover which of the MySQL solutions best fit the bill. It will also cover what you need outside of the database to ensure High Availability &#8211; state of the art monitoring being a prime example.</span></p>
<h2>The charts</h2>
<p>&nbsp;</p>
<p></p>
<h2></h2>
<h2>Questions and Answers</h2>
<ul>
<li><strong>What does &#8220;HA&#8221; stands for?</strong> High Availability</li>
<li><strong>What is meant by scale-out?</strong> Scale-out is where you increase the capacity or the throughput of the system by adding extra (usually commodity) machines &#8211; this is the opposite of scale-up where you buy the biggest single server that you can find. It tends to be much more economical this way and you can add extra capacity if and when you need it.</li>
<li><strong>Most of my applications are for small businesses were the data load is not that big, is this session worth listening too?</strong> Yes &#8211; the focus of this presentation is on keeping your data safe and accessible, not on scaling to massive volumes and throughput. Scale-out is touched on as if you need that as well then it can influence your choice of High Availability solution</li>
<li><strong>How does all this compare with Amazon Aurora?</strong> Aurora is something that&#8217;s offered by Amazon and is supported by them. Aurora can only be run on AWS &#8211; the solutions covered here can be taken to lots of different environments &#8211; whether cloud or &#8216;bare metal&#8217;</li>
<li><strong>what amount of work would be involved in migrating actual InnoDB based DB&#8217;s into NDB?</strong> As you&#8217;d expect, it depends on the application. Changes should be minor to get it to run but you might need to make more adjustments to get the best performance. This <a title="MySQL Cluster Evaluation Guide" href="http://www.mysql.com/why-mysql/white-papers/mysql-cluster-evaluation-guide/" target="_blank">white paper</a> is a good place to start.</li>
<li><strong>Is it possible to migrate InnoDB data to MySQL Cluster?</strong> Yes. The first thing to note is that to use MySQL Cluster, you have to use the mysqld (MySQL Server) process that comes with the MySQL Cluster package; once you&#8217;ve switched to that binary then you can run ALTER TABLE my_tab ENGINE=NDB; provided that your schema is compatible with MySQL Cluster. Alternatively you can perform a mysqldump and then load the data in.</li>
<li><strong>Does MySQL Fabric solutions support &#8220;rolling upgrade&#8221;?</strong> Yes &#8211; you can perform a rolling upgrade of the managed MySQL Servers but you have to do it yourself, Fabric doesn&#8217;t automate it at present</li>
<li><strong>What about master-master replication?</strong> You can use active-active replication but the application is responsibe for avoiding conflicts between the two sites unless you use MySQL Cluster/NDB (where the functionality is built into the database)</li>
<li><strong>What&#8217;s the maximum distance that the master and slave can be apart?</strong> For MySQL Replication, there is no maximum distance; the latency of the master isn&#8217;t impacted if you stick with the default asynchronous replication. If you use semi-synchronous replication then the latency of your transactions will be impacted by the WAN latency between the master and slave site(s).</li>
<li><strong>Is there a monetary investment to implement this or is it free?</strong> The database technology presented in this session can be used under the GPL open source license; if you buy a commercal edition then you get access to some additional tools such as MySQL Enterprise Monitor and MySQL Cluster Manager.</li>
<li><strong>Can we check the transaction sequence in slave side?</strong> Yes, with MySQL replication, you can check which transactions have been applied on the slave.</li>
<li><strong>Can the slave switch over to master, in case the master needs to be shut down?</strong> Yes &#8211; that&#8217;s a very common use case</li>
<li><strong>Do we have seperate binlog for different databases (schemas)?</strong> All of the databases (schemas) within the MySQL Server use the same binary log. When that log file fills up, it&#8217;s rotated out and a new one used.</li>
<li><strong>How can I implement &#8220;auto-failover&#8221; in mysql 5.6 replication? Are you talking abaout &#8220;mysqlfailover&#8221; script?</strong> You can use the mysqlfailover utility or MySQL Fabric</li>
<li><strong>What are the similarties and differences between InnoDB and the NDB/MySQL Cluster engines?</strong> We&#8217;ll cover some of this in this presentation and you can find more details in this <a title="MySQL Cluster Evaluation Guide" href="http://www.mysql.com/why-mysql/white-papers/mysql-cluster-evaluation-guide/" target="_blank">white paper</a></li>
<li><strong>With master-master replication, there were some parameters to control the autoincrement IDs so that we could avoid conflicts with active-active MySQL replication?</strong> If you ensure that the same row isn&#8217;t written to on both masters then you can avoid conflicts. One option could be to store odd primary keys on one and even on the other. You can then set the <a title="auto_increment_increment" href="http://dev.mysql.com/doc/refman/5.0/en/replication-options-master.html#sysvar_auto_increment_increment" target="_blank">auto_increment_increment</a> and <a title="auto_increment_offset" href="http://dev.mysql.com/doc/refman/5.0/en/replication-options-master.html#sysvar_auto_increment_offset" target="_blank">auto_increment_offset</a> parameters and then use auto-increment primary keys on each MySQL Server</li>
<li><strong>Is there an online backup tool available in mysql &#8211; without locking my database?</strong> Yes &#8211; if you&#8217;re using InnoDB then you can use MySQL Enterprise Backup (part of MySQL Enterprise Edition and it&#8217;s also <em>much</em> faster than mysqldump); MySQL Cluster it has a built-in online backup tool (which can be simpler to use in conjunction with MySQL Cluter Manager which is part of MySQL Cluster Carrier Grade Edition)</li>
<li><strong>In multi-source replication, how are data conflicts handled? If more than one master modifies the same row, which one is applied?</strong> That&#8217;s left as an exercise for the application. The application is responsible for making sure that there are no conflicting updates (if you care about the conflicts). You could also use MySQL Fabric to shard the data.</li>
<li><strong>With MySQL Group Replication, if one master is down, will transactions still be applied?</strong> With MySQL Group Replication, updates can be sent to any of the servers and so if one is unavailable just switch to any of the others</li>
<li><strong>what is the maximum data size MySQL can handle in it&#8217;s latest release?</strong> The maximum size of an InnoDB table[space] is four billion pages (64TB with the default 16k page size). Beyond that you start partitioning or use MySQL Fabric</li>
<li><strong>Can you give a short definition for sharding?</strong> Sharding is where you take the data for a table and split it accross multiple MySQL Server instances. Typically you&#8217;ll choose one or more columns from the table to act as the sharding key to decide which shard a specific row should be stored in. You can also have functional sharding where you decide that the whole of table A will be in one shard and all of table B in another</li>
<li><strong>Is there a storage engine to handle JSON documents?</strong> Here&#8217;s an <a title="JSON queries using MySQL" href="interesting blog on that topic. http://blog.ulf-wendel.de/2013/mysql-5-7-sql-functions-for-json-udf/" target="_blank">interesting blog on that topic</a></li>
<li><strong>What algorithms are available for sharding in MySQL?</strong> With MySQL Fabric you can use a hash or define ranges for the shard key. With MySQL Cluster, the sharding is completely transparrent to the application but it uses a MD5 hashing under the covers</li>
<li><strong>In MySQL cluster is all the data replicated to all of the data nodes?</strong> Data is synchrously replicated between the 2 data nodes forming a node group. Different node groups are responsible for different shards for any given table</li>
<li><strong>What&#8217;s the maximum number of data nodes in MySQL Cluster?</strong> 48</li>
<li><strong>Which it best inter-connect method between MySQL Cluster data nodes for highest performance?</strong> Infiniband and GB Ethernet have both shown great results. If you&#8217;re able to configure the behaviour then low latency will help get the best performance</li>
<li><strong>Can I colocate MySQL Cluster data nodes with MySQL Servers in order use all posible resources?</strong> Yes, you can co-locate data nodes and MySQL Servers. Note that the management node (ndb_mgmd) should not run on the same machine as any of the data nodes &#8211; <a title="read this post for details on how to deploy MySQL Cluster nodes for High Availability" href="http://www.clusterdb.com/mysql-cluster/mysql-cluster-fault-tolerance-impact-of-deployment-decisions" target="_blank">read this post for details on how to deploy MySQL Cluster nodes for High Availability</a></li>
</ul><br/>PlanetMySQL Voting: <a href="http://planet.mysql.com/entry/vote/?entry_id=5989006&vote=1&apivote=1">Vote UP</a> / <a href="http://planet.mysql.com/entry/vote/?entry_id=5989006&vote=-1&apivote=1">Vote DOWN</a>]]></content:encoded>
    <pubDate>Fri, 20 Feb 2015 14:49:01 +0000</pubDate>
    <dc:creator>Andrew Morgan</dc:creator>
    <category>MySQL</category>
    <category>clustering</category>
    <category>DRBD</category>
    <category>fabric</category>
    <category>HA</category>
    <category>High Availability</category>
    <category>MySQL Cluster</category>
    <category>Replication</category>
    <category>Scale</category>
  </item>

</channel>
</rss>
